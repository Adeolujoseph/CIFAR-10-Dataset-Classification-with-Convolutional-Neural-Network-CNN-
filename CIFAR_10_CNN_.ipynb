{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "machine_shape": "hm",
      "gpuType": "A100",
      "authorship_tag": "ABX9TyO3ZU14jNercKoziJLDHJ3T"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 48,
      "metadata": {
        "id": "Jc7aN1JBvvdX"
      },
      "outputs": [],
      "source": [
        "import torch\n",
        "from torchvision import datasets, transforms\n",
        "import torchvision\n",
        "from tqdm.notebook import tqdm\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "import torch.optim as optim"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "torch.manual_seed(42)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "xIzQkoB5fmuL",
        "outputId": "5244e946-69f8-4e37-bbac-db3a585c84d1"
      },
      "execution_count": 49,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<torch._C.Generator at 0x7e475d679b70>"
            ]
          },
          "metadata": {},
          "execution_count": 49
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "train_dataset=datasets.CIFAR10(root='./data',download=True, train= True)\n",
        "test_dataset=datasets.CIFAR10(root='./data',download=True, train= False)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "lH3zGHwOv6Vl",
        "outputId": "5f9e4fb1-db51-49d6-d1f0-fb5b99150725"
      },
      "execution_count": 50,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Files already downloaded and verified\n",
            "Files already downloaded and verified\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "train_dataset = datasets.CIFAR10(root='./data', download=True, train=True, transform=transforms.ToTensor())\n",
        "\n",
        "# Check the shape of the first image in the dataset to also confirm the number of channels and image size\n",
        "train_dataset[0][0].shape"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "e2VirHJ9y6X-",
        "outputId": "b281da5e-5897-495f-d296-186ee585bf8a"
      },
      "execution_count": 51,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Files already downloaded and verified\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "torch.Size([3, 32, 32])"
            ]
          },
          "metadata": {},
          "execution_count": 51
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "train_transforms = transforms.Compose([\n",
        "                                    transforms.ToTensor(),  # Convert numpy array to tensor\n",
        "                                    transforms.RandomAffine( # Data Augmentation\n",
        "                                        degrees=(-5, 5), translate=(0, 0.05), scale=(0.9, 1.1)),\n",
        "                                        transforms.RandomResizedCrop((32, 32), scale=(0.75, 1))])\n",
        "\n",
        "val_transforms = transforms.Compose([transforms.ToTensor()])"
      ],
      "metadata": {
        "id": "TOyVRGSD8FyC"
      },
      "execution_count": 60,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "train_dataset = datasets.CIFAR10(root='./data', download=True, train=True, transform=train_transforms)\n",
        "val_dataset = datasets.CIFAR10(root='./data', download=True, train=False, transform=val_transforms)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ejOjLoLKEojR",
        "outputId": "49fffd62-ac49-4257-83ce-c69d784833f5"
      },
      "execution_count": 61,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Files already downloaded and verified\n",
            "Files already downloaded and verified\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Create data loaders\n",
        "batch_size = 64\n",
        "train_loader = torch.utils.data.DataLoader(train_dataset, batch_size=batch_size, shuffle=True, )\n",
        "test_loader = torch.utils.data.DataLoader(val_dataset, batch_size=batch_size, shuffle=False, )\n",
        "print(f\"There are {len(train_dataset)} train images and {len(val_dataset)} val images\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "RSr55-c4F2A4",
        "outputId": "b9cb7053-1123-4b56-daae-e78e0abc7e04"
      },
      "execution_count": 62,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "There are 50000 train images and 10000 val images\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Create a neural net class\n",
        "class Net(nn.Module):\n",
        "    # Constructor\n",
        "    def __init__(self, num_classes=10):\n",
        "        super(Net, self).__init__()\n",
        "\n",
        "        # Our images are RGB, so input channels = 3. We'll apply 32 filters in the first convolutional layer\n",
        "        self.conv1 = nn.Conv2d(in_channels=3, out_channels=32, kernel_size=3, stride=1, padding=1)\n",
        "        self.bn1 = nn.BatchNorm2d(32)\n",
        "\n",
        "        # A second convolutional layer takes 32 input channels, and generates 64 outputs\n",
        "        self.conv2 = nn.Conv2d(in_channels=32, out_channels=64, kernel_size=3, stride=1, padding=1)\n",
        "        self.bn2 = nn.BatchNorm2d(64)\n",
        "\n",
        "        # We'll apply max pooling with a kernel size of 2\n",
        "        self.pool1 = nn.MaxPool2d(kernel_size=2)\n",
        "\n",
        "        # A third convolutional layer takes 64 input channels, and generates 128 outputs\n",
        "        self.conv3 = nn.Conv2d(in_channels=64, out_channels=128, kernel_size=3, stride=1, padding=1)\n",
        "        self.bn3 = nn.BatchNorm2d(128)\n",
        "\n",
        "        # A fourth convolutional layer takes 128 input channels, and generates 128 outputs\n",
        "        self.conv4 = nn.Conv2d(in_channels=128, out_channels=128, kernel_size=3, stride=1, padding=1)\n",
        "        self.bn4 = nn.BatchNorm2d(128)\n",
        "\n",
        "        self.pool2 = nn.MaxPool2d(kernel_size=2)\n",
        "\n",
        "        # A fifth convolutional layer takes 128 input channels, and generates 256 outputs\n",
        "        self.conv5 = nn.Conv2d(in_channels=128, out_channels=256, kernel_size=3, stride=1, padding=1)\n",
        "        self.bn5 = nn.BatchNorm2d(256)\n",
        "\n",
        "        self.conv6 = nn.Conv2d(in_channels=256, out_channels=256, kernel_size=3, stride=1, padding=1)\n",
        "        self.bn6 = nn.BatchNorm2d(256)\n",
        "\n",
        "         # We'll apply another max pooling with a kernel size of 2\n",
        "        self.pool3 = nn.MaxPool2d(kernel_size=2)\n",
        "\n",
        "        # A drop layer deletes 20% of the features to help prevent overfitting\n",
        "        self.drop = nn.Dropout2d(p=0.2)\n",
        "\n",
        "        self.fc = nn.Linear(in_features=4 * 4 * 256, out_features=2048)\n",
        "        self.fc2 = nn.Linear(in_features=2048, out_features=1024)\n",
        "        self.fc3 = nn.Linear(in_features=1024, out_features=512)\n",
        "        self.fc4 = nn.Linear(in_features=512, out_features=512)\n",
        "        self.fc5 = nn.Linear(in_features=512, out_features=256)\n",
        "        self.fc6 = nn.Linear(in_features=256, out_features=128)\n",
        "        self.fc7 = nn.Linear(in_features=128, out_features=num_classes)\n",
        "\n",
        "    def forward(self, x):\n",
        "        x = F.relu(self.bn1(self.conv1(x)))\n",
        "        x = F.relu(self.bn2(self.conv2(x)))\n",
        "        x = self.pool1(x)\n",
        "\n",
        "        x = F.relu(self.bn3(self.conv3(x)))\n",
        "        x = F.relu(self.bn4(self.conv4(x)))\n",
        "        x = self.pool2(x)\n",
        "\n",
        "        x = F.relu(self.bn5(self.conv5(x)))\n",
        "        x = F.relu(self.bn6(self.conv6(x)))\n",
        "        x = self.pool3(x)\n",
        "\n",
        "        x = self.drop(x)\n",
        "\n",
        "        # Flatten\n",
        "        x = x.view(-1, 4 * 4 * 256)\n",
        "        # Feed to fully-connected layer to predict class\n",
        "        x = self.fc(x)\n",
        "        x = F.relu(x)\n",
        "        x = self.fc2(x)\n",
        "        x = F.relu(x)\n",
        "        x = self.fc3(x)\n",
        "        x = F.relu(x)\n",
        "        x = self.fc4(x)\n",
        "        x = F.relu(x)\n",
        "        x = self.fc5(x)\n",
        "        x = F.relu(x)\n",
        "        x = self.fc6(x)\n",
        "        x = F.relu(x)\n",
        "        x = self.fc7(x)\n",
        "        # Return log_softmax tensor\n",
        "        return F.log_softmax(x, dim=1)\n",
        "# Instantiate the model\n",
        "model = Net()\n",
        "print(\"CNN model class defined!\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "l5b418mHGWuM",
        "outputId": "4f23e6d3-8f35-47e5-c1c0-19930f60b796"
      },
      "execution_count": 63,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "CNN model class defined!\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "def train(model, device, train_loader, optimizer, epoch):\n",
        "    # Set the model to training mode\n",
        "    model.train()\n",
        "    train_loss = 0\n",
        "    print(\"Epoch:\", epoch)\n",
        "    # Process the images in batches\n",
        "    for batch_idx, (data, target) in enumerate(train_loader):\n",
        "        # Use the CPU or GPU as appropriate\n",
        "        data, target = data.to(device), target.to(device)\n",
        "\n",
        "        # Reset the optimizer\n",
        "        optimizer.zero_grad()\n",
        "\n",
        "        # Push the data forward through the model layers\n",
        "        output = model(data)\n",
        "\n",
        "        # Get the loss\n",
        "        loss = loss_criteria(output, target)\n",
        "\n",
        "        # Keep a running total\n",
        "        train_loss += loss.item()\n",
        "\n",
        "        # Backpropagate\n",
        "        loss.backward()\n",
        "        optimizer.step()\n",
        "\n",
        "        # Print metrics for every 10 batches so we see some progress\n",
        "        if batch_idx % 10 == 0:\n",
        "            print(f'Training set:{batch_idx * len(data)}/{len(train_loader.dataset)} ({100. * batch_idx / len(train_loader)} Loss:{loss.item()})')\n",
        "    # return average loss for the epoch\n",
        "    avg_loss = train_loss / (batch_idx+1)\n",
        "    print('Training set: Average loss: {:.6f}'.format(avg_loss))\n",
        "    return avg_loss"
      ],
      "metadata": {
        "id": "Hdww7n-1nOEo"
      },
      "execution_count": 64,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def test(model, device, test_loader):\n",
        "    # Switch the model to evaluation mode (so we don't backpropagate or drop)\n",
        "    model.eval()\n",
        "    test_loss = 0\n",
        "    correct = 0\n",
        "    with torch.no_grad():\n",
        "        batch_count = 0\n",
        "        for data, target in test_loader:\n",
        "            batch_count += 1\n",
        "            data, target = data.to(device), target.to(device)\n",
        "\n",
        "            # Get the predicted classes for this batch\n",
        "            output = model(data)\n",
        "\n",
        "            # Calculate the loss for this batch\n",
        "            test_loss += loss_criteria(output, target).item()\n",
        "\n",
        "            # Calculate the accuracy for this batch\n",
        "            _, predicted = torch.max(output.data, 1)\n",
        "            correct += torch.sum(target==predicted).item()\n",
        "\n",
        "    # Calculate the average loss and total accuracy for this epoch\n",
        "    avg_loss = test_loss/batch_count\n",
        "    print(f'Validation set: Average loss: {avg_loss}, Accuracy: {correct}/{len(test_loader.dataset)} ({100. * correct / len(test_loader.dataset)}%)')\n",
        "   # return average loss for the epoch\n",
        "    return avg_loss"
      ],
      "metadata": {
        "id": "tyR8qLAGnR5P"
      },
      "execution_count": 65,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "device = \"cpu\"\n",
        "if (torch.cuda.is_available()):\n",
        "    # if GPU available, use cuda (on a cpu, training will take a considerable length of time!)\n",
        "    device = \"cuda\"\n",
        "print('Training on', device)\n",
        "\n",
        "# Create an instance of the model class and allocate it to the device\n",
        "model = Net(num_classes=10).to(device)\n",
        "\n",
        "# Use an \"Adam\" optimizer to adjust weights\n",
        "optimizer = optim.Adam(model.parameters(), lr=0.001)\n",
        "\n",
        "# Specify the loss criteria\n",
        "loss_criteria = nn.CrossEntropyLoss()\n",
        "\n",
        "# Track metrics in these arrays\n",
        "epoch_nums = []\n",
        "training_loss = []\n",
        "validation_loss = []\n",
        "\n",
        "# Train over  epochs\n",
        "epochs = 30\n",
        "for epoch in range(1, epochs + 1):\n",
        "        train_loss = train(model, device, train_loader, optimizer, epoch)\n",
        "        test_loss = test(model, device, test_loader)\n",
        "        epoch_nums.append(epoch)\n",
        "        training_loss.append(train_loss)\n",
        "        validation_loss.append(test_loss)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "R70R-PL8lN8v",
        "outputId": "0ef74383-85dc-4df2-e73d-a70fcf91e1bb"
      },
      "execution_count": 66,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Training on cuda\n",
            "Epoch: 1\n",
            "Training set:0/50000 (0.0 Loss:2.3020272254943848)\n",
            "Training set:640/50000 (1.278772378516624 Loss:2.2511985301971436)\n",
            "Training set:1280/50000 (2.557544757033248 Loss:2.185365915298462)\n",
            "Training set:1920/50000 (3.836317135549872 Loss:1.9921156167984009)\n",
            "Training set:2560/50000 (5.115089514066496 Loss:2.0177829265594482)\n",
            "Training set:3200/50000 (6.3938618925831205 Loss:2.128391742706299)\n",
            "Training set:3840/50000 (7.672634271099744 Loss:2.0279667377471924)\n",
            "Training set:4480/50000 (8.951406649616368 Loss:1.9136775732040405)\n",
            "Training set:5120/50000 (10.230179028132993 Loss:1.992175817489624)\n",
            "Training set:5760/50000 (11.508951406649617 Loss:2.011855363845825)\n",
            "Training set:6400/50000 (12.787723785166241 Loss:1.9492638111114502)\n",
            "Training set:7040/50000 (14.066496163682864 Loss:2.022703170776367)\n",
            "Training set:7680/50000 (15.345268542199488 Loss:2.1011548042297363)\n",
            "Training set:8320/50000 (16.624040920716112 Loss:1.8113436698913574)\n",
            "Training set:8960/50000 (17.902813299232736 Loss:1.872146725654602)\n",
            "Training set:9600/50000 (19.18158567774936 Loss:1.8781801462173462)\n",
            "Training set:10240/50000 (20.460358056265985 Loss:1.9708307981491089)\n",
            "Training set:10880/50000 (21.73913043478261 Loss:2.018216609954834)\n",
            "Training set:11520/50000 (23.017902813299234 Loss:1.9570294618606567)\n",
            "Training set:12160/50000 (24.296675191815858 Loss:1.9717588424682617)\n",
            "Training set:12800/50000 (25.575447570332482 Loss:1.8288787603378296)\n",
            "Training set:13440/50000 (26.854219948849106 Loss:1.9783220291137695)\n",
            "Training set:14080/50000 (28.132992327365727 Loss:2.1236512660980225)\n",
            "Training set:14720/50000 (29.41176470588235 Loss:1.9992276430130005)\n",
            "Training set:15360/50000 (30.690537084398976 Loss:1.969894528388977)\n",
            "Training set:16000/50000 (31.9693094629156 Loss:1.7999001741409302)\n",
            "Training set:16640/50000 (33.248081841432224 Loss:1.9461419582366943)\n",
            "Training set:17280/50000 (34.52685421994885 Loss:1.9653571844100952)\n",
            "Training set:17920/50000 (35.80562659846547 Loss:1.9213460683822632)\n",
            "Training set:18560/50000 (37.084398976982094 Loss:1.9672919511795044)\n",
            "Training set:19200/50000 (38.36317135549872 Loss:1.9373940229415894)\n",
            "Training set:19840/50000 (39.64194373401534 Loss:1.9508320093154907)\n",
            "Training set:20480/50000 (40.92071611253197 Loss:1.8825870752334595)\n",
            "Training set:21120/50000 (42.19948849104859 Loss:1.8474454879760742)\n",
            "Training set:21760/50000 (43.47826086956522 Loss:2.4009549617767334)\n",
            "Training set:22400/50000 (44.75703324808184 Loss:1.8941444158554077)\n",
            "Training set:23040/50000 (46.03580562659847 Loss:1.9909257888793945)\n",
            "Training set:23680/50000 (47.31457800511509 Loss:1.9802082777023315)\n",
            "Training set:24320/50000 (48.593350383631716 Loss:1.85296630859375)\n",
            "Training set:24960/50000 (49.87212276214834 Loss:1.9915703535079956)\n",
            "Training set:25600/50000 (51.150895140664964 Loss:1.8833703994750977)\n",
            "Training set:26240/50000 (52.429667519181585 Loss:1.8338521718978882)\n",
            "Training set:26880/50000 (53.70843989769821 Loss:1.917999267578125)\n",
            "Training set:27520/50000 (54.987212276214834 Loss:1.8019298315048218)\n",
            "Training set:28160/50000 (56.265984654731454 Loss:1.9300189018249512)\n",
            "Training set:28800/50000 (57.54475703324808 Loss:1.8318819999694824)\n",
            "Training set:29440/50000 (58.8235294117647 Loss:1.8293724060058594)\n",
            "Training set:30080/50000 (60.10230179028133 Loss:1.8688044548034668)\n",
            "Training set:30720/50000 (61.38107416879795 Loss:1.8049579858779907)\n",
            "Training set:31360/50000 (62.65984654731458 Loss:1.9232828617095947)\n",
            "Training set:32000/50000 (63.9386189258312 Loss:1.8864402770996094)\n",
            "Training set:32640/50000 (65.21739130434783 Loss:1.8598735332489014)\n",
            "Training set:33280/50000 (66.49616368286445 Loss:1.8662431240081787)\n",
            "Training set:33920/50000 (67.77493606138107 Loss:1.8495992422103882)\n",
            "Training set:34560/50000 (69.0537084398977 Loss:2.017012119293213)\n",
            "Training set:35200/50000 (70.33248081841433 Loss:1.8579819202423096)\n",
            "Training set:35840/50000 (71.61125319693095 Loss:1.7492371797561646)\n",
            "Training set:36480/50000 (72.89002557544757 Loss:1.886411190032959)\n",
            "Training set:37120/50000 (74.16879795396419 Loss:1.8566757440567017)\n",
            "Training set:37760/50000 (75.44757033248082 Loss:1.8327395915985107)\n",
            "Training set:38400/50000 (76.72634271099744 Loss:1.8382337093353271)\n",
            "Training set:39040/50000 (78.00511508951406 Loss:1.8862290382385254)\n",
            "Training set:39680/50000 (79.28388746803068 Loss:1.8216497898101807)\n",
            "Training set:40320/50000 (80.56265984654732 Loss:2.06654691696167)\n",
            "Training set:40960/50000 (81.84143222506394 Loss:1.8624063730239868)\n",
            "Training set:41600/50000 (83.12020460358056 Loss:1.7584830522537231)\n",
            "Training set:42240/50000 (84.39897698209718 Loss:1.7547539472579956)\n",
            "Training set:42880/50000 (85.67774936061382 Loss:1.8360384702682495)\n",
            "Training set:43520/50000 (86.95652173913044 Loss:1.8901876211166382)\n",
            "Training set:44160/50000 (88.23529411764706 Loss:1.8111546039581299)\n",
            "Training set:44800/50000 (89.51406649616368 Loss:1.7748738527297974)\n",
            "Training set:45440/50000 (90.79283887468031 Loss:1.8694727420806885)\n",
            "Training set:46080/50000 (92.07161125319693 Loss:1.8686751127243042)\n",
            "Training set:46720/50000 (93.35038363171356 Loss:1.9081931114196777)\n",
            "Training set:47360/50000 (94.62915601023018 Loss:1.7163187265396118)\n",
            "Training set:48000/50000 (95.9079283887468 Loss:1.8319406509399414)\n",
            "Training set:48640/50000 (97.18670076726343 Loss:1.8247560262680054)\n",
            "Training set:49280/50000 (98.46547314578005 Loss:1.6922887563705444)\n",
            "Training set:49920/50000 (99.74424552429667 Loss:1.9355461597442627)\n",
            "Training set: Average loss: 1.920406\n",
            "Validation set: Average loss: 2.419896036196666, Accuracy: 2015/10000 (20.15%)\n",
            "Epoch: 2\n",
            "Training set:0/50000 (0.0 Loss:1.8380954265594482)\n",
            "Training set:640/50000 (1.278772378516624 Loss:1.8765665292739868)\n",
            "Training set:1280/50000 (2.557544757033248 Loss:1.734649658203125)\n",
            "Training set:1920/50000 (3.836317135549872 Loss:1.730724573135376)\n",
            "Training set:2560/50000 (5.115089514066496 Loss:1.7375489473342896)\n",
            "Training set:3200/50000 (6.3938618925831205 Loss:1.762514352798462)\n",
            "Training set:3840/50000 (7.672634271099744 Loss:1.8286774158477783)\n",
            "Training set:4480/50000 (8.951406649616368 Loss:1.8167674541473389)\n",
            "Training set:5120/50000 (10.230179028132993 Loss:1.6987051963806152)\n",
            "Training set:5760/50000 (11.508951406649617 Loss:1.8796091079711914)\n",
            "Training set:6400/50000 (12.787723785166241 Loss:1.7142550945281982)\n",
            "Training set:7040/50000 (14.066496163682864 Loss:1.7549254894256592)\n",
            "Training set:7680/50000 (15.345268542199488 Loss:1.7262529134750366)\n",
            "Training set:8320/50000 (16.624040920716112 Loss:1.5081919431686401)\n",
            "Training set:8960/50000 (17.902813299232736 Loss:1.7323787212371826)\n",
            "Training set:9600/50000 (19.18158567774936 Loss:1.7139276266098022)\n",
            "Training set:10240/50000 (20.460358056265985 Loss:1.709555983543396)\n",
            "Training set:10880/50000 (21.73913043478261 Loss:1.707705020904541)\n",
            "Training set:11520/50000 (23.017902813299234 Loss:1.6702247858047485)\n",
            "Training set:12160/50000 (24.296675191815858 Loss:1.5547178983688354)\n",
            "Training set:12800/50000 (25.575447570332482 Loss:1.8652169704437256)\n",
            "Training set:13440/50000 (26.854219948849106 Loss:1.4970759153366089)\n",
            "Training set:14080/50000 (28.132992327365727 Loss:1.9015530347824097)\n",
            "Training set:14720/50000 (29.41176470588235 Loss:1.5566526651382446)\n",
            "Training set:15360/50000 (30.690537084398976 Loss:1.720969796180725)\n",
            "Training set:16000/50000 (31.9693094629156 Loss:1.574942708015442)\n",
            "Training set:16640/50000 (33.248081841432224 Loss:1.5220438241958618)\n",
            "Training set:17280/50000 (34.52685421994885 Loss:1.5552046298980713)\n",
            "Training set:17920/50000 (35.80562659846547 Loss:1.651733160018921)\n",
            "Training set:18560/50000 (37.084398976982094 Loss:1.563278317451477)\n",
            "Training set:19200/50000 (38.36317135549872 Loss:1.6416548490524292)\n",
            "Training set:19840/50000 (39.64194373401534 Loss:1.5604438781738281)\n",
            "Training set:20480/50000 (40.92071611253197 Loss:1.6382310390472412)\n",
            "Training set:21120/50000 (42.19948849104859 Loss:1.640920639038086)\n",
            "Training set:21760/50000 (43.47826086956522 Loss:1.4784531593322754)\n",
            "Training set:22400/50000 (44.75703324808184 Loss:1.6329864263534546)\n",
            "Training set:23040/50000 (46.03580562659847 Loss:1.5310657024383545)\n",
            "Training set:23680/50000 (47.31457800511509 Loss:1.7777286767959595)\n",
            "Training set:24320/50000 (48.593350383631716 Loss:1.6703197956085205)\n",
            "Training set:24960/50000 (49.87212276214834 Loss:1.6745145320892334)\n",
            "Training set:25600/50000 (51.150895140664964 Loss:1.521976113319397)\n",
            "Training set:26240/50000 (52.429667519181585 Loss:1.558209776878357)\n",
            "Training set:26880/50000 (53.70843989769821 Loss:1.408208966255188)\n",
            "Training set:27520/50000 (54.987212276214834 Loss:1.3464910984039307)\n",
            "Training set:28160/50000 (56.265984654731454 Loss:1.279246211051941)\n",
            "Training set:28800/50000 (57.54475703324808 Loss:1.5004433393478394)\n",
            "Training set:29440/50000 (58.8235294117647 Loss:1.6381797790527344)\n",
            "Training set:30080/50000 (60.10230179028133 Loss:1.5695734024047852)\n",
            "Training set:30720/50000 (61.38107416879795 Loss:1.5686253309249878)\n",
            "Training set:31360/50000 (62.65984654731458 Loss:1.2890362739562988)\n",
            "Training set:32000/50000 (63.9386189258312 Loss:1.3154712915420532)\n",
            "Training set:32640/50000 (65.21739130434783 Loss:1.5657436847686768)\n",
            "Training set:33280/50000 (66.49616368286445 Loss:1.568326711654663)\n",
            "Training set:33920/50000 (67.77493606138107 Loss:1.4014118909835815)\n",
            "Training set:34560/50000 (69.0537084398977 Loss:1.5394864082336426)\n",
            "Training set:35200/50000 (70.33248081841433 Loss:1.4276490211486816)\n",
            "Training set:35840/50000 (71.61125319693095 Loss:1.5532031059265137)\n",
            "Training set:36480/50000 (72.89002557544757 Loss:1.6878042221069336)\n",
            "Training set:37120/50000 (74.16879795396419 Loss:1.4797428846359253)\n",
            "Training set:37760/50000 (75.44757033248082 Loss:1.2461210489273071)\n",
            "Training set:38400/50000 (76.72634271099744 Loss:1.2869993448257446)\n",
            "Training set:39040/50000 (78.00511508951406 Loss:1.3680086135864258)\n",
            "Training set:39680/50000 (79.28388746803068 Loss:1.283735752105713)\n",
            "Training set:40320/50000 (80.56265984654732 Loss:1.5236663818359375)\n",
            "Training set:40960/50000 (81.84143222506394 Loss:1.5419095754623413)\n",
            "Training set:41600/50000 (83.12020460358056 Loss:1.3586727380752563)\n",
            "Training set:42240/50000 (84.39897698209718 Loss:1.4187226295471191)\n",
            "Training set:42880/50000 (85.67774936061382 Loss:1.296390414237976)\n",
            "Training set:43520/50000 (86.95652173913044 Loss:1.4304429292678833)\n",
            "Training set:44160/50000 (88.23529411764706 Loss:1.4881658554077148)\n",
            "Training set:44800/50000 (89.51406649616368 Loss:1.4153611660003662)\n",
            "Training set:45440/50000 (90.79283887468031 Loss:1.5089890956878662)\n",
            "Training set:46080/50000 (92.07161125319693 Loss:1.406556487083435)\n",
            "Training set:46720/50000 (93.35038363171356 Loss:1.5961707830429077)\n",
            "Training set:47360/50000 (94.62915601023018 Loss:1.4779566526412964)\n",
            "Training set:48000/50000 (95.9079283887468 Loss:1.4168791770935059)\n",
            "Training set:48640/50000 (97.18670076726343 Loss:1.4557771682739258)\n",
            "Training set:49280/50000 (98.46547314578005 Loss:1.5233707427978516)\n",
            "Training set:49920/50000 (99.74424552429667 Loss:1.3814697265625)\n",
            "Training set: Average loss: 1.570816\n",
            "Validation set: Average loss: 1.3533130086910952, Accuracy: 4883/10000 (48.83%)\n",
            "Epoch: 3\n",
            "Training set:0/50000 (0.0 Loss:1.2655866146087646)\n",
            "Training set:640/50000 (1.278772378516624 Loss:1.217647671699524)\n",
            "Training set:1280/50000 (2.557544757033248 Loss:1.5243241786956787)\n",
            "Training set:1920/50000 (3.836317135549872 Loss:1.422719120979309)\n",
            "Training set:2560/50000 (5.115089514066496 Loss:1.4217289686203003)\n",
            "Training set:3200/50000 (6.3938618925831205 Loss:1.6702909469604492)\n",
            "Training set:3840/50000 (7.672634271099744 Loss:1.3641427755355835)\n",
            "Training set:4480/50000 (8.951406649616368 Loss:1.1748838424682617)\n",
            "Training set:5120/50000 (10.230179028132993 Loss:1.3734112977981567)\n",
            "Training set:5760/50000 (11.508951406649617 Loss:1.3392586708068848)\n",
            "Training set:6400/50000 (12.787723785166241 Loss:1.383525013923645)\n",
            "Training set:7040/50000 (14.066496163682864 Loss:1.2445648908615112)\n",
            "Training set:7680/50000 (15.345268542199488 Loss:1.364000678062439)\n",
            "Training set:8320/50000 (16.624040920716112 Loss:1.3386811017990112)\n",
            "Training set:8960/50000 (17.902813299232736 Loss:1.2901033163070679)\n",
            "Training set:9600/50000 (19.18158567774936 Loss:1.1089184284210205)\n",
            "Training set:10240/50000 (20.460358056265985 Loss:1.117583990097046)\n",
            "Training set:10880/50000 (21.73913043478261 Loss:1.349594235420227)\n",
            "Training set:11520/50000 (23.017902813299234 Loss:1.1041195392608643)\n",
            "Training set:12160/50000 (24.296675191815858 Loss:1.1851048469543457)\n",
            "Training set:12800/50000 (25.575447570332482 Loss:1.2658551931381226)\n",
            "Training set:13440/50000 (26.854219948849106 Loss:1.3368501663208008)\n",
            "Training set:14080/50000 (28.132992327365727 Loss:1.3053525686264038)\n",
            "Training set:14720/50000 (29.41176470588235 Loss:1.3764657974243164)\n",
            "Training set:15360/50000 (30.690537084398976 Loss:1.3276540040969849)\n",
            "Training set:16000/50000 (31.9693094629156 Loss:1.1497395038604736)\n",
            "Training set:16640/50000 (33.248081841432224 Loss:1.2896790504455566)\n",
            "Training set:17280/50000 (34.52685421994885 Loss:1.2845158576965332)\n",
            "Training set:17920/50000 (35.80562659846547 Loss:1.3857507705688477)\n",
            "Training set:18560/50000 (37.084398976982094 Loss:1.4842840433120728)\n",
            "Training set:19200/50000 (38.36317135549872 Loss:1.3206095695495605)\n",
            "Training set:19840/50000 (39.64194373401534 Loss:1.3461275100708008)\n",
            "Training set:20480/50000 (40.92071611253197 Loss:1.1590991020202637)\n",
            "Training set:21120/50000 (42.19948849104859 Loss:1.3264524936676025)\n",
            "Training set:21760/50000 (43.47826086956522 Loss:1.0406094789505005)\n",
            "Training set:22400/50000 (44.75703324808184 Loss:1.2335509061813354)\n",
            "Training set:23040/50000 (46.03580562659847 Loss:1.3653571605682373)\n",
            "Training set:23680/50000 (47.31457800511509 Loss:1.217206358909607)\n",
            "Training set:24320/50000 (48.593350383631716 Loss:1.3156269788742065)\n",
            "Training set:24960/50000 (49.87212276214834 Loss:0.9745192527770996)\n",
            "Training set:25600/50000 (51.150895140664964 Loss:1.3231433629989624)\n",
            "Training set:26240/50000 (52.429667519181585 Loss:1.2747619152069092)\n",
            "Training set:26880/50000 (53.70843989769821 Loss:1.1876804828643799)\n",
            "Training set:27520/50000 (54.987212276214834 Loss:1.0399744510650635)\n",
            "Training set:28160/50000 (56.265984654731454 Loss:1.320224404335022)\n",
            "Training set:28800/50000 (57.54475703324808 Loss:0.968700110912323)\n",
            "Training set:29440/50000 (58.8235294117647 Loss:1.2900081872940063)\n",
            "Training set:30080/50000 (60.10230179028133 Loss:1.2449939250946045)\n",
            "Training set:30720/50000 (61.38107416879795 Loss:1.0634223222732544)\n",
            "Training set:31360/50000 (62.65984654731458 Loss:0.9076549410820007)\n",
            "Training set:32000/50000 (63.9386189258312 Loss:1.526465654373169)\n",
            "Training set:32640/50000 (65.21739130434783 Loss:1.1436207294464111)\n",
            "Training set:33280/50000 (66.49616368286445 Loss:1.1431013345718384)\n",
            "Training set:33920/50000 (67.77493606138107 Loss:0.9054362177848816)\n",
            "Training set:34560/50000 (69.0537084398977 Loss:1.1653589010238647)\n",
            "Training set:35200/50000 (70.33248081841433 Loss:1.042303442955017)\n",
            "Training set:35840/50000 (71.61125319693095 Loss:1.087568759918213)\n",
            "Training set:36480/50000 (72.89002557544757 Loss:1.178141474723816)\n",
            "Training set:37120/50000 (74.16879795396419 Loss:1.1395668983459473)\n",
            "Training set:37760/50000 (75.44757033248082 Loss:1.4344086647033691)\n",
            "Training set:38400/50000 (76.72634271099744 Loss:1.2568970918655396)\n",
            "Training set:39040/50000 (78.00511508951406 Loss:1.0495355129241943)\n",
            "Training set:39680/50000 (79.28388746803068 Loss:0.822733998298645)\n",
            "Training set:40320/50000 (80.56265984654732 Loss:0.8680538535118103)\n",
            "Training set:40960/50000 (81.84143222506394 Loss:0.9931559562683105)\n",
            "Training set:41600/50000 (83.12020460358056 Loss:1.4083459377288818)\n",
            "Training set:42240/50000 (84.39897698209718 Loss:1.3611823320388794)\n",
            "Training set:42880/50000 (85.67774936061382 Loss:1.1957017183303833)\n",
            "Training set:43520/50000 (86.95652173913044 Loss:1.223504662513733)\n",
            "Training set:44160/50000 (88.23529411764706 Loss:1.2539241313934326)\n",
            "Training set:44800/50000 (89.51406649616368 Loss:1.1628682613372803)\n",
            "Training set:45440/50000 (90.79283887468031 Loss:1.0481964349746704)\n",
            "Training set:46080/50000 (92.07161125319693 Loss:1.2000595331192017)\n",
            "Training set:46720/50000 (93.35038363171356 Loss:1.066436529159546)\n",
            "Training set:47360/50000 (94.62915601023018 Loss:1.2834303379058838)\n",
            "Training set:48000/50000 (95.9079283887468 Loss:1.2010068893432617)\n",
            "Training set:48640/50000 (97.18670076726343 Loss:1.1641535758972168)\n",
            "Training set:49280/50000 (98.46547314578005 Loss:1.210760474205017)\n",
            "Training set:49920/50000 (99.74424552429667 Loss:1.4189335107803345)\n",
            "Training set: Average loss: 1.240842\n",
            "Validation set: Average loss: 1.4575499079789325, Accuracy: 5277/10000 (52.77%)\n",
            "Epoch: 4\n",
            "Training set:0/50000 (0.0 Loss:1.0903584957122803)\n",
            "Training set:640/50000 (1.278772378516624 Loss:1.1077197790145874)\n",
            "Training set:1280/50000 (2.557544757033248 Loss:1.078949213027954)\n",
            "Training set:1920/50000 (3.836317135549872 Loss:0.9606202840805054)\n",
            "Training set:2560/50000 (5.115089514066496 Loss:0.9875761866569519)\n",
            "Training set:3200/50000 (6.3938618925831205 Loss:1.0201977491378784)\n",
            "Training set:3840/50000 (7.672634271099744 Loss:1.1016576290130615)\n",
            "Training set:4480/50000 (8.951406649616368 Loss:1.309056282043457)\n",
            "Training set:5120/50000 (10.230179028132993 Loss:0.9987776875495911)\n",
            "Training set:5760/50000 (11.508951406649617 Loss:1.1566287279129028)\n",
            "Training set:6400/50000 (12.787723785166241 Loss:1.160325288772583)\n",
            "Training set:7040/50000 (14.066496163682864 Loss:0.9980465769767761)\n",
            "Training set:7680/50000 (15.345268542199488 Loss:1.1352803707122803)\n",
            "Training set:8320/50000 (16.624040920716112 Loss:1.244937777519226)\n",
            "Training set:8960/50000 (17.902813299232736 Loss:0.9257560968399048)\n",
            "Training set:9600/50000 (19.18158567774936 Loss:1.1654495000839233)\n",
            "Training set:10240/50000 (20.460358056265985 Loss:0.8840777277946472)\n",
            "Training set:10880/50000 (21.73913043478261 Loss:1.151382327079773)\n",
            "Training set:11520/50000 (23.017902813299234 Loss:1.192400336265564)\n",
            "Training set:12160/50000 (24.296675191815858 Loss:0.9561269879341125)\n",
            "Training set:12800/50000 (25.575447570332482 Loss:1.0743683576583862)\n",
            "Training set:13440/50000 (26.854219948849106 Loss:1.2323416471481323)\n",
            "Training set:14080/50000 (28.132992327365727 Loss:1.110831379890442)\n",
            "Training set:14720/50000 (29.41176470588235 Loss:0.7632636427879333)\n",
            "Training set:15360/50000 (30.690537084398976 Loss:1.013698697090149)\n",
            "Training set:16000/50000 (31.9693094629156 Loss:1.375072956085205)\n",
            "Training set:16640/50000 (33.248081841432224 Loss:1.14400053024292)\n",
            "Training set:17280/50000 (34.52685421994885 Loss:0.9401170611381531)\n",
            "Training set:17920/50000 (35.80562659846547 Loss:1.189874291419983)\n",
            "Training set:18560/50000 (37.084398976982094 Loss:1.0278719663619995)\n",
            "Training set:19200/50000 (38.36317135549872 Loss:0.8537102937698364)\n",
            "Training set:19840/50000 (39.64194373401534 Loss:1.326469898223877)\n",
            "Training set:20480/50000 (40.92071611253197 Loss:0.9787615537643433)\n",
            "Training set:21120/50000 (42.19948849104859 Loss:1.0617470741271973)\n",
            "Training set:21760/50000 (43.47826086956522 Loss:0.9900599122047424)\n",
            "Training set:22400/50000 (44.75703324808184 Loss:1.3402272462844849)\n",
            "Training set:23040/50000 (46.03580562659847 Loss:0.7919061779975891)\n",
            "Training set:23680/50000 (47.31457800511509 Loss:1.0623568296432495)\n",
            "Training set:24320/50000 (48.593350383631716 Loss:1.0077074766159058)\n",
            "Training set:24960/50000 (49.87212276214834 Loss:0.9409171938896179)\n",
            "Training set:25600/50000 (51.150895140664964 Loss:1.1739599704742432)\n",
            "Training set:26240/50000 (52.429667519181585 Loss:0.9693686366081238)\n",
            "Training set:26880/50000 (53.70843989769821 Loss:0.9314420223236084)\n",
            "Training set:27520/50000 (54.987212276214834 Loss:0.9570059776306152)\n",
            "Training set:28160/50000 (56.265984654731454 Loss:1.111324429512024)\n",
            "Training set:28800/50000 (57.54475703324808 Loss:0.9097479581832886)\n",
            "Training set:29440/50000 (58.8235294117647 Loss:0.8640044331550598)\n",
            "Training set:30080/50000 (60.10230179028133 Loss:0.88326096534729)\n",
            "Training set:30720/50000 (61.38107416879795 Loss:0.88824862241745)\n",
            "Training set:31360/50000 (62.65984654731458 Loss:1.100305199623108)\n",
            "Training set:32000/50000 (63.9386189258312 Loss:1.0384869575500488)\n",
            "Training set:32640/50000 (65.21739130434783 Loss:0.8505211472511292)\n",
            "Training set:33280/50000 (66.49616368286445 Loss:1.110708236694336)\n",
            "Training set:33920/50000 (67.77493606138107 Loss:1.011963963508606)\n",
            "Training set:34560/50000 (69.0537084398977 Loss:1.2731167078018188)\n",
            "Training set:35200/50000 (70.33248081841433 Loss:1.1330037117004395)\n",
            "Training set:35840/50000 (71.61125319693095 Loss:1.3837523460388184)\n",
            "Training set:36480/50000 (72.89002557544757 Loss:0.9826967716217041)\n",
            "Training set:37120/50000 (74.16879795396419 Loss:0.8693057298660278)\n",
            "Training set:37760/50000 (75.44757033248082 Loss:0.8280534744262695)\n",
            "Training set:38400/50000 (76.72634271099744 Loss:0.9016420245170593)\n",
            "Training set:39040/50000 (78.00511508951406 Loss:0.9866344928741455)\n",
            "Training set:39680/50000 (79.28388746803068 Loss:0.9970788359642029)\n",
            "Training set:40320/50000 (80.56265984654732 Loss:0.9967570304870605)\n",
            "Training set:40960/50000 (81.84143222506394 Loss:0.7932949662208557)\n",
            "Training set:41600/50000 (83.12020460358056 Loss:1.1175142526626587)\n",
            "Training set:42240/50000 (84.39897698209718 Loss:0.9006949663162231)\n",
            "Training set:42880/50000 (85.67774936061382 Loss:0.7580295205116272)\n",
            "Training set:43520/50000 (86.95652173913044 Loss:1.2570879459381104)\n",
            "Training set:44160/50000 (88.23529411764706 Loss:0.9277368187904358)\n",
            "Training set:44800/50000 (89.51406649616368 Loss:0.9521414041519165)\n",
            "Training set:45440/50000 (90.79283887468031 Loss:0.8882464170455933)\n",
            "Training set:46080/50000 (92.07161125319693 Loss:0.9018017053604126)\n",
            "Training set:46720/50000 (93.35038363171356 Loss:0.7728585600852966)\n",
            "Training set:47360/50000 (94.62915601023018 Loss:0.9259560108184814)\n",
            "Training set:48000/50000 (95.9079283887468 Loss:0.949604868888855)\n",
            "Training set:48640/50000 (97.18670076726343 Loss:1.0443366765975952)\n",
            "Training set:49280/50000 (98.46547314578005 Loss:0.9358587265014648)\n",
            "Training set:49920/50000 (99.74424552429667 Loss:0.8070265054702759)\n",
            "Training set: Average loss: 1.030937\n",
            "Validation set: Average loss: 0.8854281594798823, Accuracy: 6935/10000 (69.35%)\n",
            "Epoch: 5\n",
            "Training set:0/50000 (0.0 Loss:0.6960855722427368)\n",
            "Training set:640/50000 (1.278772378516624 Loss:0.8176729083061218)\n",
            "Training set:1280/50000 (2.557544757033248 Loss:1.1584396362304688)\n",
            "Training set:1920/50000 (3.836317135549872 Loss:1.1436512470245361)\n",
            "Training set:2560/50000 (5.115089514066496 Loss:1.040439486503601)\n",
            "Training set:3200/50000 (6.3938618925831205 Loss:0.8255677223205566)\n",
            "Training set:3840/50000 (7.672634271099744 Loss:1.0981594324111938)\n",
            "Training set:4480/50000 (8.951406649616368 Loss:0.8491129279136658)\n",
            "Training set:5120/50000 (10.230179028132993 Loss:0.9860463738441467)\n",
            "Training set:5760/50000 (11.508951406649617 Loss:1.1489765644073486)\n",
            "Training set:6400/50000 (12.787723785166241 Loss:1.089980125427246)\n",
            "Training set:7040/50000 (14.066496163682864 Loss:0.9966678023338318)\n",
            "Training set:7680/50000 (15.345268542199488 Loss:1.0835696458816528)\n",
            "Training set:8320/50000 (16.624040920716112 Loss:0.9838060736656189)\n",
            "Training set:8960/50000 (17.902813299232736 Loss:0.7914814949035645)\n",
            "Training set:9600/50000 (19.18158567774936 Loss:1.0475597381591797)\n",
            "Training set:10240/50000 (20.460358056265985 Loss:0.689399242401123)\n",
            "Training set:10880/50000 (21.73913043478261 Loss:1.1024580001831055)\n",
            "Training set:11520/50000 (23.017902813299234 Loss:1.0199432373046875)\n",
            "Training set:12160/50000 (24.296675191815858 Loss:0.8531205654144287)\n",
            "Training set:12800/50000 (25.575447570332482 Loss:1.072076678276062)\n",
            "Training set:13440/50000 (26.854219948849106 Loss:0.6472327709197998)\n",
            "Training set:14080/50000 (28.132992327365727 Loss:0.9804927706718445)\n",
            "Training set:14720/50000 (29.41176470588235 Loss:0.6094181537628174)\n",
            "Training set:15360/50000 (30.690537084398976 Loss:0.8328582048416138)\n",
            "Training set:16000/50000 (31.9693094629156 Loss:0.9546210169792175)\n",
            "Training set:16640/50000 (33.248081841432224 Loss:0.9853262305259705)\n",
            "Training set:17280/50000 (34.52685421994885 Loss:0.9397566914558411)\n",
            "Training set:17920/50000 (35.80562659846547 Loss:0.7645953297615051)\n",
            "Training set:18560/50000 (37.084398976982094 Loss:0.5603274703025818)\n",
            "Training set:19200/50000 (38.36317135549872 Loss:1.3192869424819946)\n",
            "Training set:19840/50000 (39.64194373401534 Loss:0.8131922483444214)\n",
            "Training set:20480/50000 (40.92071611253197 Loss:0.6807702779769897)\n",
            "Training set:21120/50000 (42.19948849104859 Loss:0.771773636341095)\n",
            "Training set:21760/50000 (43.47826086956522 Loss:0.9487965106964111)\n",
            "Training set:22400/50000 (44.75703324808184 Loss:0.8440691828727722)\n",
            "Training set:23040/50000 (46.03580562659847 Loss:1.1819019317626953)\n",
            "Training set:23680/50000 (47.31457800511509 Loss:0.9995430111885071)\n",
            "Training set:24320/50000 (48.593350383631716 Loss:1.220353126525879)\n",
            "Training set:24960/50000 (49.87212276214834 Loss:0.6742132306098938)\n",
            "Training set:25600/50000 (51.150895140664964 Loss:0.8095548748970032)\n",
            "Training set:26240/50000 (52.429667519181585 Loss:0.7792615294456482)\n",
            "Training set:26880/50000 (53.70843989769821 Loss:0.8439865112304688)\n",
            "Training set:27520/50000 (54.987212276214834 Loss:0.872552216053009)\n",
            "Training set:28160/50000 (56.265984654731454 Loss:0.9985892176628113)\n",
            "Training set:28800/50000 (57.54475703324808 Loss:0.9681427478790283)\n",
            "Training set:29440/50000 (58.8235294117647 Loss:1.1580432653427124)\n",
            "Training set:30080/50000 (60.10230179028133 Loss:1.0042346715927124)\n",
            "Training set:30720/50000 (61.38107416879795 Loss:1.2229621410369873)\n",
            "Training set:31360/50000 (62.65984654731458 Loss:0.894404947757721)\n",
            "Training set:32000/50000 (63.9386189258312 Loss:0.8831620812416077)\n",
            "Training set:32640/50000 (65.21739130434783 Loss:0.8645756840705872)\n",
            "Training set:33280/50000 (66.49616368286445 Loss:0.944749653339386)\n",
            "Training set:33920/50000 (67.77493606138107 Loss:1.01040518283844)\n",
            "Training set:34560/50000 (69.0537084398977 Loss:0.9588233828544617)\n",
            "Training set:35200/50000 (70.33248081841433 Loss:0.8589158654212952)\n",
            "Training set:35840/50000 (71.61125319693095 Loss:0.8919176459312439)\n",
            "Training set:36480/50000 (72.89002557544757 Loss:0.9880717396736145)\n",
            "Training set:37120/50000 (74.16879795396419 Loss:0.7026325464248657)\n",
            "Training set:37760/50000 (75.44757033248082 Loss:0.9282575845718384)\n",
            "Training set:38400/50000 (76.72634271099744 Loss:0.8985932469367981)\n",
            "Training set:39040/50000 (78.00511508951406 Loss:0.6055869460105896)\n",
            "Training set:39680/50000 (79.28388746803068 Loss:0.7576999664306641)\n",
            "Training set:40320/50000 (80.56265984654732 Loss:0.9366660118103027)\n",
            "Training set:40960/50000 (81.84143222506394 Loss:0.7212584614753723)\n",
            "Training set:41600/50000 (83.12020460358056 Loss:0.9675914645195007)\n",
            "Training set:42240/50000 (84.39897698209718 Loss:0.9565908908843994)\n",
            "Training set:42880/50000 (85.67774936061382 Loss:0.8009524941444397)\n",
            "Training set:43520/50000 (86.95652173913044 Loss:0.9198999404907227)\n",
            "Training set:44160/50000 (88.23529411764706 Loss:0.6920998692512512)\n",
            "Training set:44800/50000 (89.51406649616368 Loss:0.9424930810928345)\n",
            "Training set:45440/50000 (90.79283887468031 Loss:0.69875568151474)\n",
            "Training set:46080/50000 (92.07161125319693 Loss:0.6410078406333923)\n",
            "Training set:46720/50000 (93.35038363171356 Loss:0.9314391016960144)\n",
            "Training set:47360/50000 (94.62915601023018 Loss:0.7352859377861023)\n",
            "Training set:48000/50000 (95.9079283887468 Loss:0.9128351211547852)\n",
            "Training set:48640/50000 (97.18670076726343 Loss:0.8982880115509033)\n",
            "Training set:49280/50000 (98.46547314578005 Loss:1.0423725843429565)\n",
            "Training set:49920/50000 (99.74424552429667 Loss:0.771903932094574)\n",
            "Training set: Average loss: 0.906774\n",
            "Validation set: Average loss: 0.9251393129111855, Accuracy: 6880/10000 (68.8%)\n",
            "Epoch: 6\n",
            "Training set:0/50000 (0.0 Loss:0.6911457777023315)\n",
            "Training set:640/50000 (1.278772378516624 Loss:1.029457688331604)\n",
            "Training set:1280/50000 (2.557544757033248 Loss:1.174900770187378)\n",
            "Training set:1920/50000 (3.836317135549872 Loss:0.8077141046524048)\n",
            "Training set:2560/50000 (5.115089514066496 Loss:0.9191250205039978)\n",
            "Training set:3200/50000 (6.3938618925831205 Loss:0.7244954109191895)\n",
            "Training set:3840/50000 (7.672634271099744 Loss:0.6199413537979126)\n",
            "Training set:4480/50000 (8.951406649616368 Loss:0.7311019897460938)\n",
            "Training set:5120/50000 (10.230179028132993 Loss:0.9940841197967529)\n",
            "Training set:5760/50000 (11.508951406649617 Loss:0.7480900883674622)\n",
            "Training set:6400/50000 (12.787723785166241 Loss:0.6192444562911987)\n",
            "Training set:7040/50000 (14.066496163682864 Loss:0.9176108837127686)\n",
            "Training set:7680/50000 (15.345268542199488 Loss:0.5960817933082581)\n",
            "Training set:8320/50000 (16.624040920716112 Loss:0.728482186794281)\n",
            "Training set:8960/50000 (17.902813299232736 Loss:0.7409672737121582)\n",
            "Training set:9600/50000 (19.18158567774936 Loss:0.7736508846282959)\n",
            "Training set:10240/50000 (20.460358056265985 Loss:0.9807007312774658)\n",
            "Training set:10880/50000 (21.73913043478261 Loss:0.7121805548667908)\n",
            "Training set:11520/50000 (23.017902813299234 Loss:1.0645209550857544)\n",
            "Training set:12160/50000 (24.296675191815858 Loss:0.7935094237327576)\n",
            "Training set:12800/50000 (25.575447570332482 Loss:0.8947179317474365)\n",
            "Training set:13440/50000 (26.854219948849106 Loss:0.6372222304344177)\n",
            "Training set:14080/50000 (28.132992327365727 Loss:1.0298813581466675)\n",
            "Training set:14720/50000 (29.41176470588235 Loss:0.6357691287994385)\n",
            "Training set:15360/50000 (30.690537084398976 Loss:0.8899803757667542)\n",
            "Training set:16000/50000 (31.9693094629156 Loss:0.6021990776062012)\n",
            "Training set:16640/50000 (33.248081841432224 Loss:0.7661651372909546)\n",
            "Training set:17280/50000 (34.52685421994885 Loss:0.6455833911895752)\n",
            "Training set:17920/50000 (35.80562659846547 Loss:0.7578852772712708)\n",
            "Training set:18560/50000 (37.084398976982094 Loss:0.9387092590332031)\n",
            "Training set:19200/50000 (38.36317135549872 Loss:1.046776533126831)\n",
            "Training set:19840/50000 (39.64194373401534 Loss:0.7833105325698853)\n",
            "Training set:20480/50000 (40.92071611253197 Loss:0.7520259022712708)\n",
            "Training set:21120/50000 (42.19948849104859 Loss:0.7140035033226013)\n",
            "Training set:21760/50000 (43.47826086956522 Loss:0.6899420619010925)\n",
            "Training set:22400/50000 (44.75703324808184 Loss:1.1463840007781982)\n",
            "Training set:23040/50000 (46.03580562659847 Loss:0.774310290813446)\n",
            "Training set:23680/50000 (47.31457800511509 Loss:0.9911422729492188)\n",
            "Training set:24320/50000 (48.593350383631716 Loss:1.158492088317871)\n",
            "Training set:24960/50000 (49.87212276214834 Loss:0.7555050253868103)\n",
            "Training set:25600/50000 (51.150895140664964 Loss:0.5044691562652588)\n",
            "Training set:26240/50000 (52.429667519181585 Loss:0.6903896331787109)\n",
            "Training set:26880/50000 (53.70843989769821 Loss:0.9011430144309998)\n",
            "Training set:27520/50000 (54.987212276214834 Loss:0.8353210687637329)\n",
            "Training set:28160/50000 (56.265984654731454 Loss:0.8801153898239136)\n",
            "Training set:28800/50000 (57.54475703324808 Loss:0.5824302434921265)\n",
            "Training set:29440/50000 (58.8235294117647 Loss:0.6633639931678772)\n",
            "Training set:30080/50000 (60.10230179028133 Loss:1.3607122898101807)\n",
            "Training set:30720/50000 (61.38107416879795 Loss:0.7780528664588928)\n",
            "Training set:31360/50000 (62.65984654731458 Loss:0.7985867857933044)\n",
            "Training set:32000/50000 (63.9386189258312 Loss:0.8330344557762146)\n",
            "Training set:32640/50000 (65.21739130434783 Loss:0.7837557196617126)\n",
            "Training set:33280/50000 (66.49616368286445 Loss:0.8263794183731079)\n",
            "Training set:33920/50000 (67.77493606138107 Loss:0.9715042114257812)\n",
            "Training set:34560/50000 (69.0537084398977 Loss:0.6909294724464417)\n",
            "Training set:35200/50000 (70.33248081841433 Loss:0.6487568020820618)\n",
            "Training set:35840/50000 (71.61125319693095 Loss:0.7240272760391235)\n",
            "Training set:36480/50000 (72.89002557544757 Loss:0.5658222436904907)\n",
            "Training set:37120/50000 (74.16879795396419 Loss:0.7863921523094177)\n",
            "Training set:37760/50000 (75.44757033248082 Loss:0.9211344718933105)\n",
            "Training set:38400/50000 (76.72634271099744 Loss:0.7721249461174011)\n",
            "Training set:39040/50000 (78.00511508951406 Loss:0.8448165059089661)\n",
            "Training set:39680/50000 (79.28388746803068 Loss:0.7947168946266174)\n",
            "Training set:40320/50000 (80.56265984654732 Loss:0.7581579089164734)\n",
            "Training set:40960/50000 (81.84143222506394 Loss:0.8114649057388306)\n",
            "Training set:41600/50000 (83.12020460358056 Loss:0.8953526020050049)\n",
            "Training set:42240/50000 (84.39897698209718 Loss:0.8020508885383606)\n",
            "Training set:42880/50000 (85.67774936061382 Loss:0.7281799912452698)\n",
            "Training set:43520/50000 (86.95652173913044 Loss:1.0093655586242676)\n",
            "Training set:44160/50000 (88.23529411764706 Loss:0.6809073686599731)\n",
            "Training set:44800/50000 (89.51406649616368 Loss:0.7740031480789185)\n",
            "Training set:45440/50000 (90.79283887468031 Loss:0.79023277759552)\n",
            "Training set:46080/50000 (92.07161125319693 Loss:0.8436492085456848)\n",
            "Training set:46720/50000 (93.35038363171356 Loss:1.0156652927398682)\n",
            "Training set:47360/50000 (94.62915601023018 Loss:0.9084137082099915)\n",
            "Training set:48000/50000 (95.9079283887468 Loss:0.626048743724823)\n",
            "Training set:48640/50000 (97.18670076726343 Loss:0.6492692232131958)\n",
            "Training set:49280/50000 (98.46547314578005 Loss:0.5741145610809326)\n",
            "Training set:49920/50000 (99.74424552429667 Loss:0.8365516066551208)\n",
            "Training set: Average loss: 0.812376\n",
            "Validation set: Average loss: 0.7610746341146482, Accuracy: 7455/10000 (74.55%)\n",
            "Epoch: 7\n",
            "Training set:0/50000 (0.0 Loss:0.8543461561203003)\n",
            "Training set:640/50000 (1.278772378516624 Loss:0.6083921790122986)\n",
            "Training set:1280/50000 (2.557544757033248 Loss:0.41534608602523804)\n",
            "Training set:1920/50000 (3.836317135549872 Loss:0.9958783388137817)\n",
            "Training set:2560/50000 (5.115089514066496 Loss:0.6512740850448608)\n",
            "Training set:3200/50000 (6.3938618925831205 Loss:0.8131327629089355)\n",
            "Training set:3840/50000 (7.672634271099744 Loss:0.6547926664352417)\n",
            "Training set:4480/50000 (8.951406649616368 Loss:0.774125337600708)\n",
            "Training set:5120/50000 (10.230179028132993 Loss:0.6904131770133972)\n",
            "Training set:5760/50000 (11.508951406649617 Loss:0.6757471561431885)\n",
            "Training set:6400/50000 (12.787723785166241 Loss:0.5319678783416748)\n",
            "Training set:7040/50000 (14.066496163682864 Loss:0.5775284767150879)\n",
            "Training set:7680/50000 (15.345268542199488 Loss:0.7187658548355103)\n",
            "Training set:8320/50000 (16.624040920716112 Loss:1.2310709953308105)\n",
            "Training set:8960/50000 (17.902813299232736 Loss:0.7898315191268921)\n",
            "Training set:9600/50000 (19.18158567774936 Loss:0.7077590823173523)\n",
            "Training set:10240/50000 (20.460358056265985 Loss:0.7047423124313354)\n",
            "Training set:10880/50000 (21.73913043478261 Loss:0.563696026802063)\n",
            "Training set:11520/50000 (23.017902813299234 Loss:0.8511763215065002)\n",
            "Training set:12160/50000 (24.296675191815858 Loss:0.5294010639190674)\n",
            "Training set:12800/50000 (25.575447570332482 Loss:0.9804389476776123)\n",
            "Training set:13440/50000 (26.854219948849106 Loss:0.5155668258666992)\n",
            "Training set:14080/50000 (28.132992327365727 Loss:0.5601125955581665)\n",
            "Training set:14720/50000 (29.41176470588235 Loss:0.5689257383346558)\n",
            "Training set:15360/50000 (30.690537084398976 Loss:0.7577545642852783)\n",
            "Training set:16000/50000 (31.9693094629156 Loss:0.6128257513046265)\n",
            "Training set:16640/50000 (33.248081841432224 Loss:0.8211143612861633)\n",
            "Training set:17280/50000 (34.52685421994885 Loss:1.0066821575164795)\n",
            "Training set:17920/50000 (35.80562659846547 Loss:0.8588999509811401)\n",
            "Training set:18560/50000 (37.084398976982094 Loss:0.7385165095329285)\n",
            "Training set:19200/50000 (38.36317135549872 Loss:0.7579615712165833)\n",
            "Training set:19840/50000 (39.64194373401534 Loss:0.967718780040741)\n",
            "Training set:20480/50000 (40.92071611253197 Loss:0.5853886604309082)\n",
            "Training set:21120/50000 (42.19948849104859 Loss:0.7017725110054016)\n",
            "Training set:21760/50000 (43.47826086956522 Loss:0.7013271450996399)\n",
            "Training set:22400/50000 (44.75703324808184 Loss:0.9922683238983154)\n",
            "Training set:23040/50000 (46.03580562659847 Loss:0.9224376678466797)\n",
            "Training set:23680/50000 (47.31457800511509 Loss:0.7545609474182129)\n",
            "Training set:24320/50000 (48.593350383631716 Loss:0.7319445013999939)\n",
            "Training set:24960/50000 (49.87212276214834 Loss:0.7665968537330627)\n",
            "Training set:25600/50000 (51.150895140664964 Loss:0.7810440063476562)\n",
            "Training set:26240/50000 (52.429667519181585 Loss:0.7343104481697083)\n",
            "Training set:26880/50000 (53.70843989769821 Loss:0.7950250506401062)\n",
            "Training set:27520/50000 (54.987212276214834 Loss:0.8587282299995422)\n",
            "Training set:28160/50000 (56.265984654731454 Loss:0.920730710029602)\n",
            "Training set:28800/50000 (57.54475703324808 Loss:0.8489600419998169)\n",
            "Training set:29440/50000 (58.8235294117647 Loss:0.6626894474029541)\n",
            "Training set:30080/50000 (60.10230179028133 Loss:0.5668197274208069)\n",
            "Training set:30720/50000 (61.38107416879795 Loss:0.48505234718322754)\n",
            "Training set:31360/50000 (62.65984654731458 Loss:0.7833539247512817)\n",
            "Training set:32000/50000 (63.9386189258312 Loss:0.7862833738327026)\n",
            "Training set:32640/50000 (65.21739130434783 Loss:0.6417856216430664)\n",
            "Training set:33280/50000 (66.49616368286445 Loss:0.6805788278579712)\n",
            "Training set:33920/50000 (67.77493606138107 Loss:0.7473935484886169)\n",
            "Training set:34560/50000 (69.0537084398977 Loss:0.7595387101173401)\n",
            "Training set:35200/50000 (70.33248081841433 Loss:0.878934919834137)\n",
            "Training set:35840/50000 (71.61125319693095 Loss:0.5743668675422668)\n",
            "Training set:36480/50000 (72.89002557544757 Loss:0.5848356485366821)\n",
            "Training set:37120/50000 (74.16879795396419 Loss:0.7199777364730835)\n",
            "Training set:37760/50000 (75.44757033248082 Loss:0.685539186000824)\n",
            "Training set:38400/50000 (76.72634271099744 Loss:0.43265122175216675)\n",
            "Training set:39040/50000 (78.00511508951406 Loss:0.5515127182006836)\n",
            "Training set:39680/50000 (79.28388746803068 Loss:0.6079897284507751)\n",
            "Training set:40320/50000 (80.56265984654732 Loss:0.8555779457092285)\n",
            "Training set:40960/50000 (81.84143222506394 Loss:0.7581454515457153)\n",
            "Training set:41600/50000 (83.12020460358056 Loss:0.6892890334129333)\n",
            "Training set:42240/50000 (84.39897698209718 Loss:0.7536296248435974)\n",
            "Training set:42880/50000 (85.67774936061382 Loss:0.5614716410636902)\n",
            "Training set:43520/50000 (86.95652173913044 Loss:0.6077061295509338)\n",
            "Training set:44160/50000 (88.23529411764706 Loss:0.6973175406455994)\n",
            "Training set:44800/50000 (89.51406649616368 Loss:0.9060556888580322)\n",
            "Training set:45440/50000 (90.79283887468031 Loss:0.8888968229293823)\n",
            "Training set:46080/50000 (92.07161125319693 Loss:0.9000772833824158)\n",
            "Training set:46720/50000 (93.35038363171356 Loss:0.6719158291816711)\n",
            "Training set:47360/50000 (94.62915601023018 Loss:0.6231436729431152)\n",
            "Training set:48000/50000 (95.9079283887468 Loss:0.7013099789619446)\n",
            "Training set:48640/50000 (97.18670076726343 Loss:0.8291856050491333)\n",
            "Training set:49280/50000 (98.46547314578005 Loss:0.5681232810020447)\n",
            "Training set:49920/50000 (99.74424552429667 Loss:0.7371463775634766)\n",
            "Training set: Average loss: 0.749522\n",
            "Validation set: Average loss: 0.6845420384482973, Accuracy: 7740/10000 (77.4%)\n",
            "Epoch: 8\n",
            "Training set:0/50000 (0.0 Loss:0.48143020272254944)\n",
            "Training set:640/50000 (1.278772378516624 Loss:0.5480177402496338)\n",
            "Training set:1280/50000 (2.557544757033248 Loss:0.6311262845993042)\n",
            "Training set:1920/50000 (3.836317135549872 Loss:0.6160886883735657)\n",
            "Training set:2560/50000 (5.115089514066496 Loss:0.7256423830986023)\n",
            "Training set:3200/50000 (6.3938618925831205 Loss:0.7118946313858032)\n",
            "Training set:3840/50000 (7.672634271099744 Loss:0.7298071980476379)\n",
            "Training set:4480/50000 (8.951406649616368 Loss:0.6363214254379272)\n",
            "Training set:5120/50000 (10.230179028132993 Loss:0.5688486695289612)\n",
            "Training set:5760/50000 (11.508951406649617 Loss:0.6540061235427856)\n",
            "Training set:6400/50000 (12.787723785166241 Loss:0.4883650839328766)\n",
            "Training set:7040/50000 (14.066496163682864 Loss:0.6751477718353271)\n",
            "Training set:7680/50000 (15.345268542199488 Loss:0.6229748129844666)\n",
            "Training set:8320/50000 (16.624040920716112 Loss:0.780931293964386)\n",
            "Training set:8960/50000 (17.902813299232736 Loss:0.6178476810455322)\n",
            "Training set:9600/50000 (19.18158567774936 Loss:0.5386867523193359)\n",
            "Training set:10240/50000 (20.460358056265985 Loss:0.5268877148628235)\n",
            "Training set:10880/50000 (21.73913043478261 Loss:0.5419728755950928)\n",
            "Training set:11520/50000 (23.017902813299234 Loss:0.6422361135482788)\n",
            "Training set:12160/50000 (24.296675191815858 Loss:0.6613428592681885)\n",
            "Training set:12800/50000 (25.575447570332482 Loss:0.6172323226928711)\n",
            "Training set:13440/50000 (26.854219948849106 Loss:0.7206986546516418)\n",
            "Training set:14080/50000 (28.132992327365727 Loss:0.5700599551200867)\n",
            "Training set:14720/50000 (29.41176470588235 Loss:0.5793259739875793)\n",
            "Training set:15360/50000 (30.690537084398976 Loss:0.5908149480819702)\n",
            "Training set:16000/50000 (31.9693094629156 Loss:0.6553611159324646)\n",
            "Training set:16640/50000 (33.248081841432224 Loss:0.8011322617530823)\n",
            "Training set:17280/50000 (34.52685421994885 Loss:0.8227270841598511)\n",
            "Training set:17920/50000 (35.80562659846547 Loss:0.49423491954803467)\n",
            "Training set:18560/50000 (37.084398976982094 Loss:0.7013331651687622)\n",
            "Training set:19200/50000 (38.36317135549872 Loss:0.6770366430282593)\n",
            "Training set:19840/50000 (39.64194373401534 Loss:0.5737299919128418)\n",
            "Training set:20480/50000 (40.92071611253197 Loss:0.44775980710983276)\n",
            "Training set:21120/50000 (42.19948849104859 Loss:0.7891476154327393)\n",
            "Training set:21760/50000 (43.47826086956522 Loss:0.7714166045188904)\n",
            "Training set:22400/50000 (44.75703324808184 Loss:0.5261514186859131)\n",
            "Training set:23040/50000 (46.03580562659847 Loss:0.740963339805603)\n",
            "Training set:23680/50000 (47.31457800511509 Loss:0.6062281131744385)\n",
            "Training set:24320/50000 (48.593350383631716 Loss:0.6576322913169861)\n",
            "Training set:24960/50000 (49.87212276214834 Loss:0.6850362420082092)\n",
            "Training set:25600/50000 (51.150895140664964 Loss:0.7319003939628601)\n",
            "Training set:26240/50000 (52.429667519181585 Loss:0.7159368395805359)\n",
            "Training set:26880/50000 (53.70843989769821 Loss:0.6015834212303162)\n",
            "Training set:27520/50000 (54.987212276214834 Loss:0.9436876177787781)\n",
            "Training set:28160/50000 (56.265984654731454 Loss:0.5759509205818176)\n",
            "Training set:28800/50000 (57.54475703324808 Loss:1.0851166248321533)\n",
            "Training set:29440/50000 (58.8235294117647 Loss:0.7178442478179932)\n",
            "Training set:30080/50000 (60.10230179028133 Loss:0.5848334431648254)\n",
            "Training set:30720/50000 (61.38107416879795 Loss:0.4822314977645874)\n",
            "Training set:31360/50000 (62.65984654731458 Loss:0.6765093207359314)\n",
            "Training set:32000/50000 (63.9386189258312 Loss:0.6130120158195496)\n",
            "Training set:32640/50000 (65.21739130434783 Loss:0.6625947952270508)\n",
            "Training set:33280/50000 (66.49616368286445 Loss:0.7030402421951294)\n",
            "Training set:33920/50000 (67.77493606138107 Loss:0.6805200576782227)\n",
            "Training set:34560/50000 (69.0537084398977 Loss:0.8310781717300415)\n",
            "Training set:35200/50000 (70.33248081841433 Loss:0.7534902095794678)\n",
            "Training set:35840/50000 (71.61125319693095 Loss:0.6361861824989319)\n",
            "Training set:36480/50000 (72.89002557544757 Loss:0.7096043229103088)\n",
            "Training set:37120/50000 (74.16879795396419 Loss:0.529916524887085)\n",
            "Training set:37760/50000 (75.44757033248082 Loss:0.7163798213005066)\n",
            "Training set:38400/50000 (76.72634271099744 Loss:0.6496964693069458)\n",
            "Training set:39040/50000 (78.00511508951406 Loss:0.8737185001373291)\n",
            "Training set:39680/50000 (79.28388746803068 Loss:0.8695258498191833)\n",
            "Training set:40320/50000 (80.56265984654732 Loss:0.620760440826416)\n",
            "Training set:40960/50000 (81.84143222506394 Loss:0.5489717125892639)\n",
            "Training set:41600/50000 (83.12020460358056 Loss:0.5354799628257751)\n",
            "Training set:42240/50000 (84.39897698209718 Loss:0.5710662007331848)\n",
            "Training set:42880/50000 (85.67774936061382 Loss:0.4676859974861145)\n",
            "Training set:43520/50000 (86.95652173913044 Loss:0.7580525279045105)\n",
            "Training set:44160/50000 (88.23529411764706 Loss:0.8689286112785339)\n",
            "Training set:44800/50000 (89.51406649616368 Loss:0.5491161942481995)\n",
            "Training set:45440/50000 (90.79283887468031 Loss:0.7603321075439453)\n",
            "Training set:46080/50000 (92.07161125319693 Loss:0.6234413385391235)\n",
            "Training set:46720/50000 (93.35038363171356 Loss:0.7162643074989319)\n",
            "Training set:47360/50000 (94.62915601023018 Loss:0.5727354884147644)\n",
            "Training set:48000/50000 (95.9079283887468 Loss:0.6883232593536377)\n",
            "Training set:48640/50000 (97.18670076726343 Loss:0.5379543304443359)\n",
            "Training set:49280/50000 (98.46547314578005 Loss:0.7092448472976685)\n",
            "Training set:49920/50000 (99.74424552429667 Loss:0.7777494192123413)\n",
            "Training set: Average loss: 0.691695\n",
            "Validation set: Average loss: 0.8236311508971415, Accuracy: 7420/10000 (74.2%)\n",
            "Epoch: 9\n",
            "Training set:0/50000 (0.0 Loss:0.623365581035614)\n",
            "Training set:640/50000 (1.278772378516624 Loss:0.5293312668800354)\n",
            "Training set:1280/50000 (2.557544757033248 Loss:0.5419993996620178)\n",
            "Training set:1920/50000 (3.836317135549872 Loss:0.8086447715759277)\n",
            "Training set:2560/50000 (5.115089514066496 Loss:0.7424648404121399)\n",
            "Training set:3200/50000 (6.3938618925831205 Loss:0.6388194561004639)\n",
            "Training set:3840/50000 (7.672634271099744 Loss:0.7940630912780762)\n",
            "Training set:4480/50000 (8.951406649616368 Loss:0.45120498538017273)\n",
            "Training set:5120/50000 (10.230179028132993 Loss:0.4041316509246826)\n",
            "Training set:5760/50000 (11.508951406649617 Loss:0.767373263835907)\n",
            "Training set:6400/50000 (12.787723785166241 Loss:0.6300795078277588)\n",
            "Training set:7040/50000 (14.066496163682864 Loss:0.8594021201133728)\n",
            "Training set:7680/50000 (15.345268542199488 Loss:0.6115961670875549)\n",
            "Training set:8320/50000 (16.624040920716112 Loss:0.641259491443634)\n",
            "Training set:8960/50000 (17.902813299232736 Loss:0.5856438279151917)\n",
            "Training set:9600/50000 (19.18158567774936 Loss:0.7566449642181396)\n",
            "Training set:10240/50000 (20.460358056265985 Loss:0.4743031859397888)\n",
            "Training set:10880/50000 (21.73913043478261 Loss:0.7334734201431274)\n",
            "Training set:11520/50000 (23.017902813299234 Loss:0.49632006883621216)\n",
            "Training set:12160/50000 (24.296675191815858 Loss:0.45379140973091125)\n",
            "Training set:12800/50000 (25.575447570332482 Loss:0.5625208020210266)\n",
            "Training set:13440/50000 (26.854219948849106 Loss:0.49175041913986206)\n",
            "Training set:14080/50000 (28.132992327365727 Loss:0.7407694458961487)\n",
            "Training set:14720/50000 (29.41176470588235 Loss:0.9301394820213318)\n",
            "Training set:15360/50000 (30.690537084398976 Loss:0.5897712111473083)\n",
            "Training set:16000/50000 (31.9693094629156 Loss:0.8291308283805847)\n",
            "Training set:16640/50000 (33.248081841432224 Loss:0.8632082343101501)\n",
            "Training set:17280/50000 (34.52685421994885 Loss:0.5986049771308899)\n",
            "Training set:17920/50000 (35.80562659846547 Loss:0.8144608736038208)\n",
            "Training set:18560/50000 (37.084398976982094 Loss:0.6656435132026672)\n",
            "Training set:19200/50000 (38.36317135549872 Loss:0.7820694446563721)\n",
            "Training set:19840/50000 (39.64194373401534 Loss:0.7274011373519897)\n",
            "Training set:20480/50000 (40.92071611253197 Loss:0.6786096692085266)\n",
            "Training set:21120/50000 (42.19948849104859 Loss:0.7428775429725647)\n",
            "Training set:21760/50000 (43.47826086956522 Loss:0.5678574442863464)\n",
            "Training set:22400/50000 (44.75703324808184 Loss:0.6232872605323792)\n",
            "Training set:23040/50000 (46.03580562659847 Loss:0.5826301574707031)\n",
            "Training set:23680/50000 (47.31457800511509 Loss:0.7269728779792786)\n",
            "Training set:24320/50000 (48.593350383631716 Loss:0.7069888114929199)\n",
            "Training set:24960/50000 (49.87212276214834 Loss:0.6953026652336121)\n",
            "Training set:25600/50000 (51.150895140664964 Loss:0.4283015727996826)\n",
            "Training set:26240/50000 (52.429667519181585 Loss:0.5247808694839478)\n",
            "Training set:26880/50000 (53.70843989769821 Loss:0.5354597568511963)\n",
            "Training set:27520/50000 (54.987212276214834 Loss:0.6668066382408142)\n",
            "Training set:28160/50000 (56.265984654731454 Loss:0.5400159358978271)\n",
            "Training set:28800/50000 (57.54475703324808 Loss:0.389281690120697)\n",
            "Training set:29440/50000 (58.8235294117647 Loss:0.389358788728714)\n",
            "Training set:30080/50000 (60.10230179028133 Loss:0.6242803335189819)\n",
            "Training set:30720/50000 (61.38107416879795 Loss:0.9641765356063843)\n",
            "Training set:31360/50000 (62.65984654731458 Loss:0.6326255798339844)\n",
            "Training set:32000/50000 (63.9386189258312 Loss:0.7127934098243713)\n",
            "Training set:32640/50000 (65.21739130434783 Loss:0.5508808493614197)\n",
            "Training set:33280/50000 (66.49616368286445 Loss:0.9911078810691833)\n",
            "Training set:33920/50000 (67.77493606138107 Loss:0.6137539744377136)\n",
            "Training set:34560/50000 (69.0537084398977 Loss:0.5213946104049683)\n",
            "Training set:35200/50000 (70.33248081841433 Loss:0.6764958500862122)\n",
            "Training set:35840/50000 (71.61125319693095 Loss:0.7177982926368713)\n",
            "Training set:36480/50000 (72.89002557544757 Loss:0.6103690266609192)\n",
            "Training set:37120/50000 (74.16879795396419 Loss:0.9042378664016724)\n",
            "Training set:37760/50000 (75.44757033248082 Loss:0.64361971616745)\n",
            "Training set:38400/50000 (76.72634271099744 Loss:0.6125636696815491)\n",
            "Training set:39040/50000 (78.00511508951406 Loss:0.5852975845336914)\n",
            "Training set:39680/50000 (79.28388746803068 Loss:0.5141943693161011)\n",
            "Training set:40320/50000 (80.56265984654732 Loss:0.41612422466278076)\n",
            "Training set:40960/50000 (81.84143222506394 Loss:0.38682225346565247)\n",
            "Training set:41600/50000 (83.12020460358056 Loss:0.5436369180679321)\n",
            "Training set:42240/50000 (84.39897698209718 Loss:0.7309081554412842)\n",
            "Training set:42880/50000 (85.67774936061382 Loss:0.47776228189468384)\n",
            "Training set:43520/50000 (86.95652173913044 Loss:0.5271914601325989)\n",
            "Training set:44160/50000 (88.23529411764706 Loss:0.7506000995635986)\n",
            "Training set:44800/50000 (89.51406649616368 Loss:0.5971221923828125)\n",
            "Training set:45440/50000 (90.79283887468031 Loss:0.6749094128608704)\n",
            "Training set:46080/50000 (92.07161125319693 Loss:0.5985470414161682)\n",
            "Training set:46720/50000 (93.35038363171356 Loss:0.5082321166992188)\n",
            "Training set:47360/50000 (94.62915601023018 Loss:0.6707578301429749)\n",
            "Training set:48000/50000 (95.9079283887468 Loss:0.43589648604393005)\n",
            "Training set:48640/50000 (97.18670076726343 Loss:0.6795603632926941)\n",
            "Training set:49280/50000 (98.46547314578005 Loss:0.9008699655532837)\n",
            "Training set:49920/50000 (99.74424552429667 Loss:0.48433393239974976)\n",
            "Training set: Average loss: 0.643584\n",
            "Validation set: Average loss: 0.6654614633435656, Accuracy: 7770/10000 (77.7%)\n",
            "Epoch: 10\n",
            "Training set:0/50000 (0.0 Loss:0.6623728275299072)\n",
            "Training set:640/50000 (1.278772378516624 Loss:0.7322826385498047)\n",
            "Training set:1280/50000 (2.557544757033248 Loss:0.362510621547699)\n",
            "Training set:1920/50000 (3.836317135549872 Loss:0.8391385674476624)\n",
            "Training set:2560/50000 (5.115089514066496 Loss:0.6117433905601501)\n",
            "Training set:3200/50000 (6.3938618925831205 Loss:0.6645824909210205)\n",
            "Training set:3840/50000 (7.672634271099744 Loss:0.4526416063308716)\n",
            "Training set:4480/50000 (8.951406649616368 Loss:0.7281652092933655)\n",
            "Training set:5120/50000 (10.230179028132993 Loss:0.6808316707611084)\n",
            "Training set:5760/50000 (11.508951406649617 Loss:0.3477526605129242)\n",
            "Training set:6400/50000 (12.787723785166241 Loss:0.6766263842582703)\n",
            "Training set:7040/50000 (14.066496163682864 Loss:0.5265645980834961)\n",
            "Training set:7680/50000 (15.345268542199488 Loss:0.45351436734199524)\n",
            "Training set:8320/50000 (16.624040920716112 Loss:0.83262699842453)\n",
            "Training set:8960/50000 (17.902813299232736 Loss:0.6693538427352905)\n",
            "Training set:9600/50000 (19.18158567774936 Loss:0.7102696299552917)\n",
            "Training set:10240/50000 (20.460358056265985 Loss:0.7561025619506836)\n",
            "Training set:10880/50000 (21.73913043478261 Loss:0.4821285307407379)\n",
            "Training set:11520/50000 (23.017902813299234 Loss:0.6319365501403809)\n",
            "Training set:12160/50000 (24.296675191815858 Loss:0.5569979548454285)\n",
            "Training set:12800/50000 (25.575447570332482 Loss:0.589847207069397)\n",
            "Training set:13440/50000 (26.854219948849106 Loss:0.5773096680641174)\n",
            "Training set:14080/50000 (28.132992327365727 Loss:0.6489791870117188)\n",
            "Training set:14720/50000 (29.41176470588235 Loss:0.5042763948440552)\n",
            "Training set:15360/50000 (30.690537084398976 Loss:0.5797781944274902)\n",
            "Training set:16000/50000 (31.9693094629156 Loss:0.4228694438934326)\n",
            "Training set:16640/50000 (33.248081841432224 Loss:0.49691513180732727)\n",
            "Training set:17280/50000 (34.52685421994885 Loss:0.5828758478164673)\n",
            "Training set:17920/50000 (35.80562659846547 Loss:0.6024718880653381)\n",
            "Training set:18560/50000 (37.084398976982094 Loss:0.7231518626213074)\n",
            "Training set:19200/50000 (38.36317135549872 Loss:0.5029280185699463)\n",
            "Training set:19840/50000 (39.64194373401534 Loss:0.47367775440216064)\n",
            "Training set:20480/50000 (40.92071611253197 Loss:0.7374509572982788)\n",
            "Training set:21120/50000 (42.19948849104859 Loss:0.756003737449646)\n",
            "Training set:21760/50000 (43.47826086956522 Loss:0.5349293947219849)\n",
            "Training set:22400/50000 (44.75703324808184 Loss:0.8998163342475891)\n",
            "Training set:23040/50000 (46.03580562659847 Loss:0.5988492369651794)\n",
            "Training set:23680/50000 (47.31457800511509 Loss:0.750351071357727)\n",
            "Training set:24320/50000 (48.593350383631716 Loss:0.5723473429679871)\n",
            "Training set:24960/50000 (49.87212276214834 Loss:0.5036491751670837)\n",
            "Training set:25600/50000 (51.150895140664964 Loss:0.4352678656578064)\n",
            "Training set:26240/50000 (52.429667519181585 Loss:0.38112127780914307)\n",
            "Training set:26880/50000 (53.70843989769821 Loss:0.6527347564697266)\n",
            "Training set:27520/50000 (54.987212276214834 Loss:0.9277256727218628)\n",
            "Training set:28160/50000 (56.265984654731454 Loss:0.563949704170227)\n",
            "Training set:28800/50000 (57.54475703324808 Loss:0.5079373717308044)\n",
            "Training set:29440/50000 (58.8235294117647 Loss:0.5543906092643738)\n",
            "Training set:30080/50000 (60.10230179028133 Loss:0.5688623785972595)\n",
            "Training set:30720/50000 (61.38107416879795 Loss:0.5747334361076355)\n",
            "Training set:31360/50000 (62.65984654731458 Loss:0.6525388360023499)\n",
            "Training set:32000/50000 (63.9386189258312 Loss:0.6718485355377197)\n",
            "Training set:32640/50000 (65.21739130434783 Loss:0.7541267275810242)\n",
            "Training set:33280/50000 (66.49616368286445 Loss:0.5805993676185608)\n",
            "Training set:33920/50000 (67.77493606138107 Loss:0.5112497806549072)\n",
            "Training set:34560/50000 (69.0537084398977 Loss:0.8603110313415527)\n",
            "Training set:35200/50000 (70.33248081841433 Loss:0.605411946773529)\n",
            "Training set:35840/50000 (71.61125319693095 Loss:1.017536997795105)\n",
            "Training set:36480/50000 (72.89002557544757 Loss:0.7240005135536194)\n",
            "Training set:37120/50000 (74.16879795396419 Loss:0.8935022950172424)\n",
            "Training set:37760/50000 (75.44757033248082 Loss:0.7095035910606384)\n",
            "Training set:38400/50000 (76.72634271099744 Loss:0.5376975536346436)\n",
            "Training set:39040/50000 (78.00511508951406 Loss:0.6722205281257629)\n",
            "Training set:39680/50000 (79.28388746803068 Loss:0.841506838798523)\n",
            "Training set:40320/50000 (80.56265984654732 Loss:0.5260736346244812)\n",
            "Training set:40960/50000 (81.84143222506394 Loss:0.885109543800354)\n",
            "Training set:41600/50000 (83.12020460358056 Loss:0.5780938267707825)\n",
            "Training set:42240/50000 (84.39897698209718 Loss:0.6214427351951599)\n",
            "Training set:42880/50000 (85.67774936061382 Loss:0.4923829734325409)\n",
            "Training set:43520/50000 (86.95652173913044 Loss:0.6186128854751587)\n",
            "Training set:44160/50000 (88.23529411764706 Loss:0.5396429300308228)\n",
            "Training set:44800/50000 (89.51406649616368 Loss:0.5570926666259766)\n",
            "Training set:45440/50000 (90.79283887468031 Loss:0.5293132662773132)\n",
            "Training set:46080/50000 (92.07161125319693 Loss:0.6890588998794556)\n",
            "Training set:46720/50000 (93.35038363171356 Loss:0.563541054725647)\n",
            "Training set:47360/50000 (94.62915601023018 Loss:0.6786942481994629)\n",
            "Training set:48000/50000 (95.9079283887468 Loss:0.7033822536468506)\n",
            "Training set:48640/50000 (97.18670076726343 Loss:0.43354520201683044)\n",
            "Training set:49280/50000 (98.46547314578005 Loss:0.6242154240608215)\n",
            "Training set:49920/50000 (99.74424552429667 Loss:0.7726796269416809)\n",
            "Training set: Average loss: 0.610436\n",
            "Validation set: Average loss: 0.5883394430397423, Accuracy: 8121/10000 (81.21%)\n",
            "Epoch: 11\n",
            "Training set:0/50000 (0.0 Loss:0.4377979040145874)\n",
            "Training set:640/50000 (1.278772378516624 Loss:0.748698890209198)\n",
            "Training set:1280/50000 (2.557544757033248 Loss:0.3602498471736908)\n",
            "Training set:1920/50000 (3.836317135549872 Loss:0.43873217701911926)\n",
            "Training set:2560/50000 (5.115089514066496 Loss:0.7711761593818665)\n",
            "Training set:3200/50000 (6.3938618925831205 Loss:0.6838834881782532)\n",
            "Training set:3840/50000 (7.672634271099744 Loss:0.49668875336647034)\n",
            "Training set:4480/50000 (8.951406649616368 Loss:0.5799857974052429)\n",
            "Training set:5120/50000 (10.230179028132993 Loss:0.6081772446632385)\n",
            "Training set:5760/50000 (11.508951406649617 Loss:0.5870836973190308)\n",
            "Training set:6400/50000 (12.787723785166241 Loss:0.5016712546348572)\n",
            "Training set:7040/50000 (14.066496163682864 Loss:0.5076776742935181)\n",
            "Training set:7680/50000 (15.345268542199488 Loss:0.5191851854324341)\n",
            "Training set:8320/50000 (16.624040920716112 Loss:0.45825856924057007)\n",
            "Training set:8960/50000 (17.902813299232736 Loss:0.3713952898979187)\n",
            "Training set:9600/50000 (19.18158567774936 Loss:0.3995920419692993)\n",
            "Training set:10240/50000 (20.460358056265985 Loss:0.6328392028808594)\n",
            "Training set:10880/50000 (21.73913043478261 Loss:0.5981576442718506)\n",
            "Training set:11520/50000 (23.017902813299234 Loss:0.5095410346984863)\n",
            "Training set:12160/50000 (24.296675191815858 Loss:0.5787904262542725)\n",
            "Training set:12800/50000 (25.575447570332482 Loss:0.41492435336112976)\n",
            "Training set:13440/50000 (26.854219948849106 Loss:0.7699460983276367)\n",
            "Training set:14080/50000 (28.132992327365727 Loss:0.6231666207313538)\n",
            "Training set:14720/50000 (29.41176470588235 Loss:0.6225574612617493)\n",
            "Training set:15360/50000 (30.690537084398976 Loss:0.5149100422859192)\n",
            "Training set:16000/50000 (31.9693094629156 Loss:0.6888404488563538)\n",
            "Training set:16640/50000 (33.248081841432224 Loss:0.48357927799224854)\n",
            "Training set:17280/50000 (34.52685421994885 Loss:0.5689697265625)\n",
            "Training set:17920/50000 (35.80562659846547 Loss:0.5732404589653015)\n",
            "Training set:18560/50000 (37.084398976982094 Loss:0.4828944802284241)\n",
            "Training set:19200/50000 (38.36317135549872 Loss:0.7021390199661255)\n",
            "Training set:19840/50000 (39.64194373401534 Loss:0.5604544281959534)\n",
            "Training set:20480/50000 (40.92071611253197 Loss:0.5232975482940674)\n",
            "Training set:21120/50000 (42.19948849104859 Loss:0.7955277562141418)\n",
            "Training set:21760/50000 (43.47826086956522 Loss:0.7697371244430542)\n",
            "Training set:22400/50000 (44.75703324808184 Loss:0.49100160598754883)\n",
            "Training set:23040/50000 (46.03580562659847 Loss:0.6756716966629028)\n",
            "Training set:23680/50000 (47.31457800511509 Loss:0.5149407982826233)\n",
            "Training set:24320/50000 (48.593350383631716 Loss:0.5156015157699585)\n",
            "Training set:24960/50000 (49.87212276214834 Loss:0.47020184993743896)\n",
            "Training set:25600/50000 (51.150895140664964 Loss:0.47376325726509094)\n",
            "Training set:26240/50000 (52.429667519181585 Loss:0.5863223671913147)\n",
            "Training set:26880/50000 (53.70843989769821 Loss:0.6347370743751526)\n",
            "Training set:27520/50000 (54.987212276214834 Loss:0.4506957232952118)\n",
            "Training set:28160/50000 (56.265984654731454 Loss:0.672164797782898)\n",
            "Training set:28800/50000 (57.54475703324808 Loss:0.9124483466148376)\n",
            "Training set:29440/50000 (58.8235294117647 Loss:0.37122222781181335)\n",
            "Training set:30080/50000 (60.10230179028133 Loss:0.6262508034706116)\n",
            "Training set:30720/50000 (61.38107416879795 Loss:0.6735727190971375)\n",
            "Training set:31360/50000 (62.65984654731458 Loss:0.5538342595100403)\n",
            "Training set:32000/50000 (63.9386189258312 Loss:0.3187616467475891)\n",
            "Training set:32640/50000 (65.21739130434783 Loss:0.4397401213645935)\n",
            "Training set:33280/50000 (66.49616368286445 Loss:0.5696421265602112)\n",
            "Training set:33920/50000 (67.77493606138107 Loss:0.5082429051399231)\n",
            "Training set:34560/50000 (69.0537084398977 Loss:0.5426066517829895)\n",
            "Training set:35200/50000 (70.33248081841433 Loss:0.4929254353046417)\n",
            "Training set:35840/50000 (71.61125319693095 Loss:0.626282811164856)\n",
            "Training set:36480/50000 (72.89002557544757 Loss:0.5567108988761902)\n",
            "Training set:37120/50000 (74.16879795396419 Loss:0.4084516763687134)\n",
            "Training set:37760/50000 (75.44757033248082 Loss:0.6151956915855408)\n",
            "Training set:38400/50000 (76.72634271099744 Loss:0.6336968541145325)\n",
            "Training set:39040/50000 (78.00511508951406 Loss:0.474896639585495)\n",
            "Training set:39680/50000 (79.28388746803068 Loss:0.4785178601741791)\n",
            "Training set:40320/50000 (80.56265984654732 Loss:0.6252738237380981)\n",
            "Training set:40960/50000 (81.84143222506394 Loss:0.5218797922134399)\n",
            "Training set:41600/50000 (83.12020460358056 Loss:0.5887988209724426)\n",
            "Training set:42240/50000 (84.39897698209718 Loss:0.5881015062332153)\n",
            "Training set:42880/50000 (85.67774936061382 Loss:0.32025566697120667)\n",
            "Training set:43520/50000 (86.95652173913044 Loss:0.5736591219902039)\n",
            "Training set:44160/50000 (88.23529411764706 Loss:0.5026801228523254)\n",
            "Training set:44800/50000 (89.51406649616368 Loss:0.7271760702133179)\n",
            "Training set:45440/50000 (90.79283887468031 Loss:0.5030919313430786)\n",
            "Training set:46080/50000 (92.07161125319693 Loss:0.5672800540924072)\n",
            "Training set:46720/50000 (93.35038363171356 Loss:0.6153160333633423)\n",
            "Training set:47360/50000 (94.62915601023018 Loss:0.6339758634567261)\n",
            "Training set:48000/50000 (95.9079283887468 Loss:0.6166077256202698)\n",
            "Training set:48640/50000 (97.18670076726343 Loss:0.4044296443462372)\n",
            "Training set:49280/50000 (98.46547314578005 Loss:0.6354194283485413)\n",
            "Training set:49920/50000 (99.74424552429667 Loss:0.4041890799999237)\n",
            "Training set: Average loss: 0.572133\n",
            "Validation set: Average loss: 0.5979185485915773, Accuracy: 8087/10000 (80.87%)\n",
            "Epoch: 12\n",
            "Training set:0/50000 (0.0 Loss:0.6180146336555481)\n",
            "Training set:640/50000 (1.278772378516624 Loss:0.47167596220970154)\n",
            "Training set:1280/50000 (2.557544757033248 Loss:0.6428313255310059)\n",
            "Training set:1920/50000 (3.836317135549872 Loss:0.49891459941864014)\n",
            "Training set:2560/50000 (5.115089514066496 Loss:0.6121205687522888)\n",
            "Training set:3840/50000 (7.672634271099744 Loss:0.6262004971504211)\n",
            "Training set:4480/50000 (8.951406649616368 Loss:0.5677587389945984)\n",
            "Training set:5120/50000 (10.230179028132993 Loss:0.7551858425140381)\n",
            "Training set:5760/50000 (11.508951406649617 Loss:0.49044111371040344)\n",
            "Training set:6400/50000 (12.787723785166241 Loss:0.6575794219970703)\n",
            "Training set:7040/50000 (14.066496163682864 Loss:0.8931663036346436)\n",
            "Training set:7680/50000 (15.345268542199488 Loss:0.33280158042907715)\n",
            "Training set:8320/50000 (16.624040920716112 Loss:0.45777830481529236)\n",
            "Training set:8960/50000 (17.902813299232736 Loss:0.6557254195213318)\n",
            "Training set:9600/50000 (19.18158567774936 Loss:0.6616231203079224)\n",
            "Training set:10240/50000 (20.460358056265985 Loss:0.6455903649330139)\n",
            "Training set:10880/50000 (21.73913043478261 Loss:0.5768004655838013)\n",
            "Training set:11520/50000 (23.017902813299234 Loss:0.5544155836105347)\n",
            "Training set:12160/50000 (24.296675191815858 Loss:0.3556669354438782)\n",
            "Training set:12800/50000 (25.575447570332482 Loss:0.6821630001068115)\n",
            "Training set:13440/50000 (26.854219948849106 Loss:0.45280808210372925)\n",
            "Training set:14080/50000 (28.132992327365727 Loss:0.6670434474945068)\n",
            "Training set:14720/50000 (29.41176470588235 Loss:0.42267361283302307)\n",
            "Training set:15360/50000 (30.690537084398976 Loss:0.5275930762290955)\n",
            "Training set:16000/50000 (31.9693094629156 Loss:0.4447004795074463)\n",
            "Training set:16640/50000 (33.248081841432224 Loss:0.7587959170341492)\n",
            "Training set:17280/50000 (34.52685421994885 Loss:0.5146864056587219)\n",
            "Training set:17920/50000 (35.80562659846547 Loss:0.5556901097297668)\n",
            "Training set:18560/50000 (37.084398976982094 Loss:0.42841923236846924)\n",
            "Training set:19200/50000 (38.36317135549872 Loss:0.6416509747505188)\n",
            "Training set:19840/50000 (39.64194373401534 Loss:0.5080199241638184)\n",
            "Training set:20480/50000 (40.92071611253197 Loss:0.5851021409034729)\n",
            "Training set:21120/50000 (42.19948849104859 Loss:0.6412449479103088)\n",
            "Training set:21760/50000 (43.47826086956522 Loss:0.7620182037353516)\n",
            "Training set:22400/50000 (44.75703324808184 Loss:0.6050037741661072)\n",
            "Training set:23040/50000 (46.03580562659847 Loss:0.47579118609428406)\n",
            "Training set:23680/50000 (47.31457800511509 Loss:0.6242186427116394)\n",
            "Training set:24320/50000 (48.593350383631716 Loss:0.5535354018211365)\n",
            "Training set:24960/50000 (49.87212276214834 Loss:0.3601101338863373)\n",
            "Training set:25600/50000 (51.150895140664964 Loss:0.6935694217681885)\n",
            "Training set:26240/50000 (52.429667519181585 Loss:0.6266418695449829)\n",
            "Training set:26880/50000 (53.70843989769821 Loss:0.4605872929096222)\n",
            "Training set:27520/50000 (54.987212276214834 Loss:0.8341778516769409)\n",
            "Training set:28160/50000 (56.265984654731454 Loss:0.5592085123062134)\n",
            "Training set:28800/50000 (57.54475703324808 Loss:0.399697482585907)\n",
            "Training set:29440/50000 (58.8235294117647 Loss:0.3202483654022217)\n",
            "Training set:30080/50000 (60.10230179028133 Loss:0.5534244179725647)\n",
            "Training set:30720/50000 (61.38107416879795 Loss:0.43953725695610046)\n",
            "Training set:31360/50000 (62.65984654731458 Loss:0.5459364652633667)\n",
            "Training set:32000/50000 (63.9386189258312 Loss:0.7506451606750488)\n",
            "Training set:32640/50000 (65.21739130434783 Loss:0.627594530582428)\n",
            "Training set:33280/50000 (66.49616368286445 Loss:0.6405752897262573)\n",
            "Training set:33920/50000 (67.77493606138107 Loss:0.5107364654541016)\n",
            "Training set:34560/50000 (69.0537084398977 Loss:0.3981248140335083)\n",
            "Training set:35200/50000 (70.33248081841433 Loss:0.546481728553772)\n",
            "Training set:35840/50000 (71.61125319693095 Loss:0.6433603763580322)\n",
            "Training set:36480/50000 (72.89002557544757 Loss:0.5646687150001526)\n",
            "Training set:37120/50000 (74.16879795396419 Loss:0.6569663286209106)\n",
            "Training set:37760/50000 (75.44757033248082 Loss:0.6022989153862)\n",
            "Training set:38400/50000 (76.72634271099744 Loss:0.4159296452999115)\n",
            "Training set:39040/50000 (78.00511508951406 Loss:0.6777490377426147)\n",
            "Training set:39680/50000 (79.28388746803068 Loss:0.6168615818023682)\n",
            "Training set:40320/50000 (80.56265984654732 Loss:0.6107571721076965)\n",
            "Training set:40960/50000 (81.84143222506394 Loss:0.37825480103492737)\n",
            "Training set:41600/50000 (83.12020460358056 Loss:0.44474509358406067)\n",
            "Training set:42240/50000 (84.39897698209718 Loss:0.36282095313072205)\n",
            "Training set:42880/50000 (85.67774936061382 Loss:0.4526905119419098)\n",
            "Training set:43520/50000 (86.95652173913044 Loss:0.5668967366218567)\n",
            "Training set:44160/50000 (88.23529411764706 Loss:0.6306516528129578)\n",
            "Training set:44800/50000 (89.51406649616368 Loss:0.6132746934890747)\n",
            "Training set:45440/50000 (90.79283887468031 Loss:0.48909100890159607)\n",
            "Training set:46080/50000 (92.07161125319693 Loss:0.5135210156440735)\n",
            "Training set:46720/50000 (93.35038363171356 Loss:0.4508684277534485)\n",
            "Training set:47360/50000 (94.62915601023018 Loss:0.3685024082660675)\n",
            "Training set:48000/50000 (95.9079283887468 Loss:0.5980675220489502)\n",
            "Training set:48640/50000 (97.18670076726343 Loss:0.561774492263794)\n",
            "Training set:49280/50000 (98.46547314578005 Loss:0.42710572481155396)\n",
            "Training set:49920/50000 (99.74424552429667 Loss:0.5673489570617676)\n",
            "Training set: Average loss: 0.538182\n",
            "Validation set: Average loss: 0.6200146328681594, Accuracy: 7995/10000 (79.95%)\n",
            "Epoch: 13\n",
            "Training set:0/50000 (0.0 Loss:0.5317267179489136)\n",
            "Training set:640/50000 (1.278772378516624 Loss:0.6874203681945801)\n",
            "Training set:1280/50000 (2.557544757033248 Loss:0.5499587059020996)\n",
            "Training set:1920/50000 (3.836317135549872 Loss:0.4728235602378845)\n",
            "Training set:2560/50000 (5.115089514066496 Loss:0.5976490378379822)\n",
            "Training set:3200/50000 (6.3938618925831205 Loss:0.40334630012512207)\n",
            "Training set:3840/50000 (7.672634271099744 Loss:0.5445085763931274)\n",
            "Training set:4480/50000 (8.951406649616368 Loss:0.7510605454444885)\n",
            "Training set:5120/50000 (10.230179028132993 Loss:0.5530077815055847)\n",
            "Training set:5760/50000 (11.508951406649617 Loss:0.5275269150733948)\n",
            "Training set:6400/50000 (12.787723785166241 Loss:0.4983040392398834)\n",
            "Training set:7040/50000 (14.066496163682864 Loss:0.8186375498771667)\n",
            "Training set:7680/50000 (15.345268542199488 Loss:0.7750111818313599)\n",
            "Training set:8320/50000 (16.624040920716112 Loss:0.2639794647693634)\n",
            "Training set:8960/50000 (17.902813299232736 Loss:0.7267314791679382)\n",
            "Training set:9600/50000 (19.18158567774936 Loss:0.49859553575515747)\n",
            "Training set:10240/50000 (20.460358056265985 Loss:0.7005987167358398)\n",
            "Training set:10880/50000 (21.73913043478261 Loss:0.4400845468044281)\n",
            "Training set:11520/50000 (23.017902813299234 Loss:0.48778173327445984)\n",
            "Training set:12160/50000 (24.296675191815858 Loss:0.5954773426055908)\n",
            "Training set:12800/50000 (25.575447570332482 Loss:0.8265419602394104)\n",
            "Training set:13440/50000 (26.854219948849106 Loss:0.6598486304283142)\n",
            "Training set:14080/50000 (28.132992327365727 Loss:0.4708999991416931)\n",
            "Training set:14720/50000 (29.41176470588235 Loss:0.49918070435523987)\n",
            "Training set:15360/50000 (30.690537084398976 Loss:0.5190576910972595)\n",
            "Training set:16000/50000 (31.9693094629156 Loss:0.8422712087631226)\n",
            "Training set:16640/50000 (33.248081841432224 Loss:0.5872020125389099)\n",
            "Training set:17280/50000 (34.52685421994885 Loss:0.5455815196037292)\n",
            "Training set:17920/50000 (35.80562659846547 Loss:0.638658881187439)\n",
            "Training set:18560/50000 (37.084398976982094 Loss:0.4489065408706665)\n",
            "Training set:19200/50000 (38.36317135549872 Loss:0.5384982824325562)\n",
            "Training set:19840/50000 (39.64194373401534 Loss:0.30928248167037964)\n",
            "Training set:20480/50000 (40.92071611253197 Loss:0.398674875497818)\n",
            "Training set:21120/50000 (42.19948849104859 Loss:0.5304004549980164)\n",
            "Training set:21760/50000 (43.47826086956522 Loss:0.5238737463951111)\n",
            "Training set:22400/50000 (44.75703324808184 Loss:0.7181709408760071)\n",
            "Training set:23040/50000 (46.03580562659847 Loss:0.8205869197845459)\n",
            "Training set:23680/50000 (47.31457800511509 Loss:0.4234224855899811)\n",
            "Training set:24320/50000 (48.593350383631716 Loss:0.6079733371734619)\n",
            "Training set:24960/50000 (49.87212276214834 Loss:0.6359845399856567)\n",
            "Training set:25600/50000 (51.150895140664964 Loss:0.35091543197631836)\n",
            "Training set:26240/50000 (52.429667519181585 Loss:0.5297628045082092)\n",
            "Training set:26880/50000 (53.70843989769821 Loss:0.3618932366371155)\n",
            "Training set:27520/50000 (54.987212276214834 Loss:0.560341477394104)\n",
            "Training set:28160/50000 (56.265984654731454 Loss:0.5964319109916687)\n",
            "Training set:28800/50000 (57.54475703324808 Loss:0.35896673798561096)\n",
            "Training set:29440/50000 (58.8235294117647 Loss:0.5697975754737854)\n",
            "Training set:30080/50000 (60.10230179028133 Loss:0.5151385068893433)\n",
            "Training set:30720/50000 (61.38107416879795 Loss:0.34205201268196106)\n",
            "Training set:31360/50000 (62.65984654731458 Loss:0.5310165286064148)\n",
            "Training set:32000/50000 (63.9386189258312 Loss:0.6106847524642944)\n",
            "Training set:32640/50000 (65.21739130434783 Loss:0.5599124431610107)\n",
            "Training set:33280/50000 (66.49616368286445 Loss:0.47681647539138794)\n",
            "Training set:33920/50000 (67.77493606138107 Loss:0.5711517333984375)\n",
            "Training set:34560/50000 (69.0537084398977 Loss:0.6477835774421692)\n",
            "Training set:35200/50000 (70.33248081841433 Loss:0.6638457179069519)\n",
            "Training set:35840/50000 (71.61125319693095 Loss:0.43041282892227173)\n",
            "Training set:36480/50000 (72.89002557544757 Loss:0.6572065353393555)\n",
            "Training set:37120/50000 (74.16879795396419 Loss:0.5262425541877747)\n",
            "Training set:37760/50000 (75.44757033248082 Loss:0.46545058488845825)\n",
            "Training set:38400/50000 (76.72634271099744 Loss:0.5032654404640198)\n",
            "Training set:39040/50000 (78.00511508951406 Loss:0.5735042095184326)\n",
            "Training set:39680/50000 (79.28388746803068 Loss:0.619609534740448)\n",
            "Training set:40320/50000 (80.56265984654732 Loss:0.38025787472724915)\n",
            "Training set:40960/50000 (81.84143222506394 Loss:0.3403770327568054)\n",
            "Training set:41600/50000 (83.12020460358056 Loss:0.5689654350280762)\n",
            "Training set:42240/50000 (84.39897698209718 Loss:0.659561038017273)\n",
            "Training set:42880/50000 (85.67774936061382 Loss:0.25616422295570374)\n",
            "Training set:43520/50000 (86.95652173913044 Loss:0.46121305227279663)\n",
            "Training set:44160/50000 (88.23529411764706 Loss:0.6651175022125244)\n",
            "Training set:44800/50000 (89.51406649616368 Loss:0.43536651134490967)\n",
            "Training set:45440/50000 (90.79283887468031 Loss:0.4517219364643097)\n",
            "Training set:46080/50000 (92.07161125319693 Loss:0.38381585478782654)\n",
            "Training set:46720/50000 (93.35038363171356 Loss:0.625251293182373)\n",
            "Training set:47360/50000 (94.62915601023018 Loss:0.5562363862991333)\n",
            "Training set:48000/50000 (95.9079283887468 Loss:0.4229695498943329)\n",
            "Training set:48640/50000 (97.18670076726343 Loss:0.4247686266899109)\n",
            "Training set:49280/50000 (98.46547314578005 Loss:0.48076286911964417)\n",
            "Training set:49920/50000 (99.74424552429667 Loss:0.6366215944290161)\n",
            "Training set: Average loss: 0.515566\n",
            "Validation set: Average loss: 0.6031917859414581, Accuracy: 8051/10000 (80.51%)\n",
            "Epoch: 14\n",
            "Training set:0/50000 (0.0 Loss:0.5697194337844849)\n",
            "Training set:640/50000 (1.278772378516624 Loss:0.5659626722335815)\n",
            "Training set:1280/50000 (2.557544757033248 Loss:0.4650534391403198)\n",
            "Training set:1920/50000 (3.836317135549872 Loss:0.5608870387077332)\n",
            "Training set:2560/50000 (5.115089514066496 Loss:0.5931220054626465)\n",
            "Training set:3200/50000 (6.3938618925831205 Loss:0.2955544888973236)\n",
            "Training set:3840/50000 (7.672634271099744 Loss:0.4584609568119049)\n",
            "Training set:4480/50000 (8.951406649616368 Loss:0.3868575096130371)\n",
            "Training set:5120/50000 (10.230179028132993 Loss:0.4740634858608246)\n",
            "Training set:5760/50000 (11.508951406649617 Loss:0.4575139880180359)\n",
            "Training set:6400/50000 (12.787723785166241 Loss:0.5083115696907043)\n",
            "Training set:7040/50000 (14.066496163682864 Loss:0.5043992400169373)\n",
            "Training set:7680/50000 (15.345268542199488 Loss:0.637914776802063)\n",
            "Training set:8320/50000 (16.624040920716112 Loss:0.41776353120803833)\n",
            "Training set:8960/50000 (17.902813299232736 Loss:0.5351181626319885)\n",
            "Training set:9600/50000 (19.18158567774936 Loss:0.9239804148674011)\n",
            "Training set:10240/50000 (20.460358056265985 Loss:0.602397084236145)\n",
            "Training set:10880/50000 (21.73913043478261 Loss:0.7060567140579224)\n",
            "Training set:11520/50000 (23.017902813299234 Loss:0.3226321339607239)\n",
            "Training set:12160/50000 (24.296675191815858 Loss:0.48861268162727356)\n",
            "Training set:12800/50000 (25.575447570332482 Loss:0.6346420645713806)\n",
            "Training set:13440/50000 (26.854219948849106 Loss:0.43017706274986267)\n",
            "Training set:14080/50000 (28.132992327365727 Loss:0.4710403084754944)\n",
            "Training set:14720/50000 (29.41176470588235 Loss:0.42784079909324646)\n",
            "Training set:15360/50000 (30.690537084398976 Loss:0.5738021731376648)\n",
            "Training set:16000/50000 (31.9693094629156 Loss:0.3598977029323578)\n",
            "Training set:16640/50000 (33.248081841432224 Loss:0.3791336119174957)\n",
            "Training set:17280/50000 (34.52685421994885 Loss:0.5455334186553955)\n",
            "Training set:17920/50000 (35.80562659846547 Loss:0.4633871912956238)\n",
            "Training set:18560/50000 (37.084398976982094 Loss:0.4705621898174286)\n",
            "Training set:19200/50000 (38.36317135549872 Loss:0.4827817380428314)\n",
            "Training set:19840/50000 (39.64194373401534 Loss:0.476340651512146)\n",
            "Training set:20480/50000 (40.92071611253197 Loss:0.44549471139907837)\n",
            "Training set:21120/50000 (42.19948849104859 Loss:0.4641030430793762)\n",
            "Training set:21760/50000 (43.47826086956522 Loss:0.5375660061836243)\n",
            "Training set:22400/50000 (44.75703324808184 Loss:0.19450828433036804)\n",
            "Training set:23040/50000 (46.03580562659847 Loss:0.3621216416358948)\n",
            "Training set:23680/50000 (47.31457800511509 Loss:0.28081852197647095)\n",
            "Training set:24320/50000 (48.593350383631716 Loss:0.6067638993263245)\n",
            "Training set:24960/50000 (49.87212276214834 Loss:0.35625895857810974)\n",
            "Training set:25600/50000 (51.150895140664964 Loss:0.3225717842578888)\n",
            "Training set:26240/50000 (52.429667519181585 Loss:0.5236268639564514)\n",
            "Training set:26880/50000 (53.70843989769821 Loss:0.5474491715431213)\n",
            "Training set:27520/50000 (54.987212276214834 Loss:0.5416885614395142)\n",
            "Training set:28160/50000 (56.265984654731454 Loss:0.2858481705188751)\n",
            "Training set:28800/50000 (57.54475703324808 Loss:0.4268358647823334)\n",
            "Training set:29440/50000 (58.8235294117647 Loss:0.3978588879108429)\n",
            "Training set:30080/50000 (60.10230179028133 Loss:0.5523050427436829)\n",
            "Training set:30720/50000 (61.38107416879795 Loss:0.3575900197029114)\n",
            "Training set:31360/50000 (62.65984654731458 Loss:0.394457072019577)\n",
            "Training set:32000/50000 (63.9386189258312 Loss:0.5791153907775879)\n",
            "Training set:32640/50000 (65.21739130434783 Loss:0.5015183687210083)\n",
            "Training set:33280/50000 (66.49616368286445 Loss:0.35415539145469666)\n",
            "Training set:33920/50000 (67.77493606138107 Loss:0.31844812631607056)\n",
            "Training set:34560/50000 (69.0537084398977 Loss:0.6477289199829102)\n",
            "Training set:35200/50000 (70.33248081841433 Loss:0.46238282322883606)\n",
            "Training set:35840/50000 (71.61125319693095 Loss:0.4094044268131256)\n",
            "Training set:36480/50000 (72.89002557544757 Loss:0.2744322419166565)\n",
            "Training set:37120/50000 (74.16879795396419 Loss:0.6351497769355774)\n",
            "Training set:37760/50000 (75.44757033248082 Loss:0.6068506836891174)\n",
            "Training set:38400/50000 (76.72634271099744 Loss:0.5382054448127747)\n",
            "Training set:39040/50000 (78.00511508951406 Loss:0.49698948860168457)\n",
            "Training set:39680/50000 (79.28388746803068 Loss:0.44743818044662476)\n",
            "Training set:40320/50000 (80.56265984654732 Loss:0.3085556626319885)\n",
            "Training set:40960/50000 (81.84143222506394 Loss:0.25237253308296204)\n",
            "Training set:41600/50000 (83.12020460358056 Loss:0.6675066947937012)\n",
            "Training set:42240/50000 (84.39897698209718 Loss:0.42434075474739075)\n",
            "Training set:42880/50000 (85.67774936061382 Loss:0.5456215739250183)\n",
            "Training set:43520/50000 (86.95652173913044 Loss:0.4739534258842468)\n",
            "Training set:44160/50000 (88.23529411764706 Loss:0.3529433310031891)\n",
            "Training set:44800/50000 (89.51406649616368 Loss:0.4074661433696747)\n",
            "Training set:45440/50000 (90.79283887468031 Loss:0.4105380177497864)\n",
            "Training set:46080/50000 (92.07161125319693 Loss:0.8159655332565308)\n",
            "Training set:46720/50000 (93.35038363171356 Loss:0.548403263092041)\n",
            "Training set:47360/50000 (94.62915601023018 Loss:0.5785802602767944)\n",
            "Training set:48000/50000 (95.9079283887468 Loss:0.4401766061782837)\n",
            "Training set:48640/50000 (97.18670076726343 Loss:0.7919341325759888)\n",
            "Training set:49280/50000 (98.46547314578005 Loss:0.6992200613021851)\n",
            "Training set:49920/50000 (99.74424552429667 Loss:0.3059011399745941)\n",
            "Training set: Average loss: 0.481759\n",
            "Validation set: Average loss: 0.5397308423260975, Accuracy: 8281/10000 (82.81%)\n",
            "Epoch: 15\n",
            "Training set:0/50000 (0.0 Loss:0.28982260823249817)\n",
            "Training set:640/50000 (1.278772378516624 Loss:0.3283228576183319)\n",
            "Training set:1280/50000 (2.557544757033248 Loss:0.4257921874523163)\n",
            "Training set:1920/50000 (3.836317135549872 Loss:0.37880611419677734)\n",
            "Training set:2560/50000 (5.115089514066496 Loss:0.44084757566452026)\n",
            "Training set:3200/50000 (6.3938618925831205 Loss:0.3782443702220917)\n",
            "Training set:3840/50000 (7.672634271099744 Loss:0.5362423658370972)\n",
            "Training set:4480/50000 (8.951406649616368 Loss:0.37065696716308594)\n",
            "Training set:5120/50000 (10.230179028132993 Loss:0.3517070412635803)\n",
            "Training set:5760/50000 (11.508951406649617 Loss:0.7189471125602722)\n",
            "Training set:6400/50000 (12.787723785166241 Loss:0.27290263772010803)\n",
            "Training set:7040/50000 (14.066496163682864 Loss:0.4024181067943573)\n",
            "Training set:7680/50000 (15.345268542199488 Loss:0.3738410174846649)\n",
            "Training set:8320/50000 (16.624040920716112 Loss:0.4027157425880432)\n",
            "Training set:8960/50000 (17.902813299232736 Loss:0.4767075777053833)\n",
            "Training set:9600/50000 (19.18158567774936 Loss:0.3091942369937897)\n",
            "Training set:10240/50000 (20.460358056265985 Loss:0.2446337342262268)\n",
            "Training set:10880/50000 (21.73913043478261 Loss:0.39753422141075134)\n",
            "Training set:11520/50000 (23.017902813299234 Loss:0.37276339530944824)\n",
            "Training set:12160/50000 (24.296675191815858 Loss:0.5565676093101501)\n",
            "Training set:12800/50000 (25.575447570332482 Loss:0.5742586255073547)\n",
            "Training set:13440/50000 (26.854219948849106 Loss:0.6814537048339844)\n",
            "Training set:14080/50000 (28.132992327365727 Loss:0.5367163419723511)\n",
            "Training set:14720/50000 (29.41176470588235 Loss:0.5255997180938721)\n",
            "Training set:15360/50000 (30.690537084398976 Loss:0.35373419523239136)\n",
            "Training set:16000/50000 (31.9693094629156 Loss:0.5512768626213074)\n",
            "Training set:16640/50000 (33.248081841432224 Loss:0.5951170325279236)\n",
            "Training set:17280/50000 (34.52685421994885 Loss:0.48482105135917664)\n",
            "Training set:17920/50000 (35.80562659846547 Loss:0.4722675085067749)\n",
            "Training set:18560/50000 (37.084398976982094 Loss:0.5319749712944031)\n",
            "Training set:19200/50000 (38.36317135549872 Loss:0.43474018573760986)\n",
            "Training set:19840/50000 (39.64194373401534 Loss:0.4463779330253601)\n",
            "Training set:20480/50000 (40.92071611253197 Loss:0.5131767392158508)\n",
            "Training set:21120/50000 (42.19948849104859 Loss:0.34696057438850403)\n",
            "Training set:21760/50000 (43.47826086956522 Loss:0.3536662757396698)\n",
            "Training set:22400/50000 (44.75703324808184 Loss:0.3845168352127075)\n",
            "Training set:23040/50000 (46.03580562659847 Loss:0.3884776830673218)\n",
            "Training set:23680/50000 (47.31457800511509 Loss:0.5546490550041199)\n",
            "Training set:24320/50000 (48.593350383631716 Loss:0.3784147799015045)\n",
            "Training set:24960/50000 (49.87212276214834 Loss:0.6455894708633423)\n",
            "Training set:25600/50000 (51.150895140664964 Loss:0.5292452573776245)\n",
            "Training set:26240/50000 (52.429667519181585 Loss:0.5870752334594727)\n",
            "Training set:26880/50000 (53.70843989769821 Loss:0.6174065470695496)\n",
            "Training set:27520/50000 (54.987212276214834 Loss:0.285370409488678)\n",
            "Training set:28160/50000 (56.265984654731454 Loss:0.3123787045478821)\n",
            "Training set:28800/50000 (57.54475703324808 Loss:0.5825345516204834)\n",
            "Training set:29440/50000 (58.8235294117647 Loss:0.5039511322975159)\n",
            "Training set:30080/50000 (60.10230179028133 Loss:0.37315788865089417)\n",
            "Training set:30720/50000 (61.38107416879795 Loss:0.39549389481544495)\n",
            "Training set:31360/50000 (62.65984654731458 Loss:0.5588024854660034)\n",
            "Training set:32000/50000 (63.9386189258312 Loss:0.4958978295326233)\n",
            "Training set:32640/50000 (65.21739130434783 Loss:0.4245070219039917)\n",
            "Training set:33280/50000 (66.49616368286445 Loss:0.46307793259620667)\n",
            "Training set:33920/50000 (67.77493606138107 Loss:0.33569204807281494)\n",
            "Training set:34560/50000 (69.0537084398977 Loss:0.39771169424057007)\n",
            "Training set:35200/50000 (70.33248081841433 Loss:0.6200477480888367)\n",
            "Training set:35840/50000 (71.61125319693095 Loss:0.4665612280368805)\n",
            "Training set:36480/50000 (72.89002557544757 Loss:0.49665287137031555)\n",
            "Training set:37120/50000 (74.16879795396419 Loss:0.41208505630493164)\n",
            "Training set:37760/50000 (75.44757033248082 Loss:0.49280425906181335)\n",
            "Training set:38400/50000 (76.72634271099744 Loss:0.45569026470184326)\n",
            "Training set:39040/50000 (78.00511508951406 Loss:0.6812602281570435)\n",
            "Training set:39680/50000 (79.28388746803068 Loss:0.40701425075531006)\n",
            "Training set:40320/50000 (80.56265984654732 Loss:0.4055100977420807)\n",
            "Training set:40960/50000 (81.84143222506394 Loss:0.511137068271637)\n",
            "Training set:41600/50000 (83.12020460358056 Loss:0.30491024255752563)\n",
            "Training set:42240/50000 (84.39897698209718 Loss:0.45045316219329834)\n",
            "Training set:42880/50000 (85.67774936061382 Loss:0.2791612148284912)\n",
            "Training set:43520/50000 (86.95652173913044 Loss:0.48184752464294434)\n",
            "Training set:44160/50000 (88.23529411764706 Loss:0.6063615679740906)\n",
            "Training set:44800/50000 (89.51406649616368 Loss:0.5372796654701233)\n",
            "Training set:45440/50000 (90.79283887468031 Loss:0.44777733087539673)\n",
            "Training set:46080/50000 (92.07161125319693 Loss:0.375300794839859)\n",
            "Training set:46720/50000 (93.35038363171356 Loss:0.39083385467529297)\n",
            "Training set:47360/50000 (94.62915601023018 Loss:0.4906935691833496)\n",
            "Training set:48000/50000 (95.9079283887468 Loss:0.5877277255058289)\n",
            "Training set:48640/50000 (97.18670076726343 Loss:0.5284886956214905)\n",
            "Training set:49280/50000 (98.46547314578005 Loss:0.47134852409362793)\n",
            "Training set:49920/50000 (99.74424552429667 Loss:0.4836777150630951)\n",
            "Training set: Average loss: 0.456342\n",
            "Validation set: Average loss: 0.5103866026090209, Accuracy: 8338/10000 (83.38%)\n",
            "Epoch: 16\n",
            "Training set:0/50000 (0.0 Loss:0.35053765773773193)\n",
            "Training set:640/50000 (1.278772378516624 Loss:0.39443331956863403)\n",
            "Training set:1280/50000 (2.557544757033248 Loss:0.5207403898239136)\n",
            "Training set:1920/50000 (3.836317135549872 Loss:0.42222484946250916)\n",
            "Training set:2560/50000 (5.115089514066496 Loss:0.26186686754226685)\n",
            "Training set:3200/50000 (6.3938618925831205 Loss:0.6926289200782776)\n",
            "Training set:3840/50000 (7.672634271099744 Loss:0.3253858983516693)\n",
            "Training set:4480/50000 (8.951406649616368 Loss:0.47366026043891907)\n",
            "Training set:5120/50000 (10.230179028132993 Loss:0.40559589862823486)\n",
            "Training set:5760/50000 (11.508951406649617 Loss:0.3796992897987366)\n",
            "Training set:6400/50000 (12.787723785166241 Loss:0.37854573130607605)\n",
            "Training set:7040/50000 (14.066496163682864 Loss:0.24609211087226868)\n",
            "Training set:7680/50000 (15.345268542199488 Loss:0.5020523071289062)\n",
            "Training set:8320/50000 (16.624040920716112 Loss:0.2886493504047394)\n",
            "Training set:8960/50000 (17.902813299232736 Loss:0.519657552242279)\n",
            "Training set:9600/50000 (19.18158567774936 Loss:0.28916358947753906)\n",
            "Training set:10240/50000 (20.460358056265985 Loss:0.5015761256217957)\n",
            "Training set:10880/50000 (21.73913043478261 Loss:0.5025933384895325)\n",
            "Training set:11520/50000 (23.017902813299234 Loss:0.36280590295791626)\n",
            "Training set:12160/50000 (24.296675191815858 Loss:0.4771658480167389)\n",
            "Training set:12800/50000 (25.575447570332482 Loss:0.5781500339508057)\n",
            "Training set:13440/50000 (26.854219948849106 Loss:0.5957196354866028)\n",
            "Training set:14080/50000 (28.132992327365727 Loss:0.393167108297348)\n",
            "Training set:14720/50000 (29.41176470588235 Loss:0.29785671830177307)\n",
            "Training set:15360/50000 (30.690537084398976 Loss:0.25059938430786133)\n",
            "Training set:16000/50000 (31.9693094629156 Loss:0.38024255633354187)\n",
            "Training set:16640/50000 (33.248081841432224 Loss:0.3589557409286499)\n",
            "Training set:17280/50000 (34.52685421994885 Loss:0.30122458934783936)\n",
            "Training set:17920/50000 (35.80562659846547 Loss:0.5017184019088745)\n",
            "Training set:18560/50000 (37.084398976982094 Loss:0.4752127230167389)\n",
            "Training set:19200/50000 (38.36317135549872 Loss:0.33446553349494934)\n",
            "Training set:19840/50000 (39.64194373401534 Loss:0.4019448459148407)\n",
            "Training set:20480/50000 (40.92071611253197 Loss:0.43004995584487915)\n",
            "Training set:21120/50000 (42.19948849104859 Loss:0.40163713693618774)\n",
            "Training set:21760/50000 (43.47826086956522 Loss:0.41752657294273376)\n",
            "Training set:22400/50000 (44.75703324808184 Loss:0.4842044413089752)\n",
            "Training set:23040/50000 (46.03580562659847 Loss:0.38583236932754517)\n",
            "Training set:23680/50000 (47.31457800511509 Loss:0.44550031423568726)\n",
            "Training set:24320/50000 (48.593350383631716 Loss:0.5726617574691772)\n",
            "Training set:24960/50000 (49.87212276214834 Loss:0.5172548294067383)\n",
            "Training set:25600/50000 (51.150895140664964 Loss:0.451728492975235)\n",
            "Training set:26240/50000 (52.429667519181585 Loss:0.6754528880119324)\n",
            "Training set:26880/50000 (53.70843989769821 Loss:0.2841190695762634)\n",
            "Training set:27520/50000 (54.987212276214834 Loss:0.40595266222953796)\n",
            "Training set:28160/50000 (56.265984654731454 Loss:0.5003417730331421)\n",
            "Training set:28800/50000 (57.54475703324808 Loss:0.5672733783721924)\n",
            "Training set:29440/50000 (58.8235294117647 Loss:0.3980627655982971)\n",
            "Training set:30080/50000 (60.10230179028133 Loss:0.24809956550598145)\n",
            "Training set:30720/50000 (61.38107416879795 Loss:0.4216538965702057)\n",
            "Training set:31360/50000 (62.65984654731458 Loss:0.36510568857192993)\n",
            "Training set:32000/50000 (63.9386189258312 Loss:0.3818052411079407)\n",
            "Training set:32640/50000 (65.21739130434783 Loss:0.420245885848999)\n",
            "Training set:33280/50000 (66.49616368286445 Loss:0.38314110040664673)\n",
            "Training set:33920/50000 (67.77493606138107 Loss:0.2999088168144226)\n",
            "Training set:34560/50000 (69.0537084398977 Loss:0.2632265090942383)\n",
            "Training set:35200/50000 (70.33248081841433 Loss:0.4585571587085724)\n",
            "Training set:35840/50000 (71.61125319693095 Loss:0.5068308115005493)\n",
            "Training set:36480/50000 (72.89002557544757 Loss:0.30106329917907715)\n",
            "Training set:37120/50000 (74.16879795396419 Loss:0.5608136653900146)\n",
            "Training set:37760/50000 (75.44757033248082 Loss:0.4484196901321411)\n",
            "Training set:38400/50000 (76.72634271099744 Loss:0.355191171169281)\n",
            "Training set:39040/50000 (78.00511508951406 Loss:0.4143297076225281)\n",
            "Training set:39680/50000 (79.28388746803068 Loss:0.5102735161781311)\n",
            "Training set:40320/50000 (80.56265984654732 Loss:0.2853691279888153)\n",
            "Training set:40960/50000 (81.84143222506394 Loss:0.3541747033596039)\n",
            "Training set:41600/50000 (83.12020460358056 Loss:0.42708519101142883)\n",
            "Training set:42240/50000 (84.39897698209718 Loss:0.16640138626098633)\n",
            "Training set:42880/50000 (85.67774936061382 Loss:0.42619746923446655)\n",
            "Training set:43520/50000 (86.95652173913044 Loss:0.6258065700531006)\n",
            "Training set:44160/50000 (88.23529411764706 Loss:0.6061260104179382)\n",
            "Training set:44800/50000 (89.51406649616368 Loss:0.47103989124298096)\n",
            "Training set:45440/50000 (90.79283887468031 Loss:0.551088809967041)\n",
            "Training set:46080/50000 (92.07161125319693 Loss:0.7117692828178406)\n",
            "Training set:46720/50000 (93.35038363171356 Loss:0.5016814470291138)\n",
            "Training set:47360/50000 (94.62915601023018 Loss:0.42517659068107605)\n",
            "Training set:48000/50000 (95.9079283887468 Loss:0.30291107296943665)\n",
            "Training set:48640/50000 (97.18670076726343 Loss:0.34842824935913086)\n",
            "Training set:49280/50000 (98.46547314578005 Loss:0.5318600535392761)\n",
            "Training set:49920/50000 (99.74424552429667 Loss:0.5577248334884644)\n",
            "Training set: Average loss: 0.436540\n",
            "Validation set: Average loss: 0.5917114626829791, Accuracy: 8197/10000 (81.97%)\n",
            "Epoch: 17\n",
            "Training set:0/50000 (0.0 Loss:0.3418594300746918)\n",
            "Training set:640/50000 (1.278772378516624 Loss:0.3468709886074066)\n",
            "Training set:1280/50000 (2.557544757033248 Loss:0.33492815494537354)\n",
            "Training set:1920/50000 (3.836317135549872 Loss:0.4463356137275696)\n",
            "Training set:2560/50000 (5.115089514066496 Loss:0.39767032861709595)\n",
            "Training set:3200/50000 (6.3938618925831205 Loss:0.5358853340148926)\n",
            "Training set:3840/50000 (7.672634271099744 Loss:0.5665926337242126)\n",
            "Training set:4480/50000 (8.951406649616368 Loss:0.2695944011211395)\n",
            "Training set:5120/50000 (10.230179028132993 Loss:0.23551489412784576)\n",
            "Training set:5760/50000 (11.508951406649617 Loss:0.6305333375930786)\n",
            "Training set:6400/50000 (12.787723785166241 Loss:0.3932994604110718)\n",
            "Training set:7040/50000 (14.066496163682864 Loss:0.3972618281841278)\n",
            "Training set:7680/50000 (15.345268542199488 Loss:0.4706740379333496)\n",
            "Training set:8320/50000 (16.624040920716112 Loss:0.6852067708969116)\n",
            "Training set:8960/50000 (17.902813299232736 Loss:0.3630225956439972)\n",
            "Training set:9600/50000 (19.18158567774936 Loss:0.40316200256347656)\n",
            "Training set:10240/50000 (20.460358056265985 Loss:0.5464217662811279)\n",
            "Training set:10880/50000 (21.73913043478261 Loss:0.3438684940338135)\n",
            "Training set:11520/50000 (23.017902813299234 Loss:0.41281530261039734)\n",
            "Training set:12160/50000 (24.296675191815858 Loss:0.4070776402950287)\n",
            "Training set:12800/50000 (25.575447570332482 Loss:0.46546828746795654)\n",
            "Training set:13440/50000 (26.854219948849106 Loss:0.4955228269100189)\n",
            "Training set:14080/50000 (28.132992327365727 Loss:0.4400385916233063)\n",
            "Training set:14720/50000 (29.41176470588235 Loss:0.2775479555130005)\n",
            "Training set:15360/50000 (30.690537084398976 Loss:0.5996081829071045)\n",
            "Training set:16000/50000 (31.9693094629156 Loss:0.486331969499588)\n",
            "Training set:16640/50000 (33.248081841432224 Loss:0.37115198373794556)\n",
            "Training set:17280/50000 (34.52685421994885 Loss:0.39584410190582275)\n",
            "Training set:17920/50000 (35.80562659846547 Loss:0.4826827347278595)\n",
            "Training set:18560/50000 (37.084398976982094 Loss:0.3251391351222992)\n",
            "Training set:19200/50000 (38.36317135549872 Loss:0.39453640580177307)\n",
            "Training set:19840/50000 (39.64194373401534 Loss:0.3435434401035309)\n",
            "Training set:20480/50000 (40.92071611253197 Loss:0.3796049654483795)\n",
            "Training set:21120/50000 (42.19948849104859 Loss:0.32100018858909607)\n",
            "Training set:21760/50000 (43.47826086956522 Loss:0.5008832216262817)\n",
            "Training set:22400/50000 (44.75703324808184 Loss:0.22923704981803894)\n",
            "Training set:23040/50000 (46.03580562659847 Loss:0.18815353512763977)\n",
            "Training set:23680/50000 (47.31457800511509 Loss:0.4442954957485199)\n",
            "Training set:24320/50000 (48.593350383631716 Loss:0.4029589891433716)\n",
            "Training set:24960/50000 (49.87212276214834 Loss:0.3212639093399048)\n",
            "Training set:25600/50000 (51.150895140664964 Loss:0.26216480135917664)\n",
            "Training set:26240/50000 (52.429667519181585 Loss:0.4160889685153961)\n",
            "Training set:26880/50000 (53.70843989769821 Loss:0.35380807518959045)\n",
            "Training set:27520/50000 (54.987212276214834 Loss:0.3717081844806671)\n",
            "Training set:28160/50000 (56.265984654731454 Loss:0.4946920573711395)\n",
            "Training set:28800/50000 (57.54475703324808 Loss:0.2319955974817276)\n",
            "Training set:29440/50000 (58.8235294117647 Loss:0.4291798770427704)\n",
            "Training set:30080/50000 (60.10230179028133 Loss:0.2908666133880615)\n",
            "Training set:30720/50000 (61.38107416879795 Loss:0.2995836138725281)\n",
            "Training set:31360/50000 (62.65984654731458 Loss:0.6487009525299072)\n",
            "Training set:32000/50000 (63.9386189258312 Loss:0.5315620303153992)\n",
            "Training set:32640/50000 (65.21739130434783 Loss:0.5446674227714539)\n",
            "Training set:33280/50000 (66.49616368286445 Loss:0.7900557518005371)\n",
            "Training set:33920/50000 (67.77493606138107 Loss:0.4306575357913971)\n",
            "Training set:34560/50000 (69.0537084398977 Loss:0.5828932523727417)\n",
            "Training set:35200/50000 (70.33248081841433 Loss:0.5978986024856567)\n",
            "Training set:35840/50000 (71.61125319693095 Loss:0.505841851234436)\n",
            "Training set:36480/50000 (72.89002557544757 Loss:0.3856462240219116)\n",
            "Training set:37120/50000 (74.16879795396419 Loss:0.3953447937965393)\n",
            "Training set:37760/50000 (75.44757033248082 Loss:0.4245796501636505)\n",
            "Training set:38400/50000 (76.72634271099744 Loss:0.45817625522613525)\n",
            "Training set:39040/50000 (78.00511508951406 Loss:0.28866586089134216)\n",
            "Training set:39680/50000 (79.28388746803068 Loss:0.36311250925064087)\n",
            "Training set:40320/50000 (80.56265984654732 Loss:0.28645119071006775)\n",
            "Training set:40960/50000 (81.84143222506394 Loss:0.5818209052085876)\n",
            "Training set:41600/50000 (83.12020460358056 Loss:0.47124266624450684)\n",
            "Training set:42240/50000 (84.39897698209718 Loss:0.3237273395061493)\n",
            "Training set:42880/50000 (85.67774936061382 Loss:0.2330411672592163)\n",
            "Training set:43520/50000 (86.95652173913044 Loss:0.6815362572669983)\n",
            "Training set:44160/50000 (88.23529411764706 Loss:0.4387369453907013)\n",
            "Training set:44800/50000 (89.51406649616368 Loss:0.3393874168395996)\n",
            "Training set:45440/50000 (90.79283887468031 Loss:0.38497164845466614)\n",
            "Training set:46080/50000 (92.07161125319693 Loss:0.4032590091228485)\n",
            "Training set:46720/50000 (93.35038363171356 Loss:0.27484214305877686)\n",
            "Training set:47360/50000 (94.62915601023018 Loss:0.5151188373565674)\n",
            "Training set:48000/50000 (95.9079283887468 Loss:0.5019969344139099)\n",
            "Training set:48640/50000 (97.18670076726343 Loss:0.39713144302368164)\n",
            "Training set:49280/50000 (98.46547314578005 Loss:0.490749329328537)\n",
            "Training set:49920/50000 (99.74424552429667 Loss:0.6183800101280212)\n",
            "Training set: Average loss: 0.421647\n",
            "Validation set: Average loss: 0.5153968441448394, Accuracy: 8340/10000 (83.4%)\n",
            "Epoch: 18\n",
            "Training set:0/50000 (0.0 Loss:0.3042488694190979)\n",
            "Training set:640/50000 (1.278772378516624 Loss:0.42941898107528687)\n",
            "Training set:1280/50000 (2.557544757033248 Loss:0.5373947620391846)\n",
            "Training set:1920/50000 (3.836317135549872 Loss:0.5690817832946777)\n",
            "Training set:2560/50000 (5.115089514066496 Loss:0.3991519510746002)\n",
            "Training set:3200/50000 (6.3938618925831205 Loss:0.4001646041870117)\n",
            "Training set:3840/50000 (7.672634271099744 Loss:0.396912157535553)\n",
            "Training set:4480/50000 (8.951406649616368 Loss:0.30335357785224915)\n",
            "Training set:5120/50000 (10.230179028132993 Loss:0.4201640784740448)\n",
            "Training set:5760/50000 (11.508951406649617 Loss:0.4879826605319977)\n",
            "Training set:6400/50000 (12.787723785166241 Loss:0.19287879765033722)\n",
            "Training set:7040/50000 (14.066496163682864 Loss:0.3726636469364166)\n",
            "Training set:7680/50000 (15.345268542199488 Loss:0.36369380354881287)\n",
            "Training set:8320/50000 (16.624040920716112 Loss:0.3776105046272278)\n",
            "Training set:8960/50000 (17.902813299232736 Loss:0.4149998128414154)\n",
            "Training set:9600/50000 (19.18158567774936 Loss:0.3618110120296478)\n",
            "Training set:10240/50000 (20.460358056265985 Loss:0.5646501779556274)\n",
            "Training set:10880/50000 (21.73913043478261 Loss:0.32477807998657227)\n",
            "Training set:11520/50000 (23.017902813299234 Loss:0.43224674463272095)\n",
            "Training set:12160/50000 (24.296675191815858 Loss:0.2962769865989685)\n",
            "Training set:12800/50000 (25.575447570332482 Loss:0.25819212198257446)\n",
            "Training set:13440/50000 (26.854219948849106 Loss:0.3155936598777771)\n",
            "Training set:14080/50000 (28.132992327365727 Loss:0.5070316195487976)\n",
            "Training set:14720/50000 (29.41176470588235 Loss:0.30697792768478394)\n",
            "Training set:15360/50000 (30.690537084398976 Loss:0.38712120056152344)\n",
            "Training set:16000/50000 (31.9693094629156 Loss:0.3430682420730591)\n",
            "Training set:16640/50000 (33.248081841432224 Loss:0.3257724344730377)\n",
            "Training set:17280/50000 (34.52685421994885 Loss:0.3915926218032837)\n",
            "Training set:17920/50000 (35.80562659846547 Loss:0.24959012866020203)\n",
            "Training set:18560/50000 (37.084398976982094 Loss:0.4495999813079834)\n",
            "Training set:19200/50000 (38.36317135549872 Loss:0.42481479048728943)\n",
            "Training set:19840/50000 (39.64194373401534 Loss:0.27584707736968994)\n",
            "Training set:20480/50000 (40.92071611253197 Loss:0.5485307574272156)\n",
            "Training set:21120/50000 (42.19948849104859 Loss:0.32689422369003296)\n",
            "Training set:21760/50000 (43.47826086956522 Loss:0.33756282925605774)\n",
            "Training set:22400/50000 (44.75703324808184 Loss:0.5172011852264404)\n",
            "Training set:23040/50000 (46.03580562659847 Loss:0.21651744842529297)\n",
            "Training set:23680/50000 (47.31457800511509 Loss:0.27125072479248047)\n",
            "Training set:24320/50000 (48.593350383631716 Loss:0.29266178607940674)\n",
            "Training set:24960/50000 (49.87212276214834 Loss:0.4289863109588623)\n",
            "Training set:25600/50000 (51.150895140664964 Loss:0.4465167820453644)\n",
            "Training set:26240/50000 (52.429667519181585 Loss:0.46100810170173645)\n",
            "Training set:26880/50000 (53.70843989769821 Loss:0.4615629017353058)\n",
            "Training set:27520/50000 (54.987212276214834 Loss:0.34079352021217346)\n",
            "Training set:28160/50000 (56.265984654731454 Loss:0.49149537086486816)\n",
            "Training set:28800/50000 (57.54475703324808 Loss:0.44014719128608704)\n",
            "Training set:29440/50000 (58.8235294117647 Loss:0.30088144540786743)\n",
            "Training set:30080/50000 (60.10230179028133 Loss:0.3421199917793274)\n",
            "Training set:30720/50000 (61.38107416879795 Loss:0.42435139417648315)\n",
            "Training set:31360/50000 (62.65984654731458 Loss:0.42824867367744446)\n",
            "Training set:32000/50000 (63.9386189258312 Loss:0.44181665778160095)\n",
            "Training set:32640/50000 (65.21739130434783 Loss:0.4282050132751465)\n",
            "Training set:33280/50000 (66.49616368286445 Loss:0.4559153914451599)\n",
            "Training set:33920/50000 (67.77493606138107 Loss:0.5968638062477112)\n",
            "Training set:34560/50000 (69.0537084398977 Loss:0.3048768937587738)\n",
            "Training set:35200/50000 (70.33248081841433 Loss:0.3113211691379547)\n",
            "Training set:35840/50000 (71.61125319693095 Loss:0.5173397064208984)\n",
            "Training set:36480/50000 (72.89002557544757 Loss:0.40805312991142273)\n",
            "Training set:37120/50000 (74.16879795396419 Loss:0.6080120205879211)\n",
            "Training set:37760/50000 (75.44757033248082 Loss:0.26759201288223267)\n",
            "Training set:38400/50000 (76.72634271099744 Loss:0.4649941325187683)\n",
            "Training set:39040/50000 (78.00511508951406 Loss:0.5528222918510437)\n",
            "Training set:39680/50000 (79.28388746803068 Loss:0.25448575615882874)\n",
            "Training set:40320/50000 (80.56265984654732 Loss:0.24882599711418152)\n",
            "Training set:40960/50000 (81.84143222506394 Loss:0.37104761600494385)\n",
            "Training set:41600/50000 (83.12020460358056 Loss:0.37018975615501404)\n",
            "Training set:42240/50000 (84.39897698209718 Loss:0.564190149307251)\n",
            "Training set:42880/50000 (85.67774936061382 Loss:0.38770225644111633)\n",
            "Training set:43520/50000 (86.95652173913044 Loss:0.279263436794281)\n",
            "Training set:44160/50000 (88.23529411764706 Loss:0.3315232992172241)\n",
            "Training set:44800/50000 (89.51406649616368 Loss:0.5251133441925049)\n",
            "Training set:45440/50000 (90.79283887468031 Loss:0.49080193042755127)\n",
            "Training set:46080/50000 (92.07161125319693 Loss:0.4634520411491394)\n",
            "Training set:46720/50000 (93.35038363171356 Loss:0.2884213626384735)\n",
            "Training set:47360/50000 (94.62915601023018 Loss:0.2666599750518799)\n",
            "Training set:48000/50000 (95.9079283887468 Loss:0.48500651121139526)\n",
            "Training set:48640/50000 (97.18670076726343 Loss:0.36779654026031494)\n",
            "Training set:49280/50000 (98.46547314578005 Loss:0.3329567313194275)\n",
            "Training set:49920/50000 (99.74424552429667 Loss:0.361521452665329)\n",
            "Training set: Average loss: 0.399525\n",
            "Validation set: Average loss: 0.49317515181128385, Accuracy: 8435/10000 (84.35%)\n",
            "Epoch: 19\n",
            "Training set:0/50000 (0.0 Loss:0.3606082797050476)\n",
            "Training set:640/50000 (1.278772378516624 Loss:0.5390186905860901)\n",
            "Training set:1280/50000 (2.557544757033248 Loss:0.34179651737213135)\n",
            "Training set:1920/50000 (3.836317135549872 Loss:0.4516734778881073)\n",
            "Training set:2560/50000 (5.115089514066496 Loss:0.42213118076324463)\n",
            "Training set:3200/50000 (6.3938618925831205 Loss:0.3127118945121765)\n",
            "Training set:3840/50000 (7.672634271099744 Loss:0.36380964517593384)\n",
            "Training set:4480/50000 (8.951406649616368 Loss:0.6656315922737122)\n",
            "Training set:5120/50000 (10.230179028132993 Loss:0.5094300508499146)\n",
            "Training set:5760/50000 (11.508951406649617 Loss:0.46978864073753357)\n",
            "Training set:6400/50000 (12.787723785166241 Loss:0.47893717885017395)\n",
            "Training set:7040/50000 (14.066496163682864 Loss:0.17708225548267365)\n",
            "Training set:7680/50000 (15.345268542199488 Loss:0.394289493560791)\n",
            "Training set:8320/50000 (16.624040920716112 Loss:0.2988992929458618)\n",
            "Training set:8960/50000 (17.902813299232736 Loss:0.5337562561035156)\n",
            "Training set:9600/50000 (19.18158567774936 Loss:0.30980464816093445)\n",
            "Training set:10240/50000 (20.460358056265985 Loss:0.23331941664218903)\n",
            "Training set:10880/50000 (21.73913043478261 Loss:0.29993972182273865)\n",
            "Training set:11520/50000 (23.017902813299234 Loss:0.3734377920627594)\n",
            "Training set:12160/50000 (24.296675191815858 Loss:0.45381611585617065)\n",
            "Training set:12800/50000 (25.575447570332482 Loss:0.21578210592269897)\n",
            "Training set:13440/50000 (26.854219948849106 Loss:0.5732502937316895)\n",
            "Training set:14080/50000 (28.132992327365727 Loss:0.4793040454387665)\n",
            "Training set:14720/50000 (29.41176470588235 Loss:0.36615750193595886)\n",
            "Training set:15360/50000 (30.690537084398976 Loss:0.4649549722671509)\n",
            "Training set:16000/50000 (31.9693094629156 Loss:0.27617549896240234)\n",
            "Training set:16640/50000 (33.248081841432224 Loss:0.6176792979240417)\n",
            "Training set:17280/50000 (34.52685421994885 Loss:0.3717465400695801)\n",
            "Training set:17920/50000 (35.80562659846547 Loss:0.3121289312839508)\n",
            "Training set:18560/50000 (37.084398976982094 Loss:0.2580116391181946)\n",
            "Training set:19200/50000 (38.36317135549872 Loss:0.38254618644714355)\n",
            "Training set:19840/50000 (39.64194373401534 Loss:0.18940262496471405)\n",
            "Training set:20480/50000 (40.92071611253197 Loss:0.35305342078208923)\n",
            "Training set:21120/50000 (42.19948849104859 Loss:0.4941914677619934)\n",
            "Training set:21760/50000 (43.47826086956522 Loss:0.33813002705574036)\n",
            "Training set:22400/50000 (44.75703324808184 Loss:0.3117828667163849)\n",
            "Training set:23040/50000 (46.03580562659847 Loss:0.411965012550354)\n",
            "Training set:23680/50000 (47.31457800511509 Loss:0.31572315096855164)\n",
            "Training set:24320/50000 (48.593350383631716 Loss:0.29113075137138367)\n",
            "Training set:24960/50000 (49.87212276214834 Loss:0.39537912607192993)\n",
            "Training set:25600/50000 (51.150895140664964 Loss:0.2401246577501297)\n",
            "Training set:26240/50000 (52.429667519181585 Loss:0.44104552268981934)\n",
            "Training set:26880/50000 (53.70843989769821 Loss:0.3391760587692261)\n",
            "Training set:27520/50000 (54.987212276214834 Loss:0.3173053562641144)\n",
            "Training set:28160/50000 (56.265984654731454 Loss:0.41217026114463806)\n",
            "Training set:28800/50000 (57.54475703324808 Loss:0.41406768560409546)\n",
            "Training set:29440/50000 (58.8235294117647 Loss:0.32906851172447205)\n",
            "Training set:30080/50000 (60.10230179028133 Loss:0.5594237446784973)\n",
            "Training set:30720/50000 (61.38107416879795 Loss:0.4781765639781952)\n",
            "Training set:31360/50000 (62.65984654731458 Loss:0.45567190647125244)\n",
            "Training set:32000/50000 (63.9386189258312 Loss:0.41930946707725525)\n",
            "Training set:32640/50000 (65.21739130434783 Loss:0.4011361300945282)\n",
            "Training set:33280/50000 (66.49616368286445 Loss:0.2607446610927582)\n",
            "Training set:33920/50000 (67.77493606138107 Loss:0.5342828631401062)\n",
            "Training set:34560/50000 (69.0537084398977 Loss:0.46004942059516907)\n",
            "Training set:35200/50000 (70.33248081841433 Loss:0.2129296213388443)\n",
            "Training set:35840/50000 (71.61125319693095 Loss:0.47384053468704224)\n",
            "Training set:36480/50000 (72.89002557544757 Loss:0.4608488082885742)\n",
            "Training set:37120/50000 (74.16879795396419 Loss:0.524019181728363)\n",
            "Training set:37760/50000 (75.44757033248082 Loss:0.21018418669700623)\n",
            "Training set:38400/50000 (76.72634271099744 Loss:0.327778697013855)\n",
            "Training set:39040/50000 (78.00511508951406 Loss:0.4819847345352173)\n",
            "Training set:39680/50000 (79.28388746803068 Loss:0.3187055289745331)\n",
            "Training set:40320/50000 (80.56265984654732 Loss:0.37578174471855164)\n",
            "Training set:40960/50000 (81.84143222506394 Loss:0.5387693047523499)\n",
            "Training set:41600/50000 (83.12020460358056 Loss:0.5762171745300293)\n",
            "Training set:42240/50000 (84.39897698209718 Loss:0.42698192596435547)\n",
            "Training set:42880/50000 (85.67774936061382 Loss:0.3254832923412323)\n",
            "Training set:43520/50000 (86.95652173913044 Loss:0.2614828944206238)\n",
            "Training set:44160/50000 (88.23529411764706 Loss:0.41190797090530396)\n",
            "Training set:44800/50000 (89.51406649616368 Loss:0.34521111845970154)\n",
            "Training set:45440/50000 (90.79283887468031 Loss:0.31661826372146606)\n",
            "Training set:46080/50000 (92.07161125319693 Loss:0.322214275598526)\n",
            "Training set:46720/50000 (93.35038363171356 Loss:0.3474741578102112)\n",
            "Training set:47360/50000 (94.62915601023018 Loss:0.43604007363319397)\n",
            "Training set:48000/50000 (95.9079283887468 Loss:0.21300332248210907)\n",
            "Training set:48640/50000 (97.18670076726343 Loss:0.4300438165664673)\n",
            "Training set:49280/50000 (98.46547314578005 Loss:0.4448215961456299)\n",
            "Training set:49920/50000 (99.74424552429667 Loss:0.23416844010353088)\n",
            "Training set: Average loss: 0.384723\n",
            "Validation set: Average loss: 0.4689831707128294, Accuracy: 8523/10000 (85.23%)\n",
            "Epoch: 20\n",
            "Training set:0/50000 (0.0 Loss:0.2735413610935211)\n",
            "Training set:640/50000 (1.278772378516624 Loss:0.1661863923072815)\n",
            "Training set:1280/50000 (2.557544757033248 Loss:0.25386878848075867)\n",
            "Training set:1920/50000 (3.836317135549872 Loss:0.195509672164917)\n",
            "Training set:2560/50000 (5.115089514066496 Loss:0.46892112493515015)\n",
            "Training set:3200/50000 (6.3938618925831205 Loss:0.3010528087615967)\n",
            "Training set:3840/50000 (7.672634271099744 Loss:0.2851946949958801)\n",
            "Training set:4480/50000 (8.951406649616368 Loss:0.42722436785697937)\n",
            "Training set:5120/50000 (10.230179028132993 Loss:0.32643795013427734)\n",
            "Training set:5760/50000 (11.508951406649617 Loss:0.35110408067703247)\n",
            "Training set:6400/50000 (12.787723785166241 Loss:0.4390442967414856)\n",
            "Training set:7040/50000 (14.066496163682864 Loss:0.20939481258392334)\n",
            "Training set:7680/50000 (15.345268542199488 Loss:0.17124666273593903)\n",
            "Training set:8320/50000 (16.624040920716112 Loss:0.24790441989898682)\n",
            "Training set:8960/50000 (17.902813299232736 Loss:0.33864977955818176)\n",
            "Training set:9600/50000 (19.18158567774936 Loss:0.23386454582214355)\n",
            "Training set:10240/50000 (20.460358056265985 Loss:0.2779849171638489)\n",
            "Training set:10880/50000 (21.73913043478261 Loss:0.2744706869125366)\n",
            "Training set:11520/50000 (23.017902813299234 Loss:0.17343629896640778)\n",
            "Training set:12160/50000 (24.296675191815858 Loss:0.34416165947914124)\n",
            "Training set:12800/50000 (25.575447570332482 Loss:0.3432058095932007)\n",
            "Training set:13440/50000 (26.854219948849106 Loss:0.2845158278942108)\n",
            "Training set:14080/50000 (28.132992327365727 Loss:0.3785751163959503)\n",
            "Training set:14720/50000 (29.41176470588235 Loss:0.481458842754364)\n",
            "Training set:15360/50000 (30.690537084398976 Loss:0.3038647472858429)\n",
            "Training set:16000/50000 (31.9693094629156 Loss:0.36425259709358215)\n",
            "Training set:16640/50000 (33.248081841432224 Loss:0.5301843881607056)\n",
            "Training set:17280/50000 (34.52685421994885 Loss:0.4981389045715332)\n",
            "Training set:17920/50000 (35.80562659846547 Loss:0.32087069749832153)\n",
            "Training set:18560/50000 (37.084398976982094 Loss:0.43424659967422485)\n",
            "Training set:19200/50000 (38.36317135549872 Loss:0.3463936448097229)\n",
            "Training set:19840/50000 (39.64194373401534 Loss:0.41765663027763367)\n",
            "Training set:20480/50000 (40.92071611253197 Loss:0.35800838470458984)\n",
            "Training set:21120/50000 (42.19948849104859 Loss:0.4344988167285919)\n",
            "Training set:21760/50000 (43.47826086956522 Loss:0.4683976173400879)\n",
            "Training set:22400/50000 (44.75703324808184 Loss:0.7482619285583496)\n",
            "Training set:23040/50000 (46.03580562659847 Loss:0.3763951063156128)\n",
            "Training set:23680/50000 (47.31457800511509 Loss:0.4202701151371002)\n",
            "Training set:24320/50000 (48.593350383631716 Loss:0.32789960503578186)\n",
            "Training set:24960/50000 (49.87212276214834 Loss:0.28602275252342224)\n",
            "Training set:25600/50000 (51.150895140664964 Loss:0.38963213562965393)\n",
            "Training set:26240/50000 (52.429667519181585 Loss:0.21085958182811737)\n",
            "Training set:26880/50000 (53.70843989769821 Loss:0.5300161838531494)\n",
            "Training set:27520/50000 (54.987212276214834 Loss:0.36712363362312317)\n",
            "Training set:28160/50000 (56.265984654731454 Loss:0.21084922552108765)\n",
            "Training set:28800/50000 (57.54475703324808 Loss:0.34565553069114685)\n",
            "Training set:29440/50000 (58.8235294117647 Loss:0.27416279911994934)\n",
            "Training set:30080/50000 (60.10230179028133 Loss:0.38645219802856445)\n",
            "Training set:30720/50000 (61.38107416879795 Loss:0.28692546486854553)\n",
            "Training set:31360/50000 (62.65984654731458 Loss:0.3898635804653168)\n",
            "Training set:32000/50000 (63.9386189258312 Loss:0.4185900092124939)\n",
            "Training set:32640/50000 (65.21739130434783 Loss:0.35346484184265137)\n",
            "Training set:33280/50000 (66.49616368286445 Loss:0.33768489956855774)\n",
            "Training set:33920/50000 (67.77493606138107 Loss:0.2251618653535843)\n",
            "Training set:34560/50000 (69.0537084398977 Loss:0.4813268184661865)\n",
            "Training set:35200/50000 (70.33248081841433 Loss:0.18132685124874115)\n",
            "Training set:35840/50000 (71.61125319693095 Loss:0.4484650194644928)\n",
            "Training set:36480/50000 (72.89002557544757 Loss:0.3063929080963135)\n",
            "Training set:37120/50000 (74.16879795396419 Loss:0.3833915889263153)\n",
            "Training set:37760/50000 (75.44757033248082 Loss:0.34907010197639465)\n",
            "Training set:38400/50000 (76.72634271099744 Loss:0.3981909155845642)\n",
            "Training set:39040/50000 (78.00511508951406 Loss:0.38974207639694214)\n",
            "Training set:39680/50000 (79.28388746803068 Loss:0.29126980900764465)\n",
            "Training set:40320/50000 (80.56265984654732 Loss:0.48030778765678406)\n",
            "Training set:40960/50000 (81.84143222506394 Loss:0.5841034054756165)\n",
            "Training set:41600/50000 (83.12020460358056 Loss:0.3319794833660126)\n",
            "Training set:42240/50000 (84.39897698209718 Loss:0.3918152451515198)\n",
            "Training set:42880/50000 (85.67774936061382 Loss:0.48205050826072693)\n",
            "Training set:43520/50000 (86.95652173913044 Loss:0.41500356793403625)\n",
            "Training set:44160/50000 (88.23529411764706 Loss:0.3204419016838074)\n",
            "Training set:44800/50000 (89.51406649616368 Loss:0.48841872811317444)\n",
            "Training set:45440/50000 (90.79283887468031 Loss:0.4027084410190582)\n",
            "Training set:46080/50000 (92.07161125319693 Loss:0.2555626630783081)\n",
            "Training set:46720/50000 (93.35038363171356 Loss:0.3466784954071045)\n",
            "Training set:47360/50000 (94.62915601023018 Loss:0.5631391406059265)\n",
            "Training set:48000/50000 (95.9079283887468 Loss:0.45148637890815735)\n",
            "Training set:48640/50000 (97.18670076726343 Loss:0.47094836831092834)\n",
            "Training set:49280/50000 (98.46547314578005 Loss:0.33731159567832947)\n",
            "Training set:49920/50000 (99.74424552429667 Loss:0.4095141589641571)\n",
            "Training set: Average loss: 0.360047\n",
            "Validation set: Average loss: 0.4963429265531005, Accuracy: 8459/10000 (84.59%)\n",
            "Epoch: 21\n",
            "Training set:0/50000 (0.0 Loss:0.13009393215179443)\n",
            "Training set:640/50000 (1.278772378516624 Loss:0.35090354084968567)\n",
            "Training set:1280/50000 (2.557544757033248 Loss:0.38378623127937317)\n",
            "Training set:1920/50000 (3.836317135549872 Loss:0.29704076051712036)\n",
            "Training set:2560/50000 (5.115089514066496 Loss:0.32197558879852295)\n",
            "Training set:3200/50000 (6.3938618925831205 Loss:0.3528779149055481)\n",
            "Training set:3840/50000 (7.672634271099744 Loss:0.1789214164018631)\n",
            "Training set:4480/50000 (8.951406649616368 Loss:0.4251006543636322)\n",
            "Training set:5120/50000 (10.230179028132993 Loss:0.3081241250038147)\n",
            "Training set:5760/50000 (11.508951406649617 Loss:0.2377774864435196)\n",
            "Training set:6400/50000 (12.787723785166241 Loss:0.4120199978351593)\n",
            "Training set:7040/50000 (14.066496163682864 Loss:0.27400124073028564)\n",
            "Training set:7680/50000 (15.345268542199488 Loss:0.17820347845554352)\n",
            "Training set:8320/50000 (16.624040920716112 Loss:0.32599806785583496)\n",
            "Training set:8960/50000 (17.902813299232736 Loss:0.48198699951171875)\n",
            "Training set:9600/50000 (19.18158567774936 Loss:0.6078872084617615)\n",
            "Training set:10240/50000 (20.460358056265985 Loss:0.37188443541526794)\n",
            "Training set:10880/50000 (21.73913043478261 Loss:0.30979666113853455)\n",
            "Training set:11520/50000 (23.017902813299234 Loss:0.19275392591953278)\n",
            "Training set:12160/50000 (24.296675191815858 Loss:0.1835739016532898)\n",
            "Training set:12800/50000 (25.575447570332482 Loss:0.11752390116453171)\n",
            "Training set:13440/50000 (26.854219948849106 Loss:0.2636280953884125)\n",
            "Training set:14080/50000 (28.132992327365727 Loss:0.49091100692749023)\n",
            "Training set:14720/50000 (29.41176470588235 Loss:0.27038612961769104)\n",
            "Training set:15360/50000 (30.690537084398976 Loss:0.31206122040748596)\n",
            "Training set:16000/50000 (31.9693094629156 Loss:0.32433030009269714)\n",
            "Training set:16640/50000 (33.248081841432224 Loss:0.2370138317346573)\n",
            "Training set:17280/50000 (34.52685421994885 Loss:0.4142012298107147)\n",
            "Training set:17920/50000 (35.80562659846547 Loss:0.3523949980735779)\n",
            "Training set:18560/50000 (37.084398976982094 Loss:0.3084368407726288)\n",
            "Training set:19200/50000 (38.36317135549872 Loss:0.4633387327194214)\n",
            "Training set:19840/50000 (39.64194373401534 Loss:0.19460336863994598)\n",
            "Training set:20480/50000 (40.92071611253197 Loss:0.4367934465408325)\n",
            "Training set:21120/50000 (42.19948849104859 Loss:0.3762570321559906)\n",
            "Training set:21760/50000 (43.47826086956522 Loss:0.18592286109924316)\n",
            "Training set:22400/50000 (44.75703324808184 Loss:0.26735514402389526)\n",
            "Training set:23040/50000 (46.03580562659847 Loss:0.45127397775650024)\n",
            "Training set:23680/50000 (47.31457800511509 Loss:0.366196870803833)\n",
            "Training set:24320/50000 (48.593350383631716 Loss:0.3610589802265167)\n",
            "Training set:24960/50000 (49.87212276214834 Loss:0.22685639560222626)\n",
            "Training set:25600/50000 (51.150895140664964 Loss:0.39492034912109375)\n",
            "Training set:26240/50000 (52.429667519181585 Loss:0.3448296785354614)\n",
            "Training set:26880/50000 (53.70843989769821 Loss:0.35741177201271057)\n",
            "Training set:27520/50000 (54.987212276214834 Loss:0.2525337338447571)\n",
            "Training set:28160/50000 (56.265984654731454 Loss:0.49941831827163696)\n",
            "Training set:28800/50000 (57.54475703324808 Loss:0.280061811208725)\n",
            "Training set:29440/50000 (58.8235294117647 Loss:0.2895687520503998)\n",
            "Training set:30080/50000 (60.10230179028133 Loss:0.39126166701316833)\n",
            "Training set:30720/50000 (61.38107416879795 Loss:0.337569922208786)\n",
            "Training set:31360/50000 (62.65984654731458 Loss:0.22246065735816956)\n",
            "Training set:32000/50000 (63.9386189258312 Loss:0.16465914249420166)\n",
            "Training set:32640/50000 (65.21739130434783 Loss:0.14349228143692017)\n",
            "Training set:33280/50000 (66.49616368286445 Loss:0.3204774856567383)\n",
            "Training set:33920/50000 (67.77493606138107 Loss:0.3832511603832245)\n",
            "Training set:34560/50000 (69.0537084398977 Loss:0.21536146104335785)\n",
            "Training set:35200/50000 (70.33248081841433 Loss:0.3495118319988251)\n",
            "Training set:35840/50000 (71.61125319693095 Loss:0.4176589846611023)\n",
            "Training set:36480/50000 (72.89002557544757 Loss:0.19667577743530273)\n",
            "Training set:37120/50000 (74.16879795396419 Loss:0.2561717629432678)\n",
            "Training set:37760/50000 (75.44757033248082 Loss:0.36547625064849854)\n",
            "Training set:38400/50000 (76.72634271099744 Loss:0.1894538402557373)\n",
            "Training set:39040/50000 (78.00511508951406 Loss:0.2415212243795395)\n",
            "Training set:39680/50000 (79.28388746803068 Loss:0.33822736144065857)\n",
            "Training set:40320/50000 (80.56265984654732 Loss:0.5075775980949402)\n",
            "Training set:40960/50000 (81.84143222506394 Loss:0.3424006700515747)\n",
            "Training set:41600/50000 (83.12020460358056 Loss:0.5496980547904968)\n",
            "Training set:42240/50000 (84.39897698209718 Loss:0.41565680503845215)\n",
            "Training set:42880/50000 (85.67774936061382 Loss:0.19823777675628662)\n",
            "Training set:43520/50000 (86.95652173913044 Loss:0.4217003881931305)\n",
            "Training set:44160/50000 (88.23529411764706 Loss:0.397087961435318)\n",
            "Training set:44800/50000 (89.51406649616368 Loss:0.45535650849342346)\n",
            "Training set:45440/50000 (90.79283887468031 Loss:0.4205431640148163)\n",
            "Training set:46080/50000 (92.07161125319693 Loss:0.5466680526733398)\n",
            "Training set:46720/50000 (93.35038363171356 Loss:0.3980710506439209)\n",
            "Training set:47360/50000 (94.62915601023018 Loss:0.4952114522457123)\n",
            "Training set:48000/50000 (95.9079283887468 Loss:0.4111901521682739)\n",
            "Training set:48640/50000 (97.18670076726343 Loss:0.18479175865650177)\n",
            "Training set:49280/50000 (98.46547314578005 Loss:0.33795034885406494)\n",
            "Training set:49920/50000 (99.74424552429667 Loss:0.35984405875205994)\n",
            "Training set: Average loss: 0.350713\n",
            "Validation set: Average loss: 0.5056592246909051, Accuracy: 8484/10000 (84.84%)\n",
            "Epoch: 22\n",
            "Training set:0/50000 (0.0 Loss:0.27726560831069946)\n",
            "Training set:640/50000 (1.278772378516624 Loss:0.38093140721321106)\n",
            "Training set:1280/50000 (2.557544757033248 Loss:0.2647627890110016)\n",
            "Training set:1920/50000 (3.836317135549872 Loss:0.4797152578830719)\n",
            "Training set:2560/50000 (5.115089514066496 Loss:0.33639001846313477)\n",
            "Training set:3200/50000 (6.3938618925831205 Loss:0.24345256388187408)\n",
            "Training set:3840/50000 (7.672634271099744 Loss:0.40446141362190247)\n",
            "Training set:4480/50000 (8.951406649616368 Loss:0.43961840867996216)\n",
            "Training set:5120/50000 (10.230179028132993 Loss:0.6827386021614075)\n",
            "Training set:5760/50000 (11.508951406649617 Loss:0.4397619962692261)\n",
            "Training set:6400/50000 (12.787723785166241 Loss:0.35888537764549255)\n",
            "Training set:7040/50000 (14.066496163682864 Loss:0.1554480344057083)\n",
            "Training set:7680/50000 (15.345268542199488 Loss:0.3593829870223999)\n",
            "Training set:8320/50000 (16.624040920716112 Loss:0.38348454236984253)\n",
            "Training set:8960/50000 (17.902813299232736 Loss:0.2080465853214264)\n",
            "Training set:9600/50000 (19.18158567774936 Loss:0.2987542450428009)\n",
            "Training set:10240/50000 (20.460358056265985 Loss:0.3088305592536926)\n",
            "Training set:10880/50000 (21.73913043478261 Loss:0.22469742596149445)\n",
            "Training set:11520/50000 (23.017902813299234 Loss:0.28832972049713135)\n",
            "Training set:12160/50000 (24.296675191815858 Loss:0.4050689935684204)\n",
            "Training set:12800/50000 (25.575447570332482 Loss:0.28222450613975525)\n",
            "Training set:13440/50000 (26.854219948849106 Loss:0.23175299167633057)\n",
            "Training set:14080/50000 (28.132992327365727 Loss:0.3614504635334015)\n",
            "Training set:14720/50000 (29.41176470588235 Loss:0.36553508043289185)\n",
            "Training set:15360/50000 (30.690537084398976 Loss:0.39520299434661865)\n",
            "Training set:16000/50000 (31.9693094629156 Loss:0.41573628783226013)\n",
            "Training set:16640/50000 (33.248081841432224 Loss:0.26635220646858215)\n",
            "Training set:17280/50000 (34.52685421994885 Loss:0.48232805728912354)\n",
            "Training set:17920/50000 (35.80562659846547 Loss:0.4707393944263458)\n",
            "Training set:18560/50000 (37.084398976982094 Loss:0.40033581852912903)\n",
            "Training set:19200/50000 (38.36317135549872 Loss:0.25587260723114014)\n",
            "Training set:19840/50000 (39.64194373401534 Loss:0.3099551498889923)\n",
            "Training set:20480/50000 (40.92071611253197 Loss:0.256869375705719)\n",
            "Training set:21120/50000 (42.19948849104859 Loss:0.2850963771343231)\n",
            "Training set:21760/50000 (43.47826086956522 Loss:0.37641412019729614)\n",
            "Training set:22400/50000 (44.75703324808184 Loss:0.2406812161207199)\n",
            "Training set:23040/50000 (46.03580562659847 Loss:0.2304394543170929)\n",
            "Training set:23680/50000 (47.31457800511509 Loss:0.31344982981681824)\n",
            "Training set:24320/50000 (48.593350383631716 Loss:0.23392772674560547)\n",
            "Training set:24960/50000 (49.87212276214834 Loss:0.35834717750549316)\n",
            "Training set:25600/50000 (51.150895140664964 Loss:0.28159961104393005)\n",
            "Training set:26240/50000 (52.429667519181585 Loss:0.3068295121192932)\n",
            "Training set:26880/50000 (53.70843989769821 Loss:0.33225375413894653)\n",
            "Training set:27520/50000 (54.987212276214834 Loss:0.31174641847610474)\n",
            "Training set:28160/50000 (56.265984654731454 Loss:0.32174575328826904)\n",
            "Training set:28800/50000 (57.54475703324808 Loss:0.37122949957847595)\n",
            "Training set:29440/50000 (58.8235294117647 Loss:0.3062668442726135)\n",
            "Training set:30080/50000 (60.10230179028133 Loss:0.4305282533168793)\n",
            "Training set:30720/50000 (61.38107416879795 Loss:0.24098312854766846)\n",
            "Training set:31360/50000 (62.65984654731458 Loss:0.3012448847293854)\n",
            "Training set:32000/50000 (63.9386189258312 Loss:0.38885700702667236)\n",
            "Training set:32640/50000 (65.21739130434783 Loss:0.31894853711128235)\n",
            "Training set:33280/50000 (66.49616368286445 Loss:0.3246035873889923)\n",
            "Training set:33920/50000 (67.77493606138107 Loss:0.3867771029472351)\n",
            "Training set:34560/50000 (69.0537084398977 Loss:0.228286474943161)\n",
            "Training set:35200/50000 (70.33248081841433 Loss:0.47665083408355713)\n",
            "Training set:35840/50000 (71.61125319693095 Loss:0.2864028811454773)\n",
            "Training set:36480/50000 (72.89002557544757 Loss:0.40095993876457214)\n",
            "Training set:37120/50000 (74.16879795396419 Loss:0.2712114155292511)\n",
            "Training set:37760/50000 (75.44757033248082 Loss:0.4067184031009674)\n",
            "Training set:38400/50000 (76.72634271099744 Loss:0.4027082622051239)\n",
            "Training set:39040/50000 (78.00511508951406 Loss:0.16724063456058502)\n",
            "Training set:39680/50000 (79.28388746803068 Loss:0.1910744607448578)\n",
            "Training set:40320/50000 (80.56265984654732 Loss:0.13448956608772278)\n",
            "Training set:40960/50000 (81.84143222506394 Loss:0.4036533832550049)\n",
            "Training set:41600/50000 (83.12020460358056 Loss:0.20068864524364471)\n",
            "Training set:42240/50000 (84.39897698209718 Loss:0.6274645924568176)\n",
            "Training set:42880/50000 (85.67774936061382 Loss:0.26495447754859924)\n",
            "Training set:43520/50000 (86.95652173913044 Loss:0.4741003215312958)\n",
            "Training set:44160/50000 (88.23529411764706 Loss:0.24378933012485504)\n",
            "Training set:44800/50000 (89.51406649616368 Loss:0.20173856616020203)\n",
            "Training set:45440/50000 (90.79283887468031 Loss:0.392031192779541)\n",
            "Training set:46080/50000 (92.07161125319693 Loss:0.5444439649581909)\n",
            "Training set:46720/50000 (93.35038363171356 Loss:0.3168644309043884)\n",
            "Training set:47360/50000 (94.62915601023018 Loss:0.33582931756973267)\n",
            "Training set:48000/50000 (95.9079283887468 Loss:0.33717507123947144)\n",
            "Training set:48640/50000 (97.18670076726343 Loss:0.21984218060970306)\n",
            "Training set:49280/50000 (98.46547314578005 Loss:0.4139016270637512)\n",
            "Training set:49920/50000 (99.74424552429667 Loss:0.32084622979164124)\n",
            "Training set: Average loss: 0.334942\n",
            "Validation set: Average loss: 0.48109382258099354, Accuracy: 8491/10000 (84.91%)\n",
            "Epoch: 23\n",
            "Training set:0/50000 (0.0 Loss:0.37024426460266113)\n",
            "Training set:640/50000 (1.278772378516624 Loss:0.35621514916419983)\n",
            "Training set:1280/50000 (2.557544757033248 Loss:0.24352297186851501)\n",
            "Training set:1920/50000 (3.836317135549872 Loss:0.2579977214336395)\n",
            "Training set:2560/50000 (5.115089514066496 Loss:0.3308180272579193)\n",
            "Training set:3200/50000 (6.3938618925831205 Loss:0.34430769085884094)\n",
            "Training set:3840/50000 (7.672634271099744 Loss:0.33609095215797424)\n",
            "Training set:4480/50000 (8.951406649616368 Loss:0.2260093092918396)\n",
            "Training set:5120/50000 (10.230179028132993 Loss:0.3295653164386749)\n",
            "Training set:5760/50000 (11.508951406649617 Loss:0.29634734988212585)\n",
            "Training set:6400/50000 (12.787723785166241 Loss:0.33555054664611816)\n",
            "Training set:7040/50000 (14.066496163682864 Loss:0.3173246681690216)\n",
            "Training set:7680/50000 (15.345268542199488 Loss:0.3171250820159912)\n",
            "Training set:8320/50000 (16.624040920716112 Loss:0.3532629609107971)\n",
            "Training set:8960/50000 (17.902813299232736 Loss:0.20336902141571045)\n",
            "Training set:9600/50000 (19.18158567774936 Loss:0.38021355867385864)\n",
            "Training set:10240/50000 (20.460358056265985 Loss:0.3044857680797577)\n",
            "Training set:10880/50000 (21.73913043478261 Loss:0.21592403948307037)\n",
            "Training set:11520/50000 (23.017902813299234 Loss:0.21409761905670166)\n",
            "Training set:12160/50000 (24.296675191815858 Loss:0.31889256834983826)\n",
            "Training set:12800/50000 (25.575447570332482 Loss:0.31953972578048706)\n",
            "Training set:13440/50000 (26.854219948849106 Loss:0.27723410725593567)\n",
            "Training set:14080/50000 (28.132992327365727 Loss:0.3866851031780243)\n",
            "Training set:14720/50000 (29.41176470588235 Loss:0.3143855631351471)\n",
            "Training set:15360/50000 (30.690537084398976 Loss:0.2451043576002121)\n",
            "Training set:16000/50000 (31.9693094629156 Loss:0.26433098316192627)\n",
            "Training set:16640/50000 (33.248081841432224 Loss:0.3630124628543854)\n",
            "Training set:17280/50000 (34.52685421994885 Loss:0.32129231095314026)\n",
            "Training set:17920/50000 (35.80562659846547 Loss:0.24512043595314026)\n",
            "Training set:18560/50000 (37.084398976982094 Loss:0.5100404024124146)\n",
            "Training set:19200/50000 (38.36317135549872 Loss:0.37676727771759033)\n",
            "Training set:19840/50000 (39.64194373401534 Loss:0.32935208082199097)\n",
            "Training set:20480/50000 (40.92071611253197 Loss:0.2861410975456238)\n",
            "Training set:21120/50000 (42.19948849104859 Loss:0.44281187653541565)\n",
            "Training set:21760/50000 (43.47826086956522 Loss:0.1602707952260971)\n",
            "Training set:22400/50000 (44.75703324808184 Loss:0.5006128549575806)\n",
            "Training set:23040/50000 (46.03580562659847 Loss:0.40557578206062317)\n",
            "Training set:23680/50000 (47.31457800511509 Loss:0.265792578458786)\n",
            "Training set:24320/50000 (48.593350383631716 Loss:0.3311065435409546)\n",
            "Training set:24960/50000 (49.87212276214834 Loss:0.31958258152008057)\n",
            "Training set:25600/50000 (51.150895140664964 Loss:0.20133714377880096)\n",
            "Training set:26240/50000 (52.429667519181585 Loss:0.2801503837108612)\n",
            "Training set:26880/50000 (53.70843989769821 Loss:0.37728506326675415)\n",
            "Training set:27520/50000 (54.987212276214834 Loss:0.1807626485824585)\n",
            "Training set:28160/50000 (56.265984654731454 Loss:0.2596743702888489)\n",
            "Training set:28800/50000 (57.54475703324808 Loss:0.34247106313705444)\n",
            "Training set:29440/50000 (58.8235294117647 Loss:0.4022965133190155)\n",
            "Training set:30080/50000 (60.10230179028133 Loss:0.2370869219303131)\n",
            "Training set:30720/50000 (61.38107416879795 Loss:0.2091599404811859)\n",
            "Training set:31360/50000 (62.65984654731458 Loss:0.11561749130487442)\n",
            "Training set:32000/50000 (63.9386189258312 Loss:0.17286646366119385)\n",
            "Training set:32640/50000 (65.21739130434783 Loss:0.22774165868759155)\n",
            "Training set:33280/50000 (66.49616368286445 Loss:0.4616946876049042)\n",
            "Training set:33920/50000 (67.77493606138107 Loss:0.41497886180877686)\n",
            "Training set:34560/50000 (69.0537084398977 Loss:0.23288202285766602)\n",
            "Training set:35200/50000 (70.33248081841433 Loss:0.31781521439552307)\n",
            "Training set:35840/50000 (71.61125319693095 Loss:0.18649061024188995)\n",
            "Training set:36480/50000 (72.89002557544757 Loss:0.18893976509571075)\n",
            "Training set:37120/50000 (74.16879795396419 Loss:0.2746833264827728)\n",
            "Training set:37760/50000 (75.44757033248082 Loss:0.24476580321788788)\n",
            "Training set:38400/50000 (76.72634271099744 Loss:0.3054514229297638)\n",
            "Training set:39040/50000 (78.00511508951406 Loss:0.23014982044696808)\n",
            "Training set:39680/50000 (79.28388746803068 Loss:0.218111053109169)\n",
            "Training set:40320/50000 (80.56265984654732 Loss:0.29816925525665283)\n",
            "Training set:40960/50000 (81.84143222506394 Loss:0.4485609829425812)\n",
            "Training set:41600/50000 (83.12020460358056 Loss:0.17276957631111145)\n",
            "Training set:42240/50000 (84.39897698209718 Loss:0.4702777862548828)\n",
            "Training set:42880/50000 (85.67774936061382 Loss:0.37661826610565186)\n",
            "Training set:43520/50000 (86.95652173913044 Loss:0.3017096519470215)\n",
            "Training set:44160/50000 (88.23529411764706 Loss:0.46005484461784363)\n",
            "Training set:44800/50000 (89.51406649616368 Loss:0.32792723178863525)\n",
            "Training set:45440/50000 (90.79283887468031 Loss:0.47296229004859924)\n",
            "Training set:46080/50000 (92.07161125319693 Loss:0.20191164314746857)\n",
            "Training set:46720/50000 (93.35038363171356 Loss:0.4593985974788666)\n",
            "Training set:47360/50000 (94.62915601023018 Loss:0.2785584628582001)\n",
            "Training set:48000/50000 (95.9079283887468 Loss:0.3774571418762207)\n",
            "Training set:48640/50000 (97.18670076726343 Loss:0.35064074397087097)\n",
            "Training set:49280/50000 (98.46547314578005 Loss:0.3066110908985138)\n",
            "Training set:49920/50000 (99.74424552429667 Loss:0.2611633539199829)\n",
            "Training set: Average loss: 0.319589\n",
            "Validation set: Average loss: 0.4679879888797262, Accuracy: 8604/10000 (86.04%)\n",
            "Epoch: 24\n",
            "Training set:0/50000 (0.0 Loss:0.42211514711380005)\n",
            "Training set:640/50000 (1.278772378516624 Loss:0.2936604619026184)\n",
            "Training set:1280/50000 (2.557544757033248 Loss:0.36054491996765137)\n",
            "Training set:1920/50000 (3.836317135549872 Loss:0.14949868619441986)\n",
            "Training set:2560/50000 (5.115089514066496 Loss:0.2694350481033325)\n",
            "Training set:3200/50000 (6.3938618925831205 Loss:0.22568580508232117)\n",
            "Training set:3840/50000 (7.672634271099744 Loss:0.30210447311401367)\n",
            "Training set:4480/50000 (8.951406649616368 Loss:0.1915876418352127)\n",
            "Training set:5120/50000 (10.230179028132993 Loss:0.46236559748649597)\n",
            "Training set:5760/50000 (11.508951406649617 Loss:0.18074649572372437)\n",
            "Training set:6400/50000 (12.787723785166241 Loss:0.27330681681632996)\n",
            "Training set:7040/50000 (14.066496163682864 Loss:0.3256323039531708)\n",
            "Training set:7680/50000 (15.345268542199488 Loss:0.3559061884880066)\n",
            "Training set:8320/50000 (16.624040920716112 Loss:0.43522006273269653)\n",
            "Training set:8960/50000 (17.902813299232736 Loss:0.21605665981769562)\n",
            "Training set:9600/50000 (19.18158567774936 Loss:0.2861706614494324)\n",
            "Training set:10240/50000 (20.460358056265985 Loss:0.19601617753505707)\n",
            "Training set:10880/50000 (21.73913043478261 Loss:0.3467981219291687)\n",
            "Training set:11520/50000 (23.017902813299234 Loss:0.398483544588089)\n",
            "Training set:12160/50000 (24.296675191815858 Loss:0.2983553111553192)\n",
            "Training set:12800/50000 (25.575447570332482 Loss:0.2875140905380249)\n",
            "Training set:13440/50000 (26.854219948849106 Loss:0.47839948534965515)\n",
            "Training set:14080/50000 (28.132992327365727 Loss:0.23233141005039215)\n",
            "Training set:14720/50000 (29.41176470588235 Loss:0.19357386231422424)\n",
            "Training set:15360/50000 (30.690537084398976 Loss:0.258134663105011)\n",
            "Training set:16000/50000 (31.9693094629156 Loss:0.15718507766723633)\n",
            "Training set:16640/50000 (33.248081841432224 Loss:0.1294432431459427)\n",
            "Training set:17280/50000 (34.52685421994885 Loss:0.4055943489074707)\n",
            "Training set:17920/50000 (35.80562659846547 Loss:0.2870543599128723)\n",
            "Training set:18560/50000 (37.084398976982094 Loss:0.32841381430625916)\n",
            "Training set:19200/50000 (38.36317135549872 Loss:0.4178101718425751)\n",
            "Training set:19840/50000 (39.64194373401534 Loss:0.1679098904132843)\n",
            "Training set:20480/50000 (40.92071611253197 Loss:0.41130390763282776)\n",
            "Training set:21120/50000 (42.19948849104859 Loss:0.19969163835048676)\n",
            "Training set:21760/50000 (43.47826086956522 Loss:0.38135209679603577)\n",
            "Training set:22400/50000 (44.75703324808184 Loss:0.35280856490135193)\n",
            "Training set:23040/50000 (46.03580562659847 Loss:0.13153064250946045)\n",
            "Training set:23680/50000 (47.31457800511509 Loss:0.16799362003803253)\n",
            "Training set:24320/50000 (48.593350383631716 Loss:0.315677285194397)\n",
            "Training set:24960/50000 (49.87212276214834 Loss:0.3070915639400482)\n",
            "Training set:25600/50000 (51.150895140664964 Loss:0.3468571901321411)\n",
            "Training set:26240/50000 (52.429667519181585 Loss:0.34918293356895447)\n",
            "Training set:26880/50000 (53.70843989769821 Loss:0.3456075191497803)\n",
            "Training set:27520/50000 (54.987212276214834 Loss:0.3618638217449188)\n",
            "Training set:28160/50000 (56.265984654731454 Loss:0.338882178068161)\n",
            "Training set:28800/50000 (57.54475703324808 Loss:0.27978479862213135)\n",
            "Training set:29440/50000 (58.8235294117647 Loss:0.36270222067832947)\n",
            "Training set:30080/50000 (60.10230179028133 Loss:0.1774430274963379)\n",
            "Training set:30720/50000 (61.38107416879795 Loss:0.17327699065208435)\n",
            "Training set:31360/50000 (62.65984654731458 Loss:0.13778191804885864)\n",
            "Training set:32000/50000 (63.9386189258312 Loss:0.27988964319229126)\n",
            "Training set:32640/50000 (65.21739130434783 Loss:0.24027085304260254)\n",
            "Training set:33280/50000 (66.49616368286445 Loss:0.352173775434494)\n",
            "Training set:33920/50000 (67.77493606138107 Loss:0.22338953614234924)\n",
            "Training set:34560/50000 (69.0537084398977 Loss:0.3690977394580841)\n",
            "Training set:35200/50000 (70.33248081841433 Loss:0.38900232315063477)\n",
            "Training set:35840/50000 (71.61125319693095 Loss:0.2637224495410919)\n",
            "Training set:36480/50000 (72.89002557544757 Loss:0.2346232682466507)\n",
            "Training set:37120/50000 (74.16879795396419 Loss:0.13917231559753418)\n",
            "Training set:37760/50000 (75.44757033248082 Loss:0.43054407835006714)\n",
            "Training set:38400/50000 (76.72634271099744 Loss:0.16399218142032623)\n",
            "Training set:39040/50000 (78.00511508951406 Loss:0.16190119087696075)\n",
            "Training set:39680/50000 (79.28388746803068 Loss:0.33195361495018005)\n",
            "Training set:40320/50000 (80.56265984654732 Loss:0.3538045883178711)\n",
            "Training set:40960/50000 (81.84143222506394 Loss:0.24271441996097565)\n",
            "Training set:41600/50000 (83.12020460358056 Loss:0.401358038187027)\n",
            "Training set:42240/50000 (84.39897698209718 Loss:0.40863317251205444)\n",
            "Training set:42880/50000 (85.67774936061382 Loss:0.20375002920627594)\n",
            "Training set:43520/50000 (86.95652173913044 Loss:0.27790603041648865)\n",
            "Training set:44160/50000 (88.23529411764706 Loss:0.22921045124530792)\n",
            "Training set:44800/50000 (89.51406649616368 Loss:0.34840017557144165)\n",
            "Training set:45440/50000 (90.79283887468031 Loss:0.3948139250278473)\n",
            "Training set:46080/50000 (92.07161125319693 Loss:0.21123084425926208)\n",
            "Training set:46720/50000 (93.35038363171356 Loss:0.5968721508979797)\n",
            "Training set:47360/50000 (94.62915601023018 Loss:0.24188439548015594)\n",
            "Training set:48000/50000 (95.9079283887468 Loss:0.20303499698638916)\n",
            "Training set:48640/50000 (97.18670076726343 Loss:0.36609992384910583)\n",
            "Training set:49280/50000 (98.46547314578005 Loss:0.2941323220729828)\n",
            "Training set:49920/50000 (99.74424552429667 Loss:0.24129575490951538)\n",
            "Training set: Average loss: 0.301912\n",
            "Validation set: Average loss: 0.45363988319210186, Accuracy: 8612/10000 (86.12%)\n",
            "Epoch: 25\n",
            "Training set:0/50000 (0.0 Loss:0.24062508344650269)\n",
            "Training set:640/50000 (1.278772378516624 Loss:0.3451979458332062)\n",
            "Training set:1280/50000 (2.557544757033248 Loss:0.25080326199531555)\n",
            "Training set:1920/50000 (3.836317135549872 Loss:0.2889827489852905)\n",
            "Training set:2560/50000 (5.115089514066496 Loss:0.2411724478006363)\n",
            "Training set:3200/50000 (6.3938618925831205 Loss:0.36732175946235657)\n",
            "Training set:3840/50000 (7.672634271099744 Loss:0.22813625633716583)\n",
            "Training set:4480/50000 (8.951406649616368 Loss:0.37654566764831543)\n",
            "Training set:5120/50000 (10.230179028132993 Loss:0.15850847959518433)\n",
            "Training set:5760/50000 (11.508951406649617 Loss:0.2851526737213135)\n",
            "Training set:6400/50000 (12.787723785166241 Loss:0.43419545888900757)\n",
            "Training set:7040/50000 (14.066496163682864 Loss:0.18054869771003723)\n",
            "Training set:7680/50000 (15.345268542199488 Loss:0.1667618751525879)\n",
            "Training set:8320/50000 (16.624040920716112 Loss:0.3877667486667633)\n",
            "Training set:8960/50000 (17.902813299232736 Loss:0.40337684750556946)\n",
            "Training set:9600/50000 (19.18158567774936 Loss:0.34718742966651917)\n",
            "Training set:10240/50000 (20.460358056265985 Loss:0.34112006425857544)\n",
            "Training set:10880/50000 (21.73913043478261 Loss:0.21221402287483215)\n",
            "Training set:11520/50000 (23.017902813299234 Loss:0.22428777813911438)\n",
            "Training set:12160/50000 (24.296675191815858 Loss:0.3766573965549469)\n",
            "Training set:12800/50000 (25.575447570332482 Loss:0.2934555113315582)\n",
            "Training set:13440/50000 (26.854219948849106 Loss:0.21763008832931519)\n",
            "Training set:14080/50000 (28.132992327365727 Loss:0.22608205676078796)\n",
            "Training set:14720/50000 (29.41176470588235 Loss:0.39348575472831726)\n",
            "Training set:15360/50000 (30.690537084398976 Loss:0.1887577921152115)\n",
            "Training set:16000/50000 (31.9693094629156 Loss:0.2370993196964264)\n",
            "Training set:16640/50000 (33.248081841432224 Loss:0.3689084053039551)\n",
            "Training set:17280/50000 (34.52685421994885 Loss:0.27780625224113464)\n",
            "Training set:17920/50000 (35.80562659846547 Loss:0.319254606962204)\n",
            "Training set:18560/50000 (37.084398976982094 Loss:0.2333565354347229)\n",
            "Training set:19200/50000 (38.36317135549872 Loss:0.26958519220352173)\n",
            "Training set:19840/50000 (39.64194373401534 Loss:0.25973081588745117)\n",
            "Training set:20480/50000 (40.92071611253197 Loss:0.3287156820297241)\n",
            "Training set:21120/50000 (42.19948849104859 Loss:0.4559791088104248)\n",
            "Training set:21760/50000 (43.47826086956522 Loss:0.39901185035705566)\n",
            "Training set:22400/50000 (44.75703324808184 Loss:0.2756645083427429)\n",
            "Training set:23040/50000 (46.03580562659847 Loss:0.31442639231681824)\n",
            "Training set:23680/50000 (47.31457800511509 Loss:0.37924665212631226)\n",
            "Training set:24320/50000 (48.593350383631716 Loss:0.35958054661750793)\n",
            "Training set:24960/50000 (49.87212276214834 Loss:0.2497498244047165)\n",
            "Training set:25600/50000 (51.150895140664964 Loss:0.33946582674980164)\n",
            "Training set:26240/50000 (52.429667519181585 Loss:0.4571482837200165)\n",
            "Training set:26880/50000 (53.70843989769821 Loss:0.259342223405838)\n",
            "Training set:27520/50000 (54.987212276214834 Loss:0.2704175114631653)\n",
            "Training set:28160/50000 (56.265984654731454 Loss:0.37195929884910583)\n",
            "Training set:28800/50000 (57.54475703324808 Loss:0.2684839069843292)\n",
            "Training set:29440/50000 (58.8235294117647 Loss:0.33356064558029175)\n",
            "Training set:30080/50000 (60.10230179028133 Loss:0.33091259002685547)\n",
            "Training set:30720/50000 (61.38107416879795 Loss:0.145400732755661)\n",
            "Training set:31360/50000 (62.65984654731458 Loss:0.2977242171764374)\n",
            "Training set:32000/50000 (63.9386189258312 Loss:0.26366814970970154)\n",
            "Training set:32640/50000 (65.21739130434783 Loss:0.19658581912517548)\n",
            "Training set:33280/50000 (66.49616368286445 Loss:0.20094260573387146)\n",
            "Training set:33920/50000 (67.77493606138107 Loss:0.1999141126871109)\n",
            "Training set:34560/50000 (69.0537084398977 Loss:0.29314395785331726)\n",
            "Training set:35200/50000 (70.33248081841433 Loss:0.4284021854400635)\n",
            "Training set:35840/50000 (71.61125319693095 Loss:0.3233785927295685)\n",
            "Training set:36480/50000 (72.89002557544757 Loss:0.29005563259124756)\n",
            "Training set:37120/50000 (74.16879795396419 Loss:0.22250334918498993)\n",
            "Training set:37760/50000 (75.44757033248082 Loss:0.16850443184375763)\n",
            "Training set:38400/50000 (76.72634271099744 Loss:0.28960174322128296)\n",
            "Training set:39040/50000 (78.00511508951406 Loss:0.22221820056438446)\n",
            "Training set:39680/50000 (79.28388746803068 Loss:0.2566322684288025)\n",
            "Training set:40320/50000 (80.56265984654732 Loss:0.2742745280265808)\n",
            "Training set:40960/50000 (81.84143222506394 Loss:0.3578609228134155)\n",
            "Training set:41600/50000 (83.12020460358056 Loss:0.3222704827785492)\n",
            "Training set:42240/50000 (84.39897698209718 Loss:0.35484012961387634)\n",
            "Training set:42880/50000 (85.67774936061382 Loss:0.35415616631507874)\n",
            "Training set:43520/50000 (86.95652173913044 Loss:0.14258626103401184)\n",
            "Training set:44160/50000 (88.23529411764706 Loss:0.3345698118209839)\n",
            "Training set:44800/50000 (89.51406649616368 Loss:0.24379438161849976)\n",
            "Training set:45440/50000 (90.79283887468031 Loss:0.19614994525909424)\n",
            "Training set:46080/50000 (92.07161125319693 Loss:0.24710093438625336)\n",
            "Training set:46720/50000 (93.35038363171356 Loss:0.1630912870168686)\n",
            "Training set:47360/50000 (94.62915601023018 Loss:0.16253598034381866)\n",
            "Training set:48000/50000 (95.9079283887468 Loss:0.3976145386695862)\n",
            "Training set:48640/50000 (97.18670076726343 Loss:0.16569112241268158)\n",
            "Training set:49280/50000 (98.46547314578005 Loss:0.3064769506454468)\n",
            "Training set:49920/50000 (99.74424552429667 Loss:0.3008877635002136)\n",
            "Training set: Average loss: 0.289229\n",
            "Validation set: Average loss: 0.4697062345636878, Accuracy: 8583/10000 (85.83%)\n",
            "Epoch: 26\n",
            "Training set:0/50000 (0.0 Loss:0.24867558479309082)\n",
            "Training set:640/50000 (1.278772378516624 Loss:0.17312654852867126)\n",
            "Training set:1280/50000 (2.557544757033248 Loss:0.22552043199539185)\n",
            "Training set:1920/50000 (3.836317135549872 Loss:0.44388508796691895)\n",
            "Training set:2560/50000 (5.115089514066496 Loss:0.31214800477027893)\n",
            "Training set:3200/50000 (6.3938618925831205 Loss:0.301923543214798)\n",
            "Training set:3840/50000 (7.672634271099744 Loss:0.337708443403244)\n",
            "Training set:4480/50000 (8.951406649616368 Loss:0.3463214933872223)\n",
            "Training set:5120/50000 (10.230179028132993 Loss:0.2173910140991211)\n",
            "Training set:5760/50000 (11.508951406649617 Loss:0.27612540125846863)\n",
            "Training set:6400/50000 (12.787723785166241 Loss:0.22976411879062653)\n",
            "Training set:7040/50000 (14.066496163682864 Loss:0.2098989635705948)\n",
            "Training set:7680/50000 (15.345268542199488 Loss:0.33021506667137146)\n",
            "Training set:8320/50000 (16.624040920716112 Loss:0.446310430765152)\n",
            "Training set:8960/50000 (17.902813299232736 Loss:0.3821738064289093)\n",
            "Training set:9600/50000 (19.18158567774936 Loss:0.3042449355125427)\n",
            "Training set:10240/50000 (20.460358056265985 Loss:0.17208009958267212)\n",
            "Training set:10880/50000 (21.73913043478261 Loss:0.31570887565612793)\n",
            "Training set:11520/50000 (23.017902813299234 Loss:0.2319929152727127)\n",
            "Training set:12160/50000 (24.296675191815858 Loss:0.46706634759902954)\n",
            "Training set:12800/50000 (25.575447570332482 Loss:0.3159223198890686)\n",
            "Training set:13440/50000 (26.854219948849106 Loss:0.3428419828414917)\n",
            "Training set:14080/50000 (28.132992327365727 Loss:0.18056198954582214)\n",
            "Training set:14720/50000 (29.41176470588235 Loss:0.24752502143383026)\n",
            "Training set:15360/50000 (30.690537084398976 Loss:0.2198311984539032)\n",
            "Training set:16000/50000 (31.9693094629156 Loss:0.18089056015014648)\n",
            "Training set:16640/50000 (33.248081841432224 Loss:0.2711573541164398)\n",
            "Training set:17280/50000 (34.52685421994885 Loss:0.14092887938022614)\n",
            "Training set:17920/50000 (35.80562659846547 Loss:0.24341794848442078)\n",
            "Training set:18560/50000 (37.084398976982094 Loss:0.08931157737970352)\n",
            "Training set:19200/50000 (38.36317135549872 Loss:0.29222947359085083)\n",
            "Training set:19840/50000 (39.64194373401534 Loss:0.24918703734874725)\n",
            "Training set:20480/50000 (40.92071611253197 Loss:0.30192726850509644)\n",
            "Training set:21120/50000 (42.19948849104859 Loss:0.6050267219543457)\n",
            "Training set:21760/50000 (43.47826086956522 Loss:0.2849139869213104)\n",
            "Training set:22400/50000 (44.75703324808184 Loss:0.19286470115184784)\n",
            "Training set:23040/50000 (46.03580562659847 Loss:0.43248865008354187)\n",
            "Training set:23680/50000 (47.31457800511509 Loss:0.3227789103984833)\n",
            "Training set:24320/50000 (48.593350383631716 Loss:0.28503748774528503)\n",
            "Training set:24960/50000 (49.87212276214834 Loss:0.2711232900619507)\n",
            "Training set:25600/50000 (51.150895140664964 Loss:0.28846311569213867)\n",
            "Training set:26240/50000 (52.429667519181585 Loss:0.32821422815322876)\n",
            "Training set:26880/50000 (53.70843989769821 Loss:0.3848719894886017)\n",
            "Training set:27520/50000 (54.987212276214834 Loss:0.3131302297115326)\n",
            "Training set:28160/50000 (56.265984654731454 Loss:0.4941694438457489)\n",
            "Training set:28800/50000 (57.54475703324808 Loss:0.22857868671417236)\n",
            "Training set:29440/50000 (58.8235294117647 Loss:0.2187664657831192)\n",
            "Training set:30080/50000 (60.10230179028133 Loss:0.3242858946323395)\n",
            "Training set:30720/50000 (61.38107416879795 Loss:0.12439658492803574)\n",
            "Training set:31360/50000 (62.65984654731458 Loss:0.3128173351287842)\n",
            "Training set:32000/50000 (63.9386189258312 Loss:0.2768501341342926)\n",
            "Training set:32640/50000 (65.21739130434783 Loss:0.5285675525665283)\n",
            "Training set:33280/50000 (66.49616368286445 Loss:0.3608900308609009)\n",
            "Training set:33920/50000 (67.77493606138107 Loss:0.16642437875270844)\n",
            "Training set:34560/50000 (69.0537084398977 Loss:0.2822083532810211)\n",
            "Training set:35200/50000 (70.33248081841433 Loss:0.3220224380493164)\n",
            "Training set:35840/50000 (71.61125319693095 Loss:0.3091405928134918)\n",
            "Training set:36480/50000 (72.89002557544757 Loss:0.4662056267261505)\n",
            "Training set:37120/50000 (74.16879795396419 Loss:0.3418513834476471)\n",
            "Training set:37760/50000 (75.44757033248082 Loss:0.19779783487319946)\n",
            "Training set:38400/50000 (76.72634271099744 Loss:0.2717781364917755)\n",
            "Training set:39040/50000 (78.00511508951406 Loss:0.4413212537765503)\n",
            "Training set:39680/50000 (79.28388746803068 Loss:0.2870257794857025)\n",
            "Training set:40320/50000 (80.56265984654732 Loss:0.25073590874671936)\n",
            "Training set:40960/50000 (81.84143222506394 Loss:0.1851942092180252)\n",
            "Training set:41600/50000 (83.12020460358056 Loss:0.12514157593250275)\n",
            "Training set:42240/50000 (84.39897698209718 Loss:0.39392367005348206)\n",
            "Training set:42880/50000 (85.67774936061382 Loss:0.29256579279899597)\n",
            "Training set:43520/50000 (86.95652173913044 Loss:0.25550734996795654)\n",
            "Training set:44160/50000 (88.23529411764706 Loss:0.6191791892051697)\n",
            "Training set:44800/50000 (89.51406649616368 Loss:0.1993861049413681)\n",
            "Training set:45440/50000 (90.79283887468031 Loss:0.3327949643135071)\n",
            "Training set:46080/50000 (92.07161125319693 Loss:0.3063536286354065)\n",
            "Training set:46720/50000 (93.35038363171356 Loss:0.6965147256851196)\n",
            "Training set:47360/50000 (94.62915601023018 Loss:0.4041416347026825)\n",
            "Training set:48000/50000 (95.9079283887468 Loss:0.23512613773345947)\n",
            "Training set:48640/50000 (97.18670076726343 Loss:0.5784879326820374)\n",
            "Training set:49280/50000 (98.46547314578005 Loss:0.20968151092529297)\n",
            "Training set:49920/50000 (99.74424552429667 Loss:0.4963153004646301)\n",
            "Training set: Average loss: 0.284523\n",
            "Validation set: Average loss: 0.4601509353252733, Accuracy: 8616/10000 (86.16%)\n",
            "Epoch: 27\n",
            "Training set:0/50000 (0.0 Loss:0.16838529706001282)\n",
            "Training set:640/50000 (1.278772378516624 Loss:0.3996143937110901)\n",
            "Training set:1280/50000 (2.557544757033248 Loss:0.24593104422092438)\n",
            "Training set:1920/50000 (3.836317135549872 Loss:0.15499891340732574)\n",
            "Training set:2560/50000 (5.115089514066496 Loss:0.2629086971282959)\n",
            "Training set:3200/50000 (6.3938618925831205 Loss:0.29441606998443604)\n",
            "Training set:3840/50000 (7.672634271099744 Loss:0.29538512229919434)\n",
            "Training set:4480/50000 (8.951406649616368 Loss:0.15428400039672852)\n",
            "Training set:5120/50000 (10.230179028132993 Loss:0.2798343896865845)\n",
            "Training set:5760/50000 (11.508951406649617 Loss:0.2874091565608978)\n",
            "Training set:6400/50000 (12.787723785166241 Loss:0.14472776651382446)\n",
            "Training set:7040/50000 (14.066496163682864 Loss:0.27162715792655945)\n",
            "Training set:7680/50000 (15.345268542199488 Loss:0.3044396638870239)\n",
            "Training set:8320/50000 (16.624040920716112 Loss:0.43997180461883545)\n",
            "Training set:8960/50000 (17.902813299232736 Loss:0.11668828129768372)\n",
            "Training set:9600/50000 (19.18158567774936 Loss:0.3113318085670471)\n",
            "Training set:10240/50000 (20.460358056265985 Loss:0.21797062456607819)\n",
            "Training set:10880/50000 (21.73913043478261 Loss:0.13654077053070068)\n",
            "Training set:11520/50000 (23.017902813299234 Loss:0.17955750226974487)\n",
            "Training set:12160/50000 (24.296675191815858 Loss:0.26104211807250977)\n",
            "Training set:12800/50000 (25.575447570332482 Loss:0.2529796361923218)\n",
            "Training set:13440/50000 (26.854219948849106 Loss:0.3009383976459503)\n",
            "Training set:14080/50000 (28.132992327365727 Loss:0.3676054775714874)\n",
            "Training set:14720/50000 (29.41176470588235 Loss:0.35376015305519104)\n",
            "Training set:15360/50000 (30.690537084398976 Loss:0.273906946182251)\n",
            "Training set:16000/50000 (31.9693094629156 Loss:0.3122798800468445)\n",
            "Training set:16640/50000 (33.248081841432224 Loss:0.40811893343925476)\n",
            "Training set:17280/50000 (34.52685421994885 Loss:0.1367599070072174)\n",
            "Training set:17920/50000 (35.80562659846547 Loss:0.11655959486961365)\n",
            "Training set:18560/50000 (37.084398976982094 Loss:0.28326699137687683)\n",
            "Training set:19200/50000 (38.36317135549872 Loss:0.3413538336753845)\n",
            "Training set:19840/50000 (39.64194373401534 Loss:0.21965518593788147)\n",
            "Training set:20480/50000 (40.92071611253197 Loss:0.12245827913284302)\n",
            "Training set:21120/50000 (42.19948849104859 Loss:0.2332247644662857)\n",
            "Training set:21760/50000 (43.47826086956522 Loss:0.21922120451927185)\n",
            "Training set:22400/50000 (44.75703324808184 Loss:0.20624381303787231)\n",
            "Training set:23040/50000 (46.03580562659847 Loss:0.33548977971076965)\n",
            "Training set:23680/50000 (47.31457800511509 Loss:0.29060330986976624)\n",
            "Training set:24320/50000 (48.593350383631716 Loss:0.22383949160575867)\n",
            "Training set:24960/50000 (49.87212276214834 Loss:0.1548517346382141)\n",
            "Training set:25600/50000 (51.150895140664964 Loss:0.08077848702669144)\n",
            "Training set:26240/50000 (52.429667519181585 Loss:0.30348876118659973)\n",
            "Training set:26880/50000 (53.70843989769821 Loss:0.2118123471736908)\n",
            "Training set:27520/50000 (54.987212276214834 Loss:0.19960631430149078)\n",
            "Training set:28160/50000 (56.265984654731454 Loss:0.26437488198280334)\n",
            "Training set:28800/50000 (57.54475703324808 Loss:0.19362503290176392)\n",
            "Training set:29440/50000 (58.8235294117647 Loss:0.18932394683361053)\n",
            "Training set:30080/50000 (60.10230179028133 Loss:0.15825200080871582)\n",
            "Training set:30720/50000 (61.38107416879795 Loss:0.34503933787345886)\n",
            "Training set:31360/50000 (62.65984654731458 Loss:0.2814653217792511)\n",
            "Training set:32000/50000 (63.9386189258312 Loss:0.29409343004226685)\n",
            "Training set:32640/50000 (65.21739130434783 Loss:0.2916780114173889)\n",
            "Training set:33280/50000 (66.49616368286445 Loss:0.3393407464027405)\n",
            "Training set:33920/50000 (67.77493606138107 Loss:0.3026311993598938)\n",
            "Training set:34560/50000 (69.0537084398977 Loss:0.4988479018211365)\n",
            "Training set:35200/50000 (70.33248081841433 Loss:0.20181341469287872)\n",
            "Training set:35840/50000 (71.61125319693095 Loss:0.5025367736816406)\n",
            "Training set:36480/50000 (72.89002557544757 Loss:0.27109941840171814)\n",
            "Training set:37120/50000 (74.16879795396419 Loss:0.1578470915555954)\n",
            "Training set:37760/50000 (75.44757033248082 Loss:0.1181492879986763)\n",
            "Training set:38400/50000 (76.72634271099744 Loss:0.2637771666049957)\n",
            "Training set:39040/50000 (78.00511508951406 Loss:0.25061047077178955)\n",
            "Training set:39680/50000 (79.28388746803068 Loss:0.1727551817893982)\n",
            "Training set:40320/50000 (80.56265984654732 Loss:0.20769329369068146)\n",
            "Training set:40960/50000 (81.84143222506394 Loss:0.2674170434474945)\n",
            "Training set:41600/50000 (83.12020460358056 Loss:0.521199107170105)\n",
            "Training set:42240/50000 (84.39897698209718 Loss:0.2957124710083008)\n",
            "Training set:42880/50000 (85.67774936061382 Loss:0.13598832488059998)\n",
            "Training set:43520/50000 (86.95652173913044 Loss:0.3825797736644745)\n",
            "Training set:44160/50000 (88.23529411764706 Loss:0.22638823091983795)\n",
            "Training set:44800/50000 (89.51406649616368 Loss:0.3991873562335968)\n",
            "Training set:45440/50000 (90.79283887468031 Loss:0.2658327519893646)\n",
            "Training set:46080/50000 (92.07161125319693 Loss:0.5470554232597351)\n",
            "Training set:46720/50000 (93.35038363171356 Loss:0.19463330507278442)\n",
            "Training set:47360/50000 (94.62915601023018 Loss:0.22630304098129272)\n",
            "Training set:48000/50000 (95.9079283887468 Loss:0.4661784768104553)\n",
            "Training set:48640/50000 (97.18670076726343 Loss:0.3619789183139801)\n",
            "Training set:49280/50000 (98.46547314578005 Loss:0.26571959257125854)\n",
            "Training set:49920/50000 (99.74424552429667 Loss:0.36705923080444336)\n",
            "Training set: Average loss: 0.271862\n",
            "Validation set: Average loss: 0.4314961780313474, Accuracy: 8718/10000 (87.18%)\n",
            "Epoch: 28\n",
            "Training set:0/50000 (0.0 Loss:0.2515566647052765)\n",
            "Training set:640/50000 (1.278772378516624 Loss:0.25922057032585144)\n",
            "Training set:1280/50000 (2.557544757033248 Loss:0.32027390599250793)\n",
            "Training set:1920/50000 (3.836317135549872 Loss:0.3637825846672058)\n",
            "Training set:2560/50000 (5.115089514066496 Loss:0.27584412693977356)\n",
            "Training set:3200/50000 (6.3938618925831205 Loss:0.19365908205509186)\n",
            "Training set:3840/50000 (7.672634271099744 Loss:0.25557971000671387)\n",
            "Training set:4480/50000 (8.951406649616368 Loss:0.3758091628551483)\n",
            "Training set:5120/50000 (10.230179028132993 Loss:0.16545774042606354)\n",
            "Training set:5760/50000 (11.508951406649617 Loss:0.21102681756019592)\n",
            "Training set:6400/50000 (12.787723785166241 Loss:0.22425657510757446)\n",
            "Training set:7040/50000 (14.066496163682864 Loss:0.3178548216819763)\n",
            "Training set:7680/50000 (15.345268542199488 Loss:0.1798345148563385)\n",
            "Training set:8320/50000 (16.624040920716112 Loss:0.31237393617630005)\n",
            "Training set:8960/50000 (17.902813299232736 Loss:0.456674188375473)\n",
            "Training set:9600/50000 (19.18158567774936 Loss:0.3172016441822052)\n",
            "Training set:10240/50000 (20.460358056265985 Loss:0.13769197463989258)\n",
            "Training set:10880/50000 (21.73913043478261 Loss:0.12130597978830338)\n",
            "Training set:11520/50000 (23.017902813299234 Loss:0.35489997267723083)\n",
            "Training set:12160/50000 (24.296675191815858 Loss:0.3181873559951782)\n",
            "Training set:12800/50000 (25.575447570332482 Loss:0.2125605195760727)\n",
            "Training set:13440/50000 (26.854219948849106 Loss:0.40261417627334595)\n",
            "Training set:14080/50000 (28.132992327365727 Loss:0.48144054412841797)\n",
            "Training set:14720/50000 (29.41176470588235 Loss:0.14869548380374908)\n",
            "Training set:15360/50000 (30.690537084398976 Loss:0.32819199562072754)\n",
            "Training set:16000/50000 (31.9693094629156 Loss:0.20334886014461517)\n",
            "Training set:16640/50000 (33.248081841432224 Loss:0.19997568428516388)\n",
            "Training set:17280/50000 (34.52685421994885 Loss:0.11422040313482285)\n",
            "Training set:17920/50000 (35.80562659846547 Loss:0.3446691632270813)\n",
            "Training set:18560/50000 (37.084398976982094 Loss:0.26052701473236084)\n",
            "Training set:19200/50000 (38.36317135549872 Loss:0.25275588035583496)\n",
            "Training set:19840/50000 (39.64194373401534 Loss:0.16197055578231812)\n",
            "Training set:20480/50000 (40.92071611253197 Loss:0.16039912402629852)\n",
            "Training set:21120/50000 (42.19948849104859 Loss:0.18749789893627167)\n",
            "Training set:21760/50000 (43.47826086956522 Loss:0.21531759202480316)\n",
            "Training set:22400/50000 (44.75703324808184 Loss:0.23939339816570282)\n",
            "Training set:23040/50000 (46.03580562659847 Loss:0.19336317479610443)\n",
            "Training set:23680/50000 (47.31457800511509 Loss:0.19786669313907623)\n",
            "Training set:24320/50000 (48.593350383631716 Loss:0.44612008333206177)\n",
            "Training set:24960/50000 (49.87212276214834 Loss:0.16981928050518036)\n",
            "Training set:25600/50000 (51.150895140664964 Loss:0.23858344554901123)\n",
            "Training set:26240/50000 (52.429667519181585 Loss:0.22679445147514343)\n",
            "Training set:26880/50000 (53.70843989769821 Loss:0.2170591950416565)\n",
            "Training set:27520/50000 (54.987212276214834 Loss:0.14158518612384796)\n",
            "Training set:28160/50000 (56.265984654731454 Loss:0.23717190325260162)\n",
            "Training set:28800/50000 (57.54475703324808 Loss:0.1547473967075348)\n",
            "Training set:29440/50000 (58.8235294117647 Loss:0.4244617223739624)\n",
            "Training set:30080/50000 (60.10230179028133 Loss:0.165413036942482)\n",
            "Training set:30720/50000 (61.38107416879795 Loss:0.24898934364318848)\n",
            "Training set:31360/50000 (62.65984654731458 Loss:0.20314323902130127)\n",
            "Training set:32000/50000 (63.9386189258312 Loss:0.09810832142829895)\n",
            "Training set:32640/50000 (65.21739130434783 Loss:0.15425843000411987)\n",
            "Training set:33280/50000 (66.49616368286445 Loss:0.37219351530075073)\n",
            "Training set:33920/50000 (67.77493606138107 Loss:0.2486691027879715)\n",
            "Training set:34560/50000 (69.0537084398977 Loss:0.21845746040344238)\n",
            "Training set:35200/50000 (70.33248081841433 Loss:0.17888808250427246)\n",
            "Training set:35840/50000 (71.61125319693095 Loss:0.32142502069473267)\n",
            "Training set:36480/50000 (72.89002557544757 Loss:0.27086520195007324)\n",
            "Training set:37120/50000 (74.16879795396419 Loss:0.3749783933162689)\n",
            "Training set:37760/50000 (75.44757033248082 Loss:0.25148651003837585)\n",
            "Training set:38400/50000 (76.72634271099744 Loss:0.15304526686668396)\n",
            "Training set:39040/50000 (78.00511508951406 Loss:0.2278057187795639)\n",
            "Training set:39680/50000 (79.28388746803068 Loss:0.37638431787490845)\n",
            "Training set:40320/50000 (80.56265984654732 Loss:0.13516780734062195)\n",
            "Training set:40960/50000 (81.84143222506394 Loss:0.36540237069129944)\n",
            "Training set:41600/50000 (83.12020460358056 Loss:0.20346586406230927)\n",
            "Training set:42240/50000 (84.39897698209718 Loss:0.2659139633178711)\n",
            "Training set:42880/50000 (85.67774936061382 Loss:0.2161734402179718)\n",
            "Training set:43520/50000 (86.95652173913044 Loss:0.1754089593887329)\n",
            "Training set:44160/50000 (88.23529411764706 Loss:0.31543585658073425)\n",
            "Training set:44800/50000 (89.51406649616368 Loss:0.5614309310913086)\n",
            "Training set:45440/50000 (90.79283887468031 Loss:0.1925613284111023)\n",
            "Training set:46080/50000 (92.07161125319693 Loss:0.3488270938396454)\n",
            "Training set:46720/50000 (93.35038363171356 Loss:0.3886600136756897)\n",
            "Training set:47360/50000 (94.62915601023018 Loss:0.31645339727401733)\n",
            "Training set:48000/50000 (95.9079283887468 Loss:0.3982347846031189)\n",
            "Training set:48640/50000 (97.18670076726343 Loss:0.25830358266830444)\n",
            "Training set:49280/50000 (98.46547314578005 Loss:0.14835411310195923)\n",
            "Training set:49920/50000 (99.74424552429667 Loss:0.2929612994194031)\n",
            "Training set: Average loss: 0.263920\n",
            "Validation set: Average loss: 0.4887612786167746, Accuracy: 8645/10000 (86.45%)\n",
            "Epoch: 29\n",
            "Training set:0/50000 (0.0 Loss:0.12132450938224792)\n",
            "Training set:640/50000 (1.278772378516624 Loss:0.25846779346466064)\n",
            "Training set:1280/50000 (2.557544757033248 Loss:0.07496315240859985)\n",
            "Training set:1920/50000 (3.836317135549872 Loss:0.2105327695608139)\n",
            "Training set:2560/50000 (5.115089514066496 Loss:0.31447726488113403)\n",
            "Training set:3200/50000 (6.3938618925831205 Loss:0.15697480738162994)\n",
            "Training set:3840/50000 (7.672634271099744 Loss:0.2413204312324524)\n",
            "Training set:4480/50000 (8.951406649616368 Loss:0.30602937936782837)\n",
            "Training set:5120/50000 (10.230179028132993 Loss:0.15689443051815033)\n",
            "Training set:5760/50000 (11.508951406649617 Loss:0.2796283960342407)\n",
            "Training set:6400/50000 (12.787723785166241 Loss:0.27499544620513916)\n",
            "Training set:7040/50000 (14.066496163682864 Loss:0.2581266462802887)\n",
            "Training set:7680/50000 (15.345268542199488 Loss:0.1856863647699356)\n",
            "Training set:8320/50000 (16.624040920716112 Loss:0.17829109728336334)\n",
            "Training set:8960/50000 (17.902813299232736 Loss:0.3134332001209259)\n",
            "Training set:9600/50000 (19.18158567774936 Loss:0.20688489079475403)\n",
            "Training set:10240/50000 (20.460358056265985 Loss:0.23368385434150696)\n",
            "Training set:10880/50000 (21.73913043478261 Loss:0.2752188742160797)\n",
            "Training set:11520/50000 (23.017902813299234 Loss:0.24180640280246735)\n",
            "Training set:12160/50000 (24.296675191815858 Loss:0.1503700613975525)\n",
            "Training set:12800/50000 (25.575447570332482 Loss:0.17377473413944244)\n",
            "Training set:13440/50000 (26.854219948849106 Loss:0.18361414968967438)\n",
            "Training set:14080/50000 (28.132992327365727 Loss:0.2965252995491028)\n",
            "Training set:14720/50000 (29.41176470588235 Loss:0.39014503359794617)\n",
            "Training set:15360/50000 (30.690537084398976 Loss:0.28632694482803345)\n",
            "Training set:16000/50000 (31.9693094629156 Loss:0.2670023441314697)\n",
            "Training set:16640/50000 (33.248081841432224 Loss:0.35257789492607117)\n",
            "Training set:17280/50000 (34.52685421994885 Loss:0.33291247487068176)\n",
            "Training set:17920/50000 (35.80562659846547 Loss:0.13570813834667206)\n",
            "Training set:18560/50000 (37.084398976982094 Loss:0.2411089837551117)\n",
            "Training set:19200/50000 (38.36317135549872 Loss:0.23319107294082642)\n",
            "Training set:19840/50000 (39.64194373401534 Loss:0.23213878273963928)\n",
            "Training set:20480/50000 (40.92071611253197 Loss:0.34811970591545105)\n",
            "Training set:21120/50000 (42.19948849104859 Loss:0.24312900006771088)\n",
            "Training set:21760/50000 (43.47826086956522 Loss:0.22199058532714844)\n",
            "Training set:22400/50000 (44.75703324808184 Loss:0.11181367933750153)\n",
            "Training set:23040/50000 (46.03580562659847 Loss:0.3157199025154114)\n",
            "Training set:23680/50000 (47.31457800511509 Loss:0.24568991363048553)\n",
            "Training set:24320/50000 (48.593350383631716 Loss:0.12942036986351013)\n",
            "Training set:24960/50000 (49.87212276214834 Loss:0.21875104308128357)\n",
            "Training set:25600/50000 (51.150895140664964 Loss:0.2949781119823456)\n",
            "Training set:26240/50000 (52.429667519181585 Loss:0.31773191690444946)\n",
            "Training set:26880/50000 (53.70843989769821 Loss:0.2163633555173874)\n",
            "Training set:27520/50000 (54.987212276214834 Loss:0.21434880793094635)\n",
            "Training set:28160/50000 (56.265984654731454 Loss:0.09756049513816833)\n",
            "Training set:28800/50000 (57.54475703324808 Loss:0.1298513412475586)\n",
            "Training set:29440/50000 (58.8235294117647 Loss:0.3123486042022705)\n",
            "Training set:30080/50000 (60.10230179028133 Loss:0.4830988049507141)\n",
            "Training set:30720/50000 (61.38107416879795 Loss:0.44901934266090393)\n",
            "Training set:31360/50000 (62.65984654731458 Loss:0.4593542218208313)\n",
            "Training set:32000/50000 (63.9386189258312 Loss:0.34894704818725586)\n",
            "Training set:32640/50000 (65.21739130434783 Loss:0.3044683337211609)\n",
            "Training set:33280/50000 (66.49616368286445 Loss:0.1483922153711319)\n",
            "Training set:33920/50000 (67.77493606138107 Loss:0.34437841176986694)\n",
            "Training set:34560/50000 (69.0537084398977 Loss:0.32223641872406006)\n",
            "Training set:35200/50000 (70.33248081841433 Loss:0.09714266657829285)\n",
            "Training set:35840/50000 (71.61125319693095 Loss:0.25644946098327637)\n",
            "Training set:36480/50000 (72.89002557544757 Loss:0.17574593424797058)\n",
            "Training set:37120/50000 (74.16879795396419 Loss:0.2577769458293915)\n",
            "Training set:37760/50000 (75.44757033248082 Loss:0.39781197905540466)\n",
            "Training set:38400/50000 (76.72634271099744 Loss:0.23076234757900238)\n",
            "Training set:39040/50000 (78.00511508951406 Loss:0.3083355128765106)\n",
            "Training set:39680/50000 (79.28388746803068 Loss:0.21296490728855133)\n",
            "Training set:40320/50000 (80.56265984654732 Loss:0.18612028658390045)\n",
            "Training set:40960/50000 (81.84143222506394 Loss:0.1716395765542984)\n",
            "Training set:41600/50000 (83.12020460358056 Loss:0.12827377021312714)\n",
            "Training set:42240/50000 (84.39897698209718 Loss:0.4184570014476776)\n",
            "Training set:42880/50000 (85.67774936061382 Loss:0.27628448605537415)\n",
            "Training set:43520/50000 (86.95652173913044 Loss:0.28763455152511597)\n",
            "Training set:44160/50000 (88.23529411764706 Loss:0.206285759806633)\n",
            "Training set:44800/50000 (89.51406649616368 Loss:0.22156047821044922)\n",
            "Training set:45440/50000 (90.79283887468031 Loss:0.3387718200683594)\n",
            "Training set:46080/50000 (92.07161125319693 Loss:0.3183327615261078)\n",
            "Training set:46720/50000 (93.35038363171356 Loss:0.33547818660736084)\n",
            "Training set:47360/50000 (94.62915601023018 Loss:0.36996540427207947)\n",
            "Training set:48000/50000 (95.9079283887468 Loss:0.36460697650909424)\n",
            "Training set:48640/50000 (97.18670076726343 Loss:0.228396475315094)\n",
            "Training set:49280/50000 (98.46547314578005 Loss:0.4492717385292053)\n",
            "Training set:49920/50000 (99.74424552429667 Loss:0.3304388225078583)\n",
            "Training set: Average loss: 0.248956\n",
            "Validation set: Average loss: 0.41732582243479743, Accuracy: 8717/10000 (87.17%)\n",
            "Epoch: 30\n",
            "Training set:0/50000 (0.0 Loss:0.13878068327903748)\n",
            "Training set:640/50000 (1.278772378516624 Loss:0.29117777943611145)\n",
            "Training set:1280/50000 (2.557544757033248 Loss:0.28797537088394165)\n",
            "Training set:1920/50000 (3.836317135549872 Loss:0.2647370398044586)\n",
            "Training set:2560/50000 (5.115089514066496 Loss:0.09014812111854553)\n",
            "Training set:3200/50000 (6.3938618925831205 Loss:0.22043852508068085)\n",
            "Training set:3840/50000 (7.672634271099744 Loss:0.14808154106140137)\n",
            "Training set:4480/50000 (8.951406649616368 Loss:0.472980260848999)\n",
            "Training set:5120/50000 (10.230179028132993 Loss:0.2708183825016022)\n",
            "Training set:5760/50000 (11.508951406649617 Loss:0.16224715113639832)\n",
            "Training set:6400/50000 (12.787723785166241 Loss:0.441215455532074)\n",
            "Training set:7040/50000 (14.066496163682864 Loss:0.31332987546920776)\n",
            "Training set:7680/50000 (15.345268542199488 Loss:0.21754078567028046)\n",
            "Training set:8320/50000 (16.624040920716112 Loss:0.15173718333244324)\n",
            "Training set:8960/50000 (17.902813299232736 Loss:0.3493817150592804)\n",
            "Training set:9600/50000 (19.18158567774936 Loss:0.15324708819389343)\n",
            "Training set:10240/50000 (20.460358056265985 Loss:0.3346576690673828)\n",
            "Training set:10880/50000 (21.73913043478261 Loss:0.28066959977149963)\n",
            "Training set:11520/50000 (23.017902813299234 Loss:0.3026665449142456)\n",
            "Training set:12160/50000 (24.296675191815858 Loss:0.1529717743396759)\n",
            "Training set:12800/50000 (25.575447570332482 Loss:0.3784141540527344)\n",
            "Training set:13440/50000 (26.854219948849106 Loss:0.20377838611602783)\n",
            "Training set:14080/50000 (28.132992327365727 Loss:0.25166088342666626)\n",
            "Training set:14720/50000 (29.41176470588235 Loss:0.15767399966716766)\n",
            "Training set:15360/50000 (30.690537084398976 Loss:0.13500897586345673)\n",
            "Training set:16000/50000 (31.9693094629156 Loss:0.5315516591072083)\n",
            "Training set:16640/50000 (33.248081841432224 Loss:0.3104541301727295)\n",
            "Training set:17280/50000 (34.52685421994885 Loss:0.13289965689182281)\n",
            "Training set:17920/50000 (35.80562659846547 Loss:0.11265141516923904)\n",
            "Training set:18560/50000 (37.084398976982094 Loss:0.15439309179782867)\n",
            "Training set:19200/50000 (38.36317135549872 Loss:0.1730068325996399)\n",
            "Training set:19840/50000 (39.64194373401534 Loss:0.158004030585289)\n",
            "Training set:20480/50000 (40.92071611253197 Loss:0.28772151470184326)\n",
            "Training set:21120/50000 (42.19948849104859 Loss:0.3720656931400299)\n",
            "Training set:21760/50000 (43.47826086956522 Loss:0.18004271388053894)\n",
            "Training set:22400/50000 (44.75703324808184 Loss:0.2790006995201111)\n",
            "Training set:23040/50000 (46.03580562659847 Loss:0.28769177198410034)\n",
            "Training set:23680/50000 (47.31457800511509 Loss:0.16773374378681183)\n",
            "Training set:24320/50000 (48.593350383631716 Loss:0.11603928357362747)\n",
            "Training set:24960/50000 (49.87212276214834 Loss:0.48085451126098633)\n",
            "Training set:25600/50000 (51.150895140664964 Loss:0.19853173196315765)\n",
            "Training set:26240/50000 (52.429667519181585 Loss:0.2544098198413849)\n",
            "Training set:26880/50000 (53.70843989769821 Loss:0.10560114681720734)\n",
            "Training set:27520/50000 (54.987212276214834 Loss:0.27576160430908203)\n",
            "Training set:28160/50000 (56.265984654731454 Loss:0.16134324669837952)\n",
            "Training set:28800/50000 (57.54475703324808 Loss:0.151886448264122)\n",
            "Training set:29440/50000 (58.8235294117647 Loss:0.20043984055519104)\n",
            "Training set:30080/50000 (60.10230179028133 Loss:0.33639273047447205)\n",
            "Training set:30720/50000 (61.38107416879795 Loss:0.15866944193840027)\n",
            "Training set:31360/50000 (62.65984654731458 Loss:0.4790719449520111)\n",
            "Training set:32000/50000 (63.9386189258312 Loss:0.32612764835357666)\n",
            "Training set:32640/50000 (65.21739130434783 Loss:0.20036882162094116)\n",
            "Training set:33280/50000 (66.49616368286445 Loss:0.18976867198944092)\n",
            "Training set:33920/50000 (67.77493606138107 Loss:0.10598372668027878)\n",
            "Training set:34560/50000 (69.0537084398977 Loss:0.32767871022224426)\n",
            "Training set:35200/50000 (70.33248081841433 Loss:0.2651110887527466)\n",
            "Training set:35840/50000 (71.61125319693095 Loss:0.21801650524139404)\n",
            "Training set:36480/50000 (72.89002557544757 Loss:0.10351601243019104)\n",
            "Training set:37120/50000 (74.16879795396419 Loss:0.314982146024704)\n",
            "Training set:37760/50000 (75.44757033248082 Loss:0.27297329902648926)\n",
            "Training set:38400/50000 (76.72634271099744 Loss:0.31909435987472534)\n",
            "Training set:39040/50000 (78.00511508951406 Loss:0.17059041559696198)\n",
            "Training set:39680/50000 (79.28388746803068 Loss:0.3139185309410095)\n",
            "Training set:40320/50000 (80.56265984654732 Loss:0.16082321107387543)\n",
            "Training set:40960/50000 (81.84143222506394 Loss:0.23064033687114716)\n",
            "Training set:41600/50000 (83.12020460358056 Loss:0.17273637652397156)\n",
            "Training set:42240/50000 (84.39897698209718 Loss:0.06850706040859222)\n",
            "Training set:42880/50000 (85.67774936061382 Loss:0.30952978134155273)\n",
            "Training set:43520/50000 (86.95652173913044 Loss:0.08991135656833649)\n",
            "Training set:44160/50000 (88.23529411764706 Loss:0.3915529251098633)\n",
            "Training set:44800/50000 (89.51406649616368 Loss:0.2771393060684204)\n",
            "Training set:45440/50000 (90.79283887468031 Loss:0.3989737331867218)\n",
            "Training set:46080/50000 (92.07161125319693 Loss:0.14716923236846924)\n",
            "Training set:46720/50000 (93.35038363171356 Loss:0.18659693002700806)\n",
            "Training set:47360/50000 (94.62915601023018 Loss:0.15002602338790894)\n",
            "Training set:48000/50000 (95.9079283887468 Loss:0.2517467737197876)\n",
            "Training set:48640/50000 (97.18670076726343 Loss:0.22516971826553345)\n",
            "Training set:49280/50000 (98.46547314578005 Loss:0.17726252973079681)\n",
            "Training set:49920/50000 (99.74424552429667 Loss:0.14935973286628723)\n",
            "Training set: Average loss: 0.242078\n",
            "Validation set: Average loss: 0.46462184114820637, Accuracy: 8683/10000 (86.83%)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from matplotlib import pyplot as plt\n",
        "\n",
        "plt.plot(epoch_nums, training_loss)\n",
        "plt.plot(epoch_nums, validation_loss)\n",
        "plt.xlabel('epoch')\n",
        "plt.ylabel('loss')\n",
        "plt.legend(['training', 'validation'], loc='upper right')\n",
        "plt.show()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 450
        },
        "id": "Ym4qSVXjmx3O",
        "outputId": "4d848fbe-ad05-4541-c607-d8d2f985c72b"
      },
      "execution_count": 67,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<Figure size 640x480 with 1 Axes>"
            ],
            "image/png": "iVBORw0KGgoAAAANSUhEUgAAAjcAAAGxCAYAAACeKZf2AAAAOXRFWHRTb2Z0d2FyZQBNYXRwbG90bGliIHZlcnNpb24zLjcuMSwgaHR0cHM6Ly9tYXRwbG90bGliLm9yZy/bCgiHAAAACXBIWXMAAA9hAAAPYQGoP6dpAABb2UlEQVR4nO3dd3iT9f7/8WfSNuledEJbKKsMAQEBCyog4OY4jtvzVY7rqHgc6HGd33GeI+6NosdzRD2O4+YoLlBBRUQBkV1WoYwuoHu3uX9/3G2gUgqUJHebvh7XlSt3kjvJOyHHvs5n2gzDMBARERHxE3arCxARERHxJIUbERER8SsKNyIiIuJXFG5ERETEryjciIiIiF9RuBERERG/onAjIiIifkXhRkRERPxKoNUF+JrL5WLnzp1ERERgs9msLkdEREQOgWEYlJWV0bVrV+z21ttmOl242blzJ6mpqVaXISIiIm2wbds2UlJSWj3H0nAzffp0PvjgA9atW0dISAijR4/m4YcfJiMj44DPmTVrFn/84x+b3ed0Oqmurj6k94yIiADMLycyMrLtxYuIiIjPlJaWkpqa6v473hpLw82CBQuYOnUqI0aMoL6+nrvuuouTTjqJNWvWEBYWdsDnRUZGkpWV5b59ON1LTedGRkYq3IiIiHQwh/I339Jw8/nnnze7PWvWLBISEli6dCknnHDCAZ9ns9lISkrydnkiIiLSAbWr2VIlJSUAxMbGtnpeeXk53bt3JzU1lTPPPJPVq1cf8NyamhpKS0ubXURERMR/tZtw43K5uOmmmxgzZgxHHXXUAc/LyMjg3//+N7Nnz+Y///kPLpeL0aNHs3379hbPnz59OlFRUe6LBhOLiIj4N5thGIbVRQBce+21fPbZZ3z//fcHHQW9r7q6Ovr3789FF13EAw88sN/jNTU11NTUuG83DUgqKSnRmBsRkQ7O5XJRW1trdRniIQ6H44DTvEtLS4mKijqkv9/tYir49ddfzyeffMK33357WMEGICgoiKFDh7Jx48YWH3c6nTidTk+UKSIi7UhtbS3Z2dm4XC6rSxEPsdvtpKen43A4juh1LA03hmHw5z//mQ8//JD58+eTnp5+2K/R0NDAypUrOe2007xQoYiItEeGYZCbm0tAQACpqakHXdRN2r+mRXZzc3NJS0s7ooV2LQ03U6dO5c0332T27NlERESQl5cHQFRUFCEhIQBceumldOvWjenTpwNw//33c+yxx9K7d2+Ki4t59NFH2bp1K1deeaVln0NERHyrvr6eyspKunbtSmhoqNXliIfEx8ezc+dO6uvrCQoKavPrWBpuXnjhBQDGjRvX7P5XXnmFKVOmAJCTk9MskRcVFXHVVVeRl5dHTEwMw4cP54cffmDAgAG+KltERCzW0NAAcMTdF9K+NP17NjQ0HFG4aTcDin3lcAYkiYhI+1RdXU12djbp6ekEBwdbXY54SGv/rofz91udlCIiIuJXFG5ERETEryjciIiIdEA9evTgqaeeOuTz58+fj81mo7i42Gs1tRftYp0bv+BqgIpdUFcBsT2trkZERNqhcePGcfTRRx9WKDmQn3/+udVNpn9r9OjR5ObmEhUVdcTv3d6p5cZTsr+Fx/vCWxdbXYmIiHRQhmFQX19/SOfGx8cf1jR4h8NBUlLSEa0f01Eo3HhKeKJ5XZ5vbR0iIp2QYRhU1tZbcjnUScdTpkxhwYIFPP3009hsNmw2G7NmzcJms/HZZ58xfPhwnE4n33//PZs2beLMM88kMTGR8PBwRowYwbx585q93m+7pWw2Gy+//DJnn302oaGh9OnTh//973/ux3/bLTVr1iyio6P54osv6N+/P+Hh4Zxyyink5ua6n1NfX88NN9xAdHQ0Xbp04fbbb+eyyy7jrLPOavO/lS+oW8pTmsJN1R6or4VArb0gIuIrVXUNDLj7C0vee839JxPqOPif06effpr169dz1FFHcf/99wOwevVqAO644w4ee+wxevbsSUxMDNu2beO0007jH//4B06nk9dee43JkyeTlZVFWlraAd/jvvvu45FHHuHRRx/l2Wef5ZJLLmHr1q3Exsa2eH5lZSWPPfYYr7/+Ona7nT/84Q/ceuutvPHGGwA8/PDDvPHGG7zyyiv079+fp59+mo8++ojx48cf7tfkU2q58ZSQGLA3/rgrCq2tRURE2p2oqCgcDgehoaEkJSWRlJREQEAAYK6+P2nSJHr16kVsbCxDhgzhT3/6E0cddRR9+vThgQceoFevXs1aYloyZcoULrroInr37s2DDz5IeXk5P/300wHPr6urY+bMmRxzzDEMGzaM66+/nq+++sr9+LPPPsudd97J2WefTb9+/XjuueeIjo72yPfhTWq58RS73Wy9Kd0B5XkQ1c3qikREOo2QoADW3H+yZe99pI455phmt8vLy7n33nuZM2cOubm51NfXU1VVRU5OTquvM3jwYPdxWFgYkZGRFBQUHPD80NBQevXq5b6dnJzsPr+kpIT8/HxGjhzpfjwgIIDhw4e3+81KFW48KTyhMdwc+IckIiKeZ7PZDqlrqL367aynW2+9lblz5/LYY4/Ru3dvQkJCOPfcc6mtrW31dX67ZYHNZms1iLR0vj9sXKBuKU/SoGIREWmFw+Fw74vVmoULFzJlyhTOPvtsBg0aRFJSElu2bPF+gfuIiooiMTGRn3/+2X1fQ0MDy5Yt82kdbdFxY257FJ5gXqvlRkREWtCjRw8WL17Mli1bCA8PP2CrSp8+ffjggw+YPHkyNpuNv/3tb5Z0Bf35z39m+vTp9O7dm379+vHss89SVFTU7qeTq+XGk9RyIyIirbj11lsJCAhgwIABxMfHH3AMzRNPPEFMTAyjR49m8uTJnHzyyQwbNszH1cLtt9/ORRddxKWXXkpmZibh4eGcfPLJ7X6zUu0K7kk//RM+vRX6T4YL/uPZ1xYRETftCm4Nl8tF//79Of/883nggQc8/vqe2hVc3VKe1NRyU6aWGxER6fi2bt3Kl19+ydixY6mpqeG5554jOzubiy9u36vxq1vKk9QtJSIifsRutzNr1ixGjBjBmDFjWLlyJfPmzaN///5Wl9Yqtdx40r4Dig0D2vmAKxERkdakpqaycOFCq8s4bGq58aSmlpv6Kqgps7YWERGRTkrhxpMcoeBsHOSk6eAiIiKWULjxNHfXVJ61dYiIiHRSCjeepkHFIiIillK48TStUiwiImIphRtPU8uNiIiIpRRuPM0dbtRyIyIintWjRw+eeuop922bzcZHH310wPO3bNmCzWZj+fLlR/S+nnodX9E6N57mXqVYA4pFRMS7cnNziYmJ8ehrTpkyheLi4mahKTU1ldzcXOLi4jz6Xt6icONparkREREfSUpK8sn7BAQE+Oy9PEHdUp7mHlCsMTciIj5jGFBbYc3lEPeffumll+jatSsul6vZ/WeeeSaXX345mzZt4swzzyQxMZHw8HBGjBjBvHnzWn3N33ZL/fTTTwwdOpTg4GCOOeYYfvnll2bnNzQ0cMUVV5Cenk5ISAgZGRk8/fTT7sfvvfdeXn31VWbPno3NZsNmszF//vwWu6UWLFjAyJEjcTqdJCcnc8cdd1BfX+9+fNy4cdxwww3cdtttxMbGkpSUxL333ntI39WRUsuNpzW13FTuAlcD2AOsrUdEpDOoq4QHu1rz3nftBEfYQU8777zz+POf/8w333zDhAkTANizZw+ff/45n376KeXl5Zx22mn84x//wOl08tprrzF58mSysrJIS0s76OuXl5dzxhlnMGnSJP7zn/+QnZ3NjTfe2Owcl8tFSkoK7777Ll26dOGHH37g6quvJjk5mfPPP59bb72VtWvXUlpayiuvvAJAbGwsO3fubPY6O3bs4LTTTmPKlCm89tprrFu3jquuuorg4OBmAebVV19l2rRpLF68mEWLFjFlyhTGjBnDpEmTDvp5joTCjaeFxYHNDoYLKnZBRKLVFYmISDsQExPDqaeeyptvvukON++99x5xcXGMHz8eu93OkCFD3Oc/8MADfPjhh/zvf//j+uuvP+jrv/nmm7hcLv71r38RHBzMwIED2b59O9dee637nKCgIO677z737fT0dBYtWsQ777zD+eefT3h4OCEhIdTU1LTaDfX888+TmprKc889h81mo1+/fuzcuZPbb7+du+++G7vd7BgaPHgw99xzDwB9+vThueee46uvvlK46XDsARAWb3ZLlecp3IiI+EJQqNmCYtV7H6JLLrmEq666iueffx6n08kbb7zBhRdeiN1up7y8nHvvvZc5c+aQm5tLfX09VVVV5OTkHNJrr127lsGDBxMcHOy+LzMzc7/zZsyYwb///W9ycnKoqqqitraWo48++pA/Q9N7ZWZmYttng+gxY8ZQXl7O9u3b3S1NgwcPbva85ORkCgq8PyZV4cYbwhMaw40GFYuI+ITNdkhdQ1abPHkyhmEwZ84cRowYwXfffceTTz4JwK233srcuXN57LHH6N27NyEhIZx77rnU1tZ67P3ffvttbr31Vh5//HEyMzOJiIjg0UcfZfHixR57j30FBQU1u22z2fYbc+QNCjfeEJ4IrNSgYhERaSY4OJhzzjmHN954g40bN5KRkcGwYcMAWLhwIVOmTOHss88GzDE0W7ZsOeTX7t+/P6+//jrV1dXu1psff/yx2TkLFy5k9OjRXHfdde77Nm3a1Owch8NBQ0PDQd/r/fffxzAMd+vNwoULiYiIICUl5ZBr9hbNlvIGrVIsIiIHcMkllzBnzhz+/e9/c8kll7jv79OnDx988AHLly/n119/5eKLLz6sVo6LL74Ym83GVVddxZo1a/j000957LHHmp3Tp08flixZwhdffMH69ev529/+xs8//9zsnB49erBixQqysrLYtWsXdXV1+73Xddddx7Zt2/jzn//MunXrmD17Nvfccw/Tpk1zj7exkvUV+CPtLyUiIgdw4oknEhsbS1ZWFhdffLH7/ieeeIKYmBhGjx7N5MmTOfnkk92tOociPDycjz/+mJUrVzJ06FD++te/8vDDDzc7509/+hPnnHMOF1xwAaNGjWL37t3NWnEArrrqKjIyMjjmmGOIj49n4cKF+71Xt27d+PTTT/npp58YMmQI11xzDVdccQX/7//9v8P8NrzDZhiHOEHfT5SWlhIVFUVJSQmRkZHeeZMfZ8Lnt8PAs+G8Wd55DxGRTqy6uprs7GzS09ObDaCVjq21f9fD+futlhtvaGq5KVO3lIiIiK8p3HiDxtyIiIhYRuHGG7S/lIiIiGUUbryhqVuqtszcd0RERLyikw0b9Xue+vdUuPEGZ8TeFSvVeiMi4nEBAea+fZ5c4E6s1/Tv2fTv21ZaxM8bbDaz9aZoiznuJjbd6opERPxKYGAgoaGhFBYWEhQU1C7WVpEj43K5KCwsJDQ0lMDAI4snCjfeEp64N9yIiIhH2Ww2kpOTyc7OZuvWrVaXIx5it9tJS0trtmdVWyjceIsW8hMR8SqHw0GfPn3UNeVHHA6HR1rhFG68RdPBRUS8zm63axE/2Y86Kb0lPMm8VrgRERHxKYUbb9EqxSIiIpZQuPEWdUuJiIhYQuHGWzSgWERExBIKN97S1HJTUQAul7W1iIiIdCIKN97S1HLjqoeqImtrERER6UQUbrwlIAhCu5jH5XnW1iIiItKJKNx4kwYVi4iI+JzCjTdpULGIiIjPKdx4k1puREREfE7hxpvUciMiIuJzCjfepC0YREREfE7hxpuauqXKNFtKRETEVxRuvEndUiIiIj6ncONNGlAsIiLicwo33tTUclNdDPU1lpYiIiLSWSjceFNIDAQ4zGN1TYmIiPiEwo032WzqmhIREfExhRtvcw8qVrgRERHxBYUbb1PLjYiIiE8p3HibpoOLiIj4lMKNt2mVYhEREZ+yNNxMnz6dESNGEBERQUJCAmeddRZZWVkHfd67775Lv379CA4OZtCgQXz66ac+qLaNmlpuyhRuREREfMHScLNgwQKmTp3Kjz/+yNy5c6mrq+Okk06ioqLigM/54YcfuOiii7jiiiv45ZdfOOusszjrrLNYtWqVDys/DBpzIyIi4lM2wzAMq4toUlhYSEJCAgsWLOCEE05o8ZwLLriAiooKPvnkE/d9xx57LEcffTQzZ8486HuUlpYSFRVFSUkJkZGRHqv9gLb9DP+aCFFpcPNK77+fiIiIHzqcv9/tasxNSUkJALGxsQc8Z9GiRUycOLHZfSeffDKLFi1q8fyamhpKS0ubXXxq36ng7SdHioiI+K12E25cLhc33XQTY8aM4aijjjrgeXl5eSQmJja7LzExkby8lnfenj59OlFRUe5LamqqR+s+qKZuqYYaqC7x7XuLiIh0Qu0m3EydOpVVq1bx9ttve/R177zzTkpKStyXbdu2efT1DyooGIKjzGNNBxcREfG6QKsLALj++uv55JNP+Pbbb0lJSWn13KSkJPLzmw/Ozc/PJykpqcXznU4nTqfTY7W2SXii2WpTngfxfa2tRURExM9Z2nJjGAbXX389H374IV9//TXp6ekHfU5mZiZfffVVs/vmzp1LZmamt8o8cu4ZU2q5ERER8TZLW26mTp3Km2++yezZs4mIiHCPm4mKiiIkJASASy+9lG7dujF9+nQAbrzxRsaOHcvjjz/O6aefzttvv82SJUt46aWXLPscB6X9pURERHzG0pabF154gZKSEsaNG0dycrL78t///td9Tk5ODrm5ue7bo0eP5s033+Sll15iyJAhvPfee3z00UetDkK2nFYpFhER8RlLW24OZYmd+fPn73ffeeedx3nnneeFirxE+0uJiIj4TLuZLeXXmsbclLU8XV1EREQ8R+HGF9RyIyIi4jMKN76g/aVERER8RuHGgypr69m6u4VNP5vCTeVuaKjzbVEiIiKdjMKNh3y1Np+j75/LLe/8uv+DoV3AFgAYULHL57WJiIh0Jgo3HtIvOZLaehfLcooorqxt/qDdvs+4Gw0qFhER8SaFGw/pFh1CRmIELgO+3dBC64wGFYuIiPiEwo0HjesXD8D8rBYCjAYVi4iI+ITCjQeN62u2zizIKsTl+s0ChdqCQURExCcUbjzomB4xhDsD2V1Ry8odJc0fdG/BoG4pERERb1K48aCgADvH94kD4Jvfdk1plWIRERGfULjxsPEZZvfTN1mFzR/QgGIRERGfULjxsLEZ5qDiFduL2V1es/cBDSgWERHxCYUbD0uMDGZAciSGAd9u2Kf1Ri03IiIiPqFw4wXjG6eEf7Nu33DT2HJTVwE15RZUJSIi0jko3HhB07ibBesLaWiaEu4MB0e4eayuKREREa9RuPGCo1OjiQoJoqSqjuXbivY+oLVuREREvE7hxgsCA+yc0LdpteIWuqYUbkRERLxG4cZLxjWGm2br3WhQsYiIiNcp3HhJ05TwVTtKKSitNu90r1KslhsRERFvUbjxkrhwJ0NSogCYv76xa0pjbkRERLxO4caLxjXOmnLvEu7egkHhRkRExFsUbrxofD8z3Hy3YRd1DS4NKBYREfEBhRsvGtwtitgwB2XV9SzbWqQBxSIiIj6gcONFdruNse5ZU4UQ0TiguKIQXA0WViYiIuK/FG68bFxG03o3BRAaB9jAaIDKPdYWJiIi4qcUbrzshD7x2G2wLq+MnWV1EBZnPlCeZ21hIiIifkrhxstiwhwMTYsBzL2mNKhYRETEuxRufMC9WvG6Ag0qFhER8TKFGx9omhK+cOMuGkK1kJ+IiIg3Kdz4wIDkSOIjnFTUNpDrMlctVsuNiIiIdyjc+IDdbnN3Ta0pDTbvVMuNiIiIVyjc+EhT19RPu4LMO7QFg4iIiFco3PjImN5xBNhtrCpRy42IiIg3Kdz4SFRIEMO7x1BoaMyNiIiINync+ND4jAQKDHPNG2pKoK7K2oJERET8kMKND43vF08ZIVQbjeNu1HojIiLicQo3PpSRGEFyVAiFRrR5h8bdiIiIeJzCjQ/ZbDbGZcRTSNO4G4UbERERT1O48bFxGQlquREREfEihRsfG9M7jl1EA1BUsN3aYkRERPyQwo2PhTsDccYkA5C/M8fiakRERPyPwo0FErt2B6By9w6LKxEREfE/CjcW6JXeE4DAqkIqa+strkZERMS/KNxYILmb2XLThWIWbdptcTUiIiL+ReHGAraIJADiKeabdZoxJSIi4kkKN1YIiwfAYWtgybpsDMOwuCARERH/oXBjhUAnRoi5x1RDaR4bC8otLkhERMR/KNxYxBaeCEC8rZj5WYUWVyMiIuI/FG6sEp4ANI67ydIGmiIiIp6icGOV8MZBxbYSft6yh7LqOosLEhER8Q8KN1ZpbLnpHVpBXYPBwo2aEi4iIuIJCjdWaRxz0z+iCoD56poSERHxCIUbqzSGm7SgMgDmZxVqSriIiIgHKNxYpbFbKsq1h+AgO3ml1azLK7O4KBERkY5P4cYqjS039vICxvSKA9CsKREREQ9QuLFK4xYMVO3hxL7mgn7z12m9GxERkSOlcGOV4GiwBwEwvpt519KcIkqqNCVcRETkSCjcWMVud4+76RpYQu+EcBpcBt9tUOuNiIjIkVC4sVJjuKG8gBP7mcdfrdW4GxERkSOhcGOlxkHFlOczaYB5/PW6AuobXBYWJSIi0rEp3FjJHW4KGJYWQ0xoECVVdSzZWmRtXSIiIh2Ywo2V9mm5CbDbOLGfeXvumnwLixIREenYFG6s1DTmpiwPgEkDzNvz1uZrtWIREZE2Urix0j7dUgDH94nHEWhn6+5KNhaUW1iYiIhIx6VwY6V9uqUAwpyBjOnVBYC5a9U1JSIi0haWhptvv/2WyZMn07VrV2w2Gx999FGr58+fPx+bzbbfJS8vzzcFe1rEPi03jd1QExtnTc3TuBsREZE2sTTcVFRUMGTIEGbMmHFYz8vKyiI3N9d9SUhI8FKFXhbWWHd9FdSYm2ZOaBxU/Mu2YgrLaqyqTEREpMMKtPLNTz31VE499dTDfl5CQgLR0dGeL8jXHKHgjISaUrP1JjiSpKhgBqdEsWJ7CV+vy+eCEWlWVykiItKhdMgxN0cffTTJyclMmjSJhQsXtnpuTU0NpaWlzS7tinuV4r1daxP7N00J12rFIiIih6tDhZvk5GRmzpzJ+++/z/vvv09qairjxo1j2bJlB3zO9OnTiYqKcl9SU1N9WPEh+M2gYtgbbr7fWEhVbYMVVYmIiHRYlnZLHa6MjAwyMjLct0ePHs2mTZt48sknef3111t8zp133sm0adPct0tLS9tXwPnNdHCA/skRdIsOYUdxFQs37nIPMhYREZGD61AtNy0ZOXIkGzduPODjTqeTyMjIZpd2pYWWG5vN5t5rap6mhIuIiByWDh9uli9fTnJystVltN0+O4Pvq6lrat7aAlwurVYsIiJyqCztliovL2/W6pKdnc3y5cuJjY0lLS2NO++8kx07dvDaa68B8NRTT5Gens7AgQOprq7m5Zdf5uuvv+bLL7+06iMcuaaWm7Lma/WMTI8lwhnIrvIalm8vZlhajAXFiYiIdDyWttwsWbKEoUOHMnToUACmTZvG0KFDufvuuwHIzc0lJyfHfX5tbS233HILgwYNYuzYsfz666/MmzePCRMmWFK/R7Qw5gbAEWhnbEY8cAgL+rka4LsnYOV73qhQRESkQ7EZnWyHxtLSUqKioigpKWkf42/yVsLM48wF/f6yodlDs5fv4Ma3l9M3MZwvbx7b8vNdLph9Hfz6FgQ44a4dEBDkg8JFRER853D+fnf4MTcdXlPLTeUuswVmH+P6JhBot7E+v5ytuyv2f65hwJybzWAD0FADuzbsf56IiEgnonBjtdAuYLOD4YKKXc0eigoNYmR6LABzf9s1ZRjw+R2wdBZgM18HoGCN92sWERFpxxRurGYPgDBzbM2+qxQ32Ttrap9wYxgw7x5YPNO8feYM6P878zh/tTerFRERafcUbtqDA0wHB9zr3fy8pYjiylrzzvkPwcKnzePTn4Chl0DiQPO2Wm5ERKSTU7hpD1pYyK9Jamwo/ZIiaHAZzM8qNGdFLXjIfPDk6TDiCvM4YYB5na9wIyIinZvCTXsQnmRetxBuYG/XVP0PM+Cr+8w7J9wDmdftPSmxMdyU5EB1O9scVERExIcUbtqDVrqlACYOSOSSgHmcWzjDvGPsHXD8tOYnhcRARFfzuGCtlwoVERFp/xRu2oMDrFLcZHDhx/wj6N8AbOt/NYy7o+XXaRp3k7/K0xWKiIh0GAo37UFrLTcr38P+vz8D8Er9yfzTeSnYbC2/TlPXlAYVi4hIJ6Zw0x4caEDxmv/BB1cDBjt6XcB99Zcyb20BB1xUOqGp5UbhRkREOq82hZtXX32VOXPmuG/fdtttREdHM3r0aLZu3eqx4jqNiKYBxfu03Kz/At67HIwGGHIxXS6YQUhQIDtLqlmTe4ABw+6Wm9XmWjgiIiKdUJvCzYMPPkhISAgAixYtYsaMGTzyyCPExcVx8803e7TATqGpW6q2DGorYNPX8N//A1cdDDwHznyOYEcQx/eJA1pYrbhJXF+wBUB1CZTu9FHxIiIi7Uubws22bdvo3bs3AB999BG///3vufrqq5k+fTrfffedRwvsFBzhEBRqHq/+EN662Nwnqt8ZcM5L5irGmLOm4DerFe8r0AlxfcxjjbsREZFOqk3hJjw8nN27dwPw5ZdfMmnSJACCg4OpqqryXHWdhc22t/Vm9vVQXwW9J8G5/262w/eJ/RKw2WDVjlJySw7wPbsX89M2DCIi0jm1KdxMmjSJK6+8kiuvvJL169dz2mmnAbB69Wp69Ojhyfo6j6ZBxRiQPhYueN1sidlHXLiTYWkxAMxb2/KaOJoxJSIinV2bws2MGTPIzMyksLCQ999/ny5dzB2ply5dykUXXeTRAjuNmB7mddpouOgtCApp8bSmvabmHWjcjWZMiYhIJ2czDjiv2D+VlpYSFRVFSUkJkZGRVpezV/E2WP85DLkQnBEHPG1jQTkTn1iAI8DOsrsnEe4MbH5C0RZ4eggEOOCunc26tURERDqqw/n73aaWm88//5zvv//efXvGjBkcffTRXHzxxRQVFbXlJSU6FUZe1WqwAegVH0Z6XBi1DS6+XV+4/wlRaeYA5YZa2L3RS8WKiIi0X20KN3/5y18oLTXXWlm5ciW33HILp512GtnZ2UybNu0gz5YjYbPZmNjfHHzcYteU3Q4J/c1jDSoWEZFOqE3hJjs7mwEDzIGr77//PmeccQYPPvggM2bM4LPPPvNogbK/pl3Cv84qoL7Btf8JTXtMaVCxiIh0Qm0KNw6Hg8rKSgDmzZvHSSedBEBsbKy7RUe8Z3j3GGJCgyiurGPp1ha6ATWoWEREOrE2hZvjjjuOadOm8cADD/DTTz9x+umnA7B+/XpSUlI8WqDsLzDAzvh+jV1TLS3ot+82DCIiIp1Mm8LNc889R2BgIO+99x4vvPAC3bp1A+Czzz7jlFNO8WiB0rJJjV1Tc9fk77+RZtNCfsU5UFPm48pERESsFXjwU/aXlpbGJ598st/9Tz755BEXJIfm+L7xOALsbNldyabCcnon7DPLKjQWIpKhLBcK1kLqSOsKFRER8bE2hRuAhoYGPvroI9auXQvAwIED+d3vfkdAQIDHipMDC3cGktmrCwvWFzJ3TUHzcANm601ZrjljSuFGREQ6kTZ1S23cuJH+/ftz6aWX8sEHH/DBBx/whz/8gYEDB7Jp0yZP1ygHMKm1jTS1DYOIiHRSbQo3N9xwA7169WLbtm0sW7aMZcuWkZOTQ3p6OjfccIOna5QDmNC43s2ynCJ2ldc0f1AzpkREpJNqU7hZsGABjzzyCLGxse77unTpwkMPPcSCBQs8Vpy0LjkqhEHdojAM+Pq3G2k2tdzkr4LOtcOGiIh0cm0KN06nk7Ky/WfhlJeX43A4jrgoOXRNC/rN/W3XVFwG2AKgutgceyMiItJJtCncnHHGGVx99dUsXrwYwzAwDIMff/yRa665ht/97neerlFaMXGA2TX13YZCqusa9j4QFAxdepnH6poSEZFOpE3h5plnnqFXr15kZmYSHBxMcHAwo0ePpnfv3jz11FMeLlFaMyA5km7RIVTXuVi4cVfzB93bMGgxPxER6TzaNBU8Ojqa2bNns3HjRvdU8P79+9O7d2+PFicH17SR5quLtjJvbT4TGrupAHNQ8eoP1XIjIiKdyiGHm4Pt9v3NN9+4j5944om2VySHbeKARF5dtJW5a/K593cNOAMb1xrSNgwiItIJHXK4+eWXXw7pPJvN1uZipG1GpXchKTKYvNJq3l+6g4tHpZkPNG3DULgeGuohoM1rNoqIiHQYh/zXbt+WGWlfHIF2rjqhJw98soaZCzZx/jEpBAbYIbo7BIVBXQXs2QTxGVaXKiIi4nVtGlAs7c9FI1OJDXOQs6eSj1fsNO+02yGhv3mcr64pERHpHBRu/ESoI5ArjksH4PlvNuFyNS7cp20YRESkk1G48SP/l9mdiOBANhSU8+WaPPNO9zYMarkREZHOQeHGj0QGB3FZZg8AnvtmI4Zh7LMNg8KNiIh0Dgo3fuby49IJCQpg1Y5SFqwv3NtyU7wVavbfMkNERMTfKNz4mdgwBxeNNKeCP//NJgjrAuGNC/sVrLOwMhEREd9QuPFDV5/QE0eAnZ+27OGn7D3ahkFERDoVhRs/lBQVzO+HpwDm2Bv3Yn7ahkFERDoBhRs/de3YXgTYbXy7vpBtQeYUcU0HFxGRzkDhxk+ldQnld0O6AvDq5lDzzvzVYBgWViUiIuJ9Cjd+7LpxvQB4fWMIhs0OVXugPN/iqkRERLxL4caP9UmM4JSBSdTgoCCom3mn1rsRERE/p3Dj56aO7w3A0qpk8w6NuxERET+ncOPnBqVEMbZvPOtc5to3arkRERF/p3DTCVx/Ym+yjFQA6nJXWVyNiIiIdyncdAIjesTi6DYIAFthFjTUW1yRiIiI9yjcdBLnTTyOSsNJoFFL0XZtwyAiIv5L4aaTOL5vAtuDugPw7cIFFlcjIiLiPQo3nYTNZiM0dTAAuVlLKamqs7giERER71C46US69h0OQLprK68v2mJtMSIiIl6icNOJ2Bt3B8+wbeNf32dTWauBxSIi4n8UbjqTxnCTZi+gurKMNxfnWFyQiIiI5yncdCZhcRCWgB2Dvrbt/PO7zdTUN1hdlYiIiEcp3HQ2iQMAGBWaS35pDe8t3W5xQSIiIp6lcNPZJJhdU2d2LQZg5oJN1De4LCxIRETEsxRuOpvGlpt+9u10CXOwbU8V//t1p8VFiYiIeI7CTWeTYIabgMI1XD6mBwDPz9+Ey2VYWJSIiIjnKNx0NvH9ABtU7ubSwSFEBAeysaCcL9fkWV2ZiIiIRyjcdDaOUIjtCUBEyXqmjO4BwHPfbMQw1HojIiIdn8JNZ9S43g35a/jjmHRCggJYtaOUeWsLrK1LRETEAxRuOqOmcFOwhtgwB5dmmhtq3vber2zbU2lhYSIiIkfO0nDz7bffMnnyZLp27YrNZuOjjz466HPmz5/PsGHDcDqd9O7dm1mzZnm9Tr/TOKiY/NUA3DypL0d1i6Soso6rX1+qbRlERKRDszTcVFRUMGTIEGbMmHFI52dnZ3P66aczfvx4li9fzk033cSVV17JF1984eVK/UxTy03hOnA1EBwUwEv/dwxx4Q7W5pbyl3dXaPyNiIh0WIFWvvmpp57Kqaeeesjnz5w5k/T0dB5//HEA+vfvz/fff8+TTz7JySef7K0y/U9MDwgMgfoq2LMZ4vrQNTqEF/4wnIv/+SNzVuYyYH4kU8f3trpSERGRw9ahxtwsWrSIiRMnNrvv5JNPZtGiRQd8Tk1NDaWlpc0unZ49ABL6mceNXVMAI3rEct/vjgLgsS+zmLcm34rqREREjkiHCjd5eXkkJiY2uy8xMZHS0lKqqqpafM706dOJiopyX1JTU31RavuXsHdQ8b4uHpXGH45NwzDgpv8uZ2NBmQXFiYiItF2HCjdtceedd1JSUuK+bNu2zeqS2ofE5oOK93X3GQMZmR5LeU09V722lJKqOh8XJyIi0nYdKtwkJSWRn9+8qyQ/P5/IyEhCQkJafI7T6SQyMrLZRdg7Y+o3LTcAjkA7z18yjG7RIWTvquCGt36hQdsziIhIB9Ghwk1mZiZfffVVs/vmzp1LZmamRRV1YE0zpvZkQ23Ffg/HhTt58f+GExxkZ8H6Qh75Yp2PCxQREWkbS8NNeXk5y5cvZ/ny5YA51Xv58uXk5OQAZpfSpZde6j7/mmuuYfPmzdx2222sW7eO559/nnfeeYebb77ZivI7tvAECI0DDHNKeAuO6hbFI+cOAeDFBZuZvXyHDwsUERFpG0vDzZIlSxg6dChDhw4FYNq0aQwdOpS7774bgNzcXHfQAUhPT2fOnDnMnTuXIUOG8Pjjj/Pyyy9rGnhbucfd7N811eR3Q7py7bheANz23gpWbi/xRWUiIiJtZjM62WptpaWlREVFUVJSovE3n98JPz4Px14Hp0w/4GkNLoMrX/2Zb7IKSY4K5n/XH0d8hNOHhYqISGd3OH+/O9SYG/GwhAPPmNpXgN3G0xcNpWdcGLkl1Vz3xlJq610+KFBEROTwKdx0Zq1MB/+tyOAgXrr0GCKcgfy8pYh7Pz74c0RERKygcNOZxfcHbFC5C8oLDnp674Rwnr7oaGw2eHNxDv/5cav3axQRETlMCjedmSMUYtPN40NovQE4sV8ifzk5A4B7/7eaJeu3w4p3YMNcb1UpIiJyWCzdOFPagYQB5uaZBWug1/hDesq1Y3tRtvlnUja/S783fwAat764/EtIG+W9WkVERA6BWm46u6bF/FqZDu5WXQI/v4ztxRO4PecaLgn8inCqqKZx5tScadBQ771aRUREDoFabjo79zYMB+iWMgzYthiWvgqrP4T6xlaaAAeVvU7npo1DWFKZxHehtxGWvwoWz4TR1/umdhERkRYo3HR2TS03BevA1QD2APN2xW5Y8TYse635Csbx/WDYZTDkQkJDY7l8826+fnkx99dcwMNB/8T45kFsA8+CqBSffxQRERFQuJHYnhAYbLbI7MmG0u1mK826T6Ch1jwnKBQGngPDL4OUEWCzuZ9+bM8uPHPRUG5628X5rvkMr9tAw6e3E3DRGxZ9IBER6ewUbjo7ewDEZ0Dur/Dyiea4mibJQ8xWmkHnQnDUAV/itEHJhDlHcd/rV/KBcSeBWZ9QufpTQgee5oMPICIi0pwGFAskDjKvq0vAEQHHXA5XL4A/fQsjrmg12DQZ2zeev11xPv/BDDSl79/E7qIib1YtIiLSIrXcCBw/Dex2SB0FA88GR1ibXmZEj1jC/vgoebMWkeTK5z8v/IUJU58lOSrEwwWLiIgcmFpuBLr0gt89C0P/0OZg02RAj65w6sMAnF/zAbfMeIfsXRWeqFJEROSQKNyIxyWNPJfq9Ik4bA38uWom573wA2tzS60uS0REOgmFG/E8m43g3z2OERhCZsAajqv6mgteXMTSrRqDIyIi3qdwI94R0wPbCbcCcG/wm1BdzB9eXsz3G3ZZXJiIiPg7hRvxntE3QFwG0a5inuzyP6rqGrh81s98virP6spERMSPKdyI9wQ64PTHATixYg7X9C6itsHFdW8s5b2l2y0uTkRE/JXCjXhX+vEw+EJsGNxe/yIXDEvCZcCt7/7KKwuzra5ORET8kMKNeN9Jf4fgKGx5K5ie+hNXHJcOwH0fr+GZrzZgGIY1de3aCK+cDiveseb9RUTEKxRuxPvC42HivQDYv/kH/++EaKZN6gvAE3PX8/c5a30fcOqq4d3LYOv38L8bzH21RETELyjciG8MmwLdjoHaMmxf3MUNE/pwz+QBAPzr+2ymvPIz2/ZU+q6euXdD/irzuL4KPrkZrGpBEhERj1K4Ed+w2+GMJ8Bmh9Ufwsav+OOYdB47bwiOADsL1hcy6ckFvDB/E3UNLu/WkvUZ/PSieXzqI+au6Ju/gRX/9e77ioiITyjciO8kD4FR15jHc26BuirOHZ7CZzcdz7E9Y6muc/Hw5+uY/Oz33lvwrzQXPrrOPD72Ohj1Jxh7m3n78zuhQuvwiIh0dAo34lvj74KIZCjKhu+fBKBXfDhvXXUsj583hJjQINbllfH7F37grg9XUlJZ57n3djXAh1dD1R5IGuQeB8ToGyDxKPP+z+/03PuJiIglFG7Et5wRcMp08/j7J80ZS4DNZuP3w1P46pZxnDc8BYA3F+cw4YkFzF6+wzMDjhc+DdnfQlAonPsKBDrN+wOCYPIzZpfZyndg47wjfy8REbGMwo343oCzoPdEaKiFT29pNpA3NszBo+cN4e2rj6VXfBi7ymu48e3lXPrvn9i6+wh2F9++BL75h3l86iMQ16f54ynD93aZfXIz1GoncxGRjkrhRnzPZoPTHm0cyDsfVr2/3ynH9uzCpzcez7RJfXEE2vluwy5OevJbZnyzkdr6wxxwXF0K710OrnoYeDYM/UPL543/K0SlQXEOfPPg4X8uERFpFxRuxBqxPeH4W8zjL+6C6pL9TnEGBnDDhD58fuPxjO7VhZp6F49+kcXpz3zHz1v2HNr7GAbMmQbFW83gcsZTZrhqiTPcnNEF8OPzsGPZ4X8uERGxnMKNWGfMjdClN5Tnw9uXQFl+i6f1jA/njStH8eQFQ4gNc7ChoJzzZi7izg9WUFxZ2/p7/Po2rHwXbAHw+5chJLr18/tMgqPOBcMFH98ADR4c0CwiIj6hcCPWCXTC756DoDDY8h28eDxs+b7FU202G2cPTeHrW8Zy4YhUAN76aRsTHl/Ah79sb3nA8e5N8Omt5vG4OyFt1KHVdcpDEBIDeSth0Yy2fDIREbGQwo1Yq3smXD0f4vubLTivTobvHgdXy+NqokMdPPT7wbzzp0x6J4Szu6KWm//7K+fOXMSv24r3nlhfa46zqS2H7sfB8dMOvabweDipcfDx/OlmSBIRkQ5D4UasF98XrvoKhlxkdgd9dT+8dQFUHnhczcj0WD694Xj+cnIGIUEBLN1axJkzFjLtneXkl1bD1w9A7nKzBeacl8AecHg1HX0xpI+F+mptzSAi0sHYDMu2ZLZGaWkpUVFRlJSUEBkZaXU5si/DgF/+Y3Yl1VdDZAqcNwtSR7T6tLySah75Yh0fLNsBwETHKl62N852uuAN6H9G2+rZvQleGG3WcubzMPSStr2OiIgcscP5+62WG2k/bDYY9n9w5TyI7QWl2+GVU2DR8622nCRFBfPE+Ufz0dQxjE+B6TZznMwHAacwp2542xcA7NILxt1hHn/5VygvbNvriIiITyncSPuTNMgchzPgLHNtmi/uhHf+r8Xp4vs6ulsk/45+hXhbCZtsqdxZcSFT31zGBS/+yKodrT/3gDKvN+upKoLP72jba4iIiE8p3Ej7FBxpdkmd+ijYg2Dtx/DiWMj99cDPWTwT28a5EBhM1yve4poJAwkOsvPTlj1Mfu57bnvvVwrKqg+vjoAg+N2z5tYMq96DDXOP6GOJiIj3KdxI+2Wzwair4fIvzAX4irLh5Umw5JX9u6lyf4W5d5vHJ/2dkJRB3DypL1/fMo4zj+6KYcA7S7Zz4mMLeGH+JmrqGw69jq5DzR3EwRxcXFPumc8nIiJeoXAj7V/KcPjTAuh7CjTUwCc3wYd/2hsyasobt1eog4zTYcSV7qd2jQ7h6QuH8v61oxmSEkV5TT0Pf76OSU98y+er8g59PM74uyA6DUq27d2jSkRE2iXNlpKOw+WCH54xp4obDRCXAee/BoueNWdZRXSFaxdCaOwBnm7w4S87ePjzdRSU1QCQ2bMLd57Wj8Ep0Qd//43z4D+/N7uorphnhi4REfGJw/n7rXAjHc+WhWZLTXkeBDjN1hxscNnHkH78QZ9eUVPPzAWbeOnbzdQ0bsJ5fJ84po7vzaj0WGwH2nsK4P2rYOU7kDDQbE0KCPLQhxIRkdZoKrj4tx5j4JrvzUX2GswWGI6/5ZCCDUCYM5BbTsrgq1vGcs6wbgTYbXy3YRcXvvQj585cxNfr8g/cXXXKdAiJhYLVZiuSiIi0O2q5kY7L1QA/vQQVu8z1aNrYirJtTyUvfruJd5Zsp7axJad/ciRTx/fi1KOSCbD/piVn+Vvw0TVmq9F1i8z1cERExKvULdUKhRs5kILSav71fTb/+XErFbXmbKr0uDCuHduLs4Z2wxHY2NBpGPD62bD5G+hxPFz6P7CrEVRExJsUblqhcCMHU1xZy6wftvDKwi2UVNUBkBwVzNUn9OTCEWmEOAJgTzY8nwn1VdBrApzzTwjrYnHlIiL+S+GmFQo3cqjKa+p5a3EOL323mcLG2VVdwhxcflw6/5fZnchNn8CH15oBJ7IbnPfqQffBEhGRtlG4aYXCjRyu6roG3lu6nZkLNrG9qAqACGcgl47uzlV9q4j++ArYs8lcSfmkv8OoP5kLEIqIiMco3LRC4Ubaqr7BxccrdvL8N5vYUGAuIOgIsDOpVwh31s0gJfdL88QBZ5lbNgTr9yUi4ikKN61QuJEj5XIZfLkmn+fnb2TF9qYNOQ2uCPqSOwP+QyANNMT0IuDC1yFxoKW1ioj4C4WbVijciCdtyC9jzspcPluZR1Z+GcNs63nO8QxdbXuosTlZNuhu+p50FV3CnVaXKiLSoSnctELhRrxlY0E5n6/K5btfs5i65yFOCFgJwFsN4/k8dRoTB3fnlIFJxEdYGHRcLti9AXYsg0AHxPeHLr3NYxGRdkzhphUKN+IL2QWlFH76d47Z8hJ2DFa7unNt3U1sI5GRPWI5bVAypxyVRGJksHcLqdxjBpntP8P2n2D7UqgpaX6OPRBie0F8BiT0h/h+5kWhR0TaEYWbVijciE9t+pqGd68goHoP5bYwptVczZcuc7q4zQaj0mM5/5hUTj0q2Vw/50g01EPhWtj2E2xfYgaa3Rv2Py8wBLoOBVc9FK6DmtKWX0+hR0TaEYWbVijciM+V7ID3/gjbFgOwIu1SHqg6l5+3lbtPCXcGcsbgZM47JpVhadGtb97ZpLygsUXmZzPM7FgGdRX7nxfbC1JGmGvwpIyAhAF7t6owDCjdaYaignVm2ClcZx7XlrX8vvZAM+QcfwsMPFvT3kXEJxRuWqFwI5ZoqIN598Ki58zbaZnknvQC72bV897S7eTsqQTAjouhXeq5oJ+TSWkGMa4iKM+HsnzzurzA3A29vABqy/d/H2ckdBtuhpiUEZByDITGHn69hgGlO/YGHXf4yWoeevqcDKc/BtFph/8eVjAMMFxgP8JWMhHxOYWbVijciKXW/A9mTzW7gsLioc9JGOUFVO7eQUNZHmF1RQTYDvV/kjazuyjlmL1hJi7Du/tcNYWeZa/Bd0+Aqw6CQmH8X2HUNRAQ6L33PlIF6+Ddy6CmHM55EXocZ3VFInIYFG5aoXAjltu9Cd65FPJXtfiwCzsl9ih21kdSaERTYERTFhhL19TuHJWRQWpqDwhPhIgkcIT5tvZ9FWbBxzdBzg/m7eQhMPlpczxPe7P2E/jwT3tbu2x2OPH/wZibtempSAehcNMKhRtpF+qqzNaP2nIzqIQnQXiCeRwWB/YANhWW897S7XywbDv5pTXupw7sGsn5x6Ry5tFdiQ61eGCvywW/vA5z/wbVJWZoGHWN2ZLjDLe2tqb6FjwMCx4yb/c4HqJS4Ne3zNu9J8HZL2rTU5EOQOGmFQo30tHUN7j4bsMu3l26jblr8qlrMP8nGxRg44Q+8Zw+OJmJAxKJDA6yrsiyfPjiTlj1vnk7KhVOewwyTrGuppoy+PAaWPeJeXvUNebeXwFB8Mt/YM4tUF9tbnp67iuQNsq6WkXkoBRuWqFwIx1ZUUUts5fv4J0l21mTu3cKtyPAztiMeM4YnMyE/omEOy0a+7JhHsy5GYpzzNsDzoRTHobIZN/WsXsTvH2xOSA6wAFnPAlD/9D8nLxV5hic3RvNGWAT74PMqZr9JdJOKdy0QuFG/MX6/DLmrMjlkxU72VS4dwq4I9DO+Ix4zhjclRP7JRDm66BTWwHzH4JFM8BoMGdwTbwHhl/um/EtG+bB+5eb3WQRyXDBf8xB1y2pKYOPb9zb4pRxOpw1A0JivF+niBwWhZtWKNyIvzEMg/X55XyyYiefrMgle9feoBMcZOfEfgmcMbgr4zMSjnyhwMORu8IMDjuXmbdTRpoDjhMHeOf9DAMWPg1f3WdO904ZCRe8bg68PtjzlvwLPr8TGmohujucNwu6DfNOnSLSJgo3rVC4EX9mGAZrc8v4ZMVO5qzMZevuSvdjIUEBTOifwBmDkxmXkUBwkA+CjqsBfvonfP2AOXjaHgijb4Cxt0FQiOfep7YS/nf93haYYZfBaY9C4GHs47XzF3jnMijeanZlnfwgjLhS3VQi7YTCTSsUbqSzMAyD1TtL+aSx62p7UZX7sTBHAKN7xzEsLYbh3WMYnBLl3bBTsh0+vQ2y5pi3Q2Kh90ToezL0OrFtCw02KdoK/70E8laa4enUh+GYK9oWSqqKzXWImgYhDzwbJj8DwfpvhYjVFG5aoXAjnZFhGKzYXsKclbnMWZHLjuKqZo8H2m0M7BrJ0MawM6x7DF2jgg9tG4jDsfZjM+SU7dx7n81udiH1PQn6nASJRx16MMn+zhwUXLnbXBTx/Neg++gjq9Ew4McXzOntrnpz+4rzX4WkQUf2uiJyRBRuWqFwI52dYRj8ur2En7P3sCyniKVbiygoq9nvvKTIYIZ1j2ZYmhl2BnaNxBnogdadhjpzc88NX8CGuVCwpvnjEV2hzySzVSd9bMvr5RgG/PSSOU7GaIDko+HCN8w1bDxl28/w7hQo3Q6BwWY319D/UzeViEUUblqhcCPSnGEY7CiuYunWIn7JKWbp1iLW5JbS4Gr+nwZHoJ1B3aIYlhbN8O6xHN8nzjMzsYpzzJCz4UvYvADq92lVCnBA9zFm0OlzEnTpBXXV5ho1y/9jnjP4AnOgsifH8DSp3GOubLzhy8b3utBc2TgqRSFHxMc6XLiZMWMGjz76KHl5eQwZMoRnn32WkSNHtnjurFmz+OMf/9jsPqfTSXV19SG9l8KNyMFV1TawYnsxS3OKWLa1mGU5ReypqG12jjPQnIl1+uBkTuyXQKjDA0Gnrhq2fG+26qz/whzcu6/YXuYg4YI1ZnfWSX+HY6/zbtBwuWDhk/D1381ZWGCuKN20y3rKSOh6tHfClYi4dahw89///pdLL72UmTNnMmrUKJ566ineffddsrKySEhI2O/8WbNmceONN5KVleW+z2azkZiYeEjvp3AjcvgMw2Dr7kqWbi1iWU4R32/c1WwmVnCQnQn9EjltUDLj+8V7JugYBuzaYLaabPgCtv5gjoEBcx2ac1+BXuOP/H0O1ZbvYe49kLt8bx1N7IHmmJymsJNyDMT06DitO/U1jTvOF5i7z1fsc1xeABW7ILKrOZ6px3EQ17fjfDbxGx0q3IwaNYoRI0bw3HPPAeByuUhNTeXPf/4zd9xxx37nz5o1i5tuuoni4uJDev2amhpqavaOJygtLSU1NVXhRuQINM3EahqgnLOn+ZTzE/sncMYgc8q5x9bWqS6FzfMhb4W52nBMD8+87uGqrTQDzvafzbFD2382Q8BvhcXv3a09ZYS5oagV+225XOYmrXkrGsNK4T6hpTHAVJcc3muGxplBp/sY8zrxKG1AKl7XYcJNbW0toaGhvPfee5x11lnu+y+77DKKi4uZPXv2fs+ZNWsWV155Jd26dcPlcjFs2DAefPBBBg4c2OJ73Hvvvdx333373a9wI+IZhmGwakcpn6zcyZwVuc2mnIc6ApjQP5HTByUzLiPeN2vr+JphQMm2xrDzs3md+yu46pqfZ7ObIaD7aEjLNK/D92+d9ojSXNj8DWz62gyEFYUHf449qHET14R9LokQlmBO1d+1AbYuND9f/W+GAQRHNX6mMeYleQgEWLQFiJhKd5rdp34UOjtMuNm5cyfdunXjhx9+IDMz033/bbfdxoIFC1i8ePF+z1m0aBEbNmxg8ODBlJSU8Nhjj/Htt9+yevVqUlL2nymhlhsR32macv7pylw++c2U87CmoDM4mbF9/TToNKmrNltKtv0E23+C7UugdMf+53Xp3Rh2RpvX0Wlt6+6prTC77TZ9DZu+gcK1zR8PCjO7yqJSzBalZiGm8Tg4+tDeu77GXPBw60LYshC2LTYXaPzt+6WNamzdOc5c7flwFlSUttu1Eb78f7D+M+g6DH7/sjkQ3w/4dbj5rbq6Ovr3789FF13EAw88cNDzNeZGxDeappzPWWG26Ows2fv/9h2BdoalRZPZM45je8ZydFq0Z6aZt2clO2Dbj7B1kRlECtYAv/nPb2S3va063UdDXEbL/8/b5YK8X/eGmW2Lza0j3GxmN1ivE81xSSkjIdDhnc/VUG/WsvWHvZfq4ubn2IPMYBXTA2K6m1tcuI97mC1DGsNzZKqKYMEj5hIJ+44JCwqDUx/yi2UMOky4aUu3VEvOO+88AgMDeeuttw56rsKNiO+5XAbLtxczZ0Uun67MJbekebdGcJCd4d1jODa9C5m9ujA4JRpHoP80p7eoco8ZSrb+ADmLzNaQ3w5UDondG3ZSjoFd680ws3k+VO1pfm5Umhlkeo031wc6klWfj4TLZQa3rQsbLz8cvFvMEW6GnejuZuDZ9zi6OzhCfVF5c3XVsGeTuS5TeKLZ4tUeu9oa6mDJKzD/QTPggLlsQub1ZtjZ+r15X//J5mrbVv0uPKDDhBswBxSPHDmSZ599FjAHFKelpXH99de3OKD4txoaGhg4cCCnnXYaTzzxxEHPV7gRsZZhGGwqrGDR5t38uHk3izfvZld582nmIUEBHNMjhmN7duHYnl0YnBJFUICfh53aCrP7KmeRGQq2/dx8zZ/fckRA+gmNgeZEiO3ZPv+fuWGY228UbzW3yijeCkVbzOOiLVCed/DXCE80lwGI7QldeprXTRdnxJHVV1VshsbCLPO66bh4696p/wDYILSLuRGruzsvcW+3XkTS3mNnpG/+LTbMgy/ugl2Ns4fj+8HJ/zC3NgFzb7cfnoGv/2GOAYtIhrNe8O4sw+pSWDrLDKYDzvToS3eocPPf//6Xyy67jBdffJGRI0fy1FNP8c4777Bu3ToSExO59NJL6datG9OnTwfg/vvv59hjj6V3794UFxfz6KOP8tFHH7F06VIGDDj4bsMKNyLti2EYbCwod4edHzfv2W9NnVBHAMf0iCWzZxeO7RnLAE+tltye1deaA5NzfjC7snYuM1sxmsJMt+EQEGR1lUeurgqKt5lBxx18tuwNQzWlrT8/LMEcUxLbc/9L055ghgFluXsDzL5BpqWZbk2CoyAwxGx5MhoO/TMFhjQGnmRIHWmuuJ16rOe6BgvWwZd/hY3zzNshsXDiX2HYlJZbl3Yuh/evhN0bzNuZ18OEuz07Dqq8EBbPhJ//ac6+i8uA63706IDmDhVuAJ577jn3In5HH300zzzzDKNGjQJg3Lhx9OjRg1mzZgFw880388EHH5CXl0dMTAzDhw/n73//O0OHDj2k91K4EWnfXC6DDQXlLNq0i0Wbd7M4ew/Flc1nHgXabfRNjGBQtyiO6hbJwG5RDEiO9O9Byp2RYZhdLUXZsCcbdm+CPZvN7qI9m809xVrTNHi6aCvUlh34vIiuEN/X/IMc1wfiM8zj8ASzBcbVYHYjluf/5lIAZXnN1wSqOcC0ekcE9Bxrtqr0mdS2rUIqdsP86bDk32bYsgfBqD/BCX+BkOjWn1tbaQaiJf82bycOgt//ExL6H34d+yraAj88B7+8vncWXZc+cNxN5oreHuzK63DhxpcUbkQ6FpfLYF1eGT9u3s2izbv5ecv+YQcgwG6jd3w4A7tFNoYeM/B4ZIsIaZ+qihvDzj6XpgBUuav5ubYAiE03Q4s7yPQ1w4wnd32vrdy7COKebHPQ98Z5+9cT329v0EnLbL0Vpb7WbBFZ8PDeNYn6nQGT7j/8mVBZn8HsqWYwDAw2X2Pk1YffjZa/Gr5/Cla9v7dVq9twOO5myDjdK1PQFW5aoXAj0rE17YW1akcpq3aUsGpnCat2lOw3bgfM/173jAvjqG5RHNXVDDz9kyOIDvXSzCFpP6pLzJBTXmB258X29N6MsYNxucyFHzd+BRvnmmsF7TueJyiseatOdJp5v2GYYeTL/2e2VoHZ4nLKg+Z4q7Yqy4fZ1+3t1uo9Cc6cARGHsNL/1kXw/ZPmquFNep1ohpoex3t1rJHCTSsUbkT8j2EY5JfW7BN2zOCTV9rynnOxYQ56dAklPS6cnvFhpMeF0aNLGD3iQj2zdYRIayr3mIssbphnBoyKguaPx/U1A0f+KsheYN4XlgAT/gZHXwJ2D3S/GoY5bfzLv0FDjbnq9JkzIOOUls9d/4UZarb92HinDQaeBWNuMvdW8wGFm1Yo3Ih0HoVlNazeWcLqnaWs3G4Gn31XUG5JclQw6XFh+11SY0P9f8aW+J7LBfkrYcNcM+hs+6n54OUAJ2ROheOnHfnMsJYUrDUHG+evMm8fc4W5Ia0j1FzDaPUHZqgpWNNYjwOGXARjbvT54oAKN61QuBHp3Cpr69myq5LsXRVk7ypn864KsndVsGVXBUUtjOVpEmC3kRYbysCukQxLi2FoWnTnmLUlvlVVbK5jtOkrM0iMvsGcVu1N9TXw1f2wyNzjkbi+ZoBZ+goU55j3OcLhmMvh2OsgMtm79RyAwk0rFG5E5ECKKmrJ3l1BdmFFY/jZe6mq238qsCPAzsBue8PO0LQYukYFY2uP682IHMymr+HDa5uvPRQaB8deCyOugJAY62pD4aZVCjcicriaxvRsKCjj123F/JJTzC/bivdbjwcgIcLJ0LToxsATw6BuUZ7bGV3E2yr3wKd/MWdDjbgChv4BgkKsrgpQuGmVwo2IeIJhGOTsqWRZTpEZdnKKWZtbSr2r+X9SA+w2+idHMDQ1hkEpUfSKDyM9LpyY0CC18IgcBoWbVijciIi3VNU2sGpnCcu2moFnWU4RBWU1LZ4bGRxIenw4PRtnaqXHh5HeOGMrItgPVh4W8TCFm1Yo3IiIrxiGQW5JNctyili2tZis/FKyCyua7ZDekrhwJz0bZ2n1aLzuGR9Gz7gwAjVjSzophZtWKNyIiNWqahvYusecobV5lzmAectuc+ByS4sRNglzBDC8Ryyj0s1Lp9g9XaSRwk0rFG5EpD0rra5jy29mam3ZVcGmwgrKa+qbnRscZGdYWgwj02MZld6FoWnR2l9L/JbCTSsUbkSkI2raY2tx9m5+yt7DT9l72P2b2VqOADtDUqMYld6FUT1jGZYWo721xG8o3LRC4UZE/IFhGGwsKGdx9h7zsnn3foOXA+02juoWxaiesQxNjSEpKpj4CCdx4Q4tPigdjsJNKxRuRMQfGYbB1t2VLM7ezeLNZuDZUXzgrSYigwOJj3A2hh2n+zg+3Elc43VChJPYMIcGMUu7oHDTCoUbEeksthdVNgad3azLK2NXWQ27ymupbXAd/MmNbDaIDXWQGhtKr/hweiWEmdfx4XTvov22xHcUblqhcCMinZlhGJRW1VNYXk1BY9gpLKuhsKyGXeU17uPC8hp2l9fgauUvRKDdRlqXUHfY6RUfRq8E8zgqRGv1iGcp3LRC4UZE5NA0uAyKKmspKK1h6+4KNhWWs6mw8bqgnIra/ffbahIX7mwWdnrGh9ErLpxuMSEE2LUysxy+w/n7rWH0IiLSogC7jbhwc0zOgK7N/5g07be1qbCcjQXljcGnnE0FFeSVVrOr3GwJWpy9p9nzHIF2enQJpWecGXh67hN8okLV2iOeoZYbERHxqPKaejbvE3Y2FZazubCC7N0V1NYfeLxPlzBH40rMzYNP99hQDWoWdUu1RuFGRMQaDS6DncVV7rCzeVfjdaHZ2nMgjgA7vRLCyUgMJyMpkn5JEWQkRZAcFazNRzsRhZtWKNyIiLQ/FTX1ZO+q2Cf4VLC58biqruWxPZHBgWQ0Bp2MpEgyEs1jDWb2Two3rVC4ERHpOFwugx3FVazLKyMrr7TxuozNuypoOMBUruSo4L2hJzGCtNhQUmNDiQ93Ytdg5g5L4aYVCjciIh1fTX0DmwsryMorcwefrLyyVndcdwTY6RYTQkpMCCkxoY3X5nFqbAjx4U51c7Vjmi0lIiJ+zRkYQP/kSPonN/8jV1JVx/r8vYFnQ34524uqyC2porbB5d6MtOXXbAo/oaQ2XseFOwhzBhLqCNh77Qgk1GlehwQFqDWoHVLLjYiI+L26Bhd5JdVsL6piW1El24uq2N50vaeSvNLqVhcsbE2oI4BQRyBhzsZrRwChzkDCnQEkRYaQGtu8pSgiWGOC2kItNyIiIvsICrCT2jj2JpMu+z1eW98UfirdAWjbnkqKKuuorK2noqbBvK5toLKmnsq6BpqaBiprG6isbWBX+aHVEhUS5A46qe7QE0pKYwgK107uR0zfoIiIdHqOQDtpXUJJ6xJ6SOcbhkF1nYuK2noqaxrM631DUE0DpdV15DYGpm17zJaioso6SqrMy+qdpS2+dnSoGX6SIkPoEuYgJsxBbFgQMaEOuoQ7iAl1ENt4f4QzUOOEWqBwIyIicphsNhshjgBCHAEQfujPK6+pZ8e+XWJNwafYvF1cWee+rNrRcvjZV6DdRkyYwwxB7tATRGyYk9SYEHrGh5EeF05MaFCnCkEKNyIiIj4S7ty7Nk9Lyqrr2FFcxbY9VRSUVVNUUcvuilqKKmrZU1lnXjdequoaqHcZ7s1OWxMVEkR6XBg948JIjwsjPb7xOi6MUIf/RQENKBYREemAqmobKKo0g07T9Z7GIFRYXkvOngqyCytanR4PkBQZ7A48TeEnNTaU6JAgokKDcAYG+OgTtU4DikVERPyc2S0WQtfokFbPq6ptYMvuCvc0+M2FFWTvKid7VwVFlXXklVaTV1rNos27W3x+cJCd6BAHUY1hJyokyAw+jZfo0CAiQ4KIDnW4H4sJdVi6EarCjYiIiB8LcbS8JhBAUUUt2bvNFh53+NlVwc7iKkqr6zAMqK5zkVdX3er+X781sGskc2443pMf47Ao3IiIiHRSMY2zroalxez3mMtlUFZTT0njDK/iqlr3TK/iyjpKG6/3PlbfeF8t0Ra22oDCjYiIiLTAbre5u54Ol6utKyJ6iN3SdxcRERG/Y/WWFAo3IiIi4lcUbkRERMSvKNyIiIiIX1G4EREREb+icCMiIiJ+ReFGRERE/IrCjYiIiPgVhRsRERHxKwo3IiIi4lcUbkRERMSvKNyIiIiIX1G4EREREb+icCMiIiJ+JdDqAnzNMMxt2EtLSy2uRERERA5V09/tpr/jrel04aasrAyA1NRUiysRERGRw1VWVkZUVFSr59iMQ4lAfsTlcrFz504iIiKw2WzNHistLSU1NZVt27YRGRlpUYUdj763ttH31jb63g6fvrO20ffWNt763gzDoKysjK5du2K3tz6qptO13NjtdlJSUlo9JzIyUj/kNtD31jb63tpG39vh03fWNvre2sYb39vBWmyaaECxiIiI+BWFGxEREfErCjf7cDqd3HPPPTidTqtL6VD0vbWNvre20fd2+PSdtY2+t7ZpD99bpxtQLCIiIv5NLTciIiLiVxRuRERExK8o3IiIiIhfUbgRERERv6Jws48ZM2bQo0cPgoODGTVqFD/99JPVJbVr9957LzabrdmlX79+VpfV7nz77bdMnjyZrl27YrPZ+Oijj5o9bhgGd999N8nJyYSEhDBx4kQ2bNhgTbHtxMG+sylTpuz32zvllFOsKbadmD59OiNGjCAiIoKEhATOOusssrKymp1TXV3N1KlT6dKlC+Hh4fz+978nPz/foorbh0P53saNG7ff7+2aa66xqOL24YUXXmDw4MHuhfoyMzP57LPP3I9b/VtTuGn03//+l2nTpnHPPfewbNkyhgwZwsknn0xBQYHVpbVrAwcOJDc31335/vvvrS6p3amoqGDIkCHMmDGjxccfeeQRnnnmGWbOnMnixYsJCwvj5JNPprq62seVth8H+84ATjnllGa/vbfeesuHFbY/CxYsYOrUqfz444/MnTuXuro6TjrpJCoqKtzn3HzzzXz88ce8++67LFiwgJ07d3LOOedYWLX1DuV7A7jqqqua/d4eeeQRiypuH1JSUnjooYdYunQpS5Ys4cQTT+TMM89k9erVQDv4rRliGIZhjBw50pg6dar7dkNDg9G1a1dj+vTpFlbVvt1zzz3GkCFDrC6jQwGMDz/80H3b5XIZSUlJxqOPPuq+r7i42HA6ncZbb71lQYXtz2+/M8MwjMsuu8w488wzLamnoygoKDAAY8GCBYZhmL+roKAg491333Wfs3btWgMwFi1aZFWZ7c5vvzfDMIyxY8caN954o3VFdRAxMTHGyy+/3C5+a2q5AWpra1m6dCkTJ05032e325k4cSKLFi2ysLL2b8OGDXTt2pWePXtyySWXkJOTY3VJHUp2djZ5eXnNfntRUVGMGjVKv72DmD9/PgkJCWRkZHDttdeye/duq0tqV0pKSgCIjY0FYOnSpdTV1TX7rfXr14+0tDT91vbx2++tyRtvvEFcXBxHHXUUd955J5WVlVaU1y41NDTw9ttvU1FRQWZmZrv4rXW6jTNbsmvXLhoaGkhMTGx2f2JiIuvWrbOoqvZv1KhRzJo1i4yMDHJzc7nvvvs4/vjjWbVqFREREVaX1yHk5eUBtPjba3pM9nfKKadwzjnnkJ6ezqZNm7jrrrs49dRTWbRoEQEBAVaXZzmXy8VNN93EmDFjOOqoowDzt+ZwOIiOjm52rn5re7X0vQFcfPHFdO/ena5du7JixQpuv/12srKy+OCDDyys1norV64kMzOT6upqwsPD+fDDDxkwYADLly+3/LemcCNtduqpp7qPBw8ezKhRo+jevTvvvPMOV1xxhYWVib+78MIL3ceDBg1i8ODB9OrVi/nz5zNhwgQLK2sfpk6dyqpVqzQG7jAd6Hu7+uqr3ceDBg0iOTmZCRMmsGnTJnr16uXrMtuNjIwMli9fTklJCe+99x6XXXYZCxYssLosQAOKAYiLiyMgIGC/kdz5+fkkJSVZVFXHEx0dTd++fdm4caPVpXQYTb8v/faOTM+ePYmLi9NvD7j++uv55JNP+Oabb0hJSXHfn5SURG1tLcXFxc3O12/NdKDvrSWjRo0C6PS/N4fDQe/evRk+fDjTp09nyJAhPP300+3it6Zwg/kPNHz4cL766iv3fS6Xi6+++orMzEwLK+tYysvL2bRpE8nJyVaX0mGkp6eTlJTU7LdXWlrK4sWL9ds7DNu3b2f37t2d+rdnGAbXX389H374IV9//TXp6enNHh8+fDhBQUHNfmtZWVnk5OR06t/awb63lixfvhygU//eWuJyuaipqWkfvzWfDFvuAN5++23D6XQas2bNMtasWWNcffXVRnR0tJGXl2d1ae3WLbfcYsyfP9/Izs42Fi5caEycONGIi4szCgoKrC6tXSkrKzN++eUX45dffjEA44knnjB++eUXY+vWrYZhGMZDDz1kREdHG7NnzzZWrFhhnHnmmUZ6erpRVVVlceXWae07KysrM2699VZj0aJFRnZ2tjFv3jxj2LBhRp8+fYzq6mqrS7fMtddea0RFRRnz5883cnNz3ZfKykr3Oddcc42RlpZmfP3118aSJUuMzMxMIzMz08KqrXew723jxo3G/fffbyxZssTIzs42Zs+ebfTs2dM44YQTLK7cWnfccYexYMECIzs721ixYoVxxx13GDabzfjyyy8Nw7D+t6Zws49nn33WSEtLMxwOhzFy5Ejjxx9/tLqkdu2CCy4wkpOTDYfDYXTr1s244IILjI0bN1pdVrvzzTffGMB+l8suu8wwDHM6+N/+9jcjMTHRcDqdxoQJE4ysrCxri7ZYa99ZZWWlcdJJJxnx8fFGUFCQ0b17d+Oqq67q9P9HpKXvCzBeeeUV9zlVVVXGddddZ8TExBihoaHG2WefbeTm5lpXdDtwsO8tJyfHOOGEE4zY2FjD6XQavXv3Nv7yl78YJSUl1hZuscsvv9zo3r274XA4jPj4eGPChAnuYGMY1v/WbIZhGL5pIxIRERHxPo25EREREb+icCMiIiJ+ReFGRERE/IrCjYiIiPgVhRsRERHxKwo3IiIi4lcUbkRERMSvKNyIiIiIX1G4EZFOb/78+dhstv02+hORjknhRkRERPyKwo2IiIj4FYUbEbGcy+Vi+vTppKenExISwpAhQ3jvvfeAvV1Gc+bMYfDgwQQHB3PssceyatWqZq/x/vvvM3DgQJxOJz169ODxxx9v9nhNTQ233347qampOJ1Oevfuzb/+9a9m5yxdupRjjjmG0NBQRo8eTVZWlnc/uIh4hcKNiFhu+vTpvPbaa8ycOZPVq1dz880384c//IEFCxa4z/nLX/7C448/zs8//0x8fDyTJ0+mrq4OMEPJ+eefz4UXXsjKlSu59957+dvf/sasWbPcz7/00kt56623eOaZZ1i7di0vvvgi4eHhzer461//yuOPP86SJUsIDAzk8ssv98nnFxHP0q7gImKpmpoaYmNjmTdvHpmZme77r7zySiorK7n66qsZP348b7/9NhdccAEAe/bsISUlhVmzZnH++edzySWXUFhYyJdfful+/m233cacOXNYvXo169evJyMjg7lz5zJx4sT9apg/fz7jx49n3rx5TJgwAYBPP/2U008/naqqKoKDg738LYiIJ6nlRkQstXHjRiorK5k0aRLh4eHuy2uvvcamTZvc5+0bfGJjY8nIyGDt2rUArF27ljFjxjR73TFjxrBhwwYaGhpYvnw5AQEBjB07ttVaBg8e7D5OTk4GoKCg4Ig/o4j4VqDVBYhI51ZeXg7AnDlz6NatW7PHnE5ns4DTViEhIYd0XlBQkPvYZrMB5nggEelY1HIjIpYaMGAATqeTnJwcevfu3eySmprqPu/HH390HxcVFbF+/Xr69+8PQP/+/Vm4cGGz1124cCF9+/YlICCAQYMG4XK5mo3hERH/pZYbEbFUREQEt956KzfffDMul4vjjjuOkpISFi5cSGRkJN27dwfg/vvvp0uXLiQmJvLXv/6VuLg4zjrrLABuueUWRowYwQMPPMAFF1zAokWLeO6553j++ecB6NGjB5dddhmXX345zzzzDEOGDGHr1q0UFBRw/vnnW/XRRcRLFG5ExHIPPPAA8fHxTJ8+nc2bNxMdHc2wYcO466673N1CDz30EDfeeCMbNmzg6KOP5uOPP8bhcAAwbNgw3nnnHe6++24eeOABkpOTuf/++5kyZYr7PV544QXuuusurrvuOnbv3k1aWhp33XWXFR9XRLxMs6VEpF1rmslUVFREdHS01eWISAegMTciIiLiVxRuRERExK+oW0pERET8ilpuRERExK8o3IiIiIhfUbgRERERv6JwIyIiIn5F4UZERET8isKNiIiI+BWFGxEREfErCjciIiLiV/4/7RWYBk80F5kAAAAASUVORK5CYII=\n"
          },
          "metadata": {}
        }
      ]
    }
  ]
}